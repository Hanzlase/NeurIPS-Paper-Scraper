title,authors,year,abstract,pdf_link,abstract_link,domain
Bit-Serial Neural Networks,"Alan Murray, Anthony Smith, Zoe Butler",1987,"A  bit  - serial  VLSI  neural  network  is  described  from  an  initial  architecture  for  a  synapse array through to silicon layout and board design.  The issues surrounding bit  - serial  computation,  and  analog/digital  arithmetic  are  discussed  and  the  parallel  development  of  a  hybrid  analog/digital  neural  network  is  outlined.  Learning  and  recall  capabilities  are  reported  for  the  bit  - serial  network  along  with  a  projected  specification  for  a  64  - neuron,  bit  - serial  board  operating  at 20 MHz.  This tech(cid:173) nique  is  extended  to  a  256  (2562  synapses)  network  with  an  update  time  of 3ms,  using  a  ""paging""  technique  to  time  - multiplex  calculations  through  the  synapse  array.",https://papers.nips.cc/paper_files/paper/1987/file/02e74f10e0327ad868d138f2b4fdd6f0-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/02e74f10e0327ad868d138f2b4fdd6f0-Abstract.html,Machine Learning and Deep Learning
Connectivity Versus Entropy,Yaser Abu-Mostafa,1987,"How  does  the  connectivity  of a  neural  network  (number  of synapses  per  neuron)  relate  to  the complexity  of the  problems  it  can  handle  (measured  by  the entropy)?  Switching theory would suggest no relation at all, since all Boolean  functions  can be  implemented  using  a  circuit  with very  low  connectivity  (e.g.,  using  two-input  NAND  gates).  However,  for  a  network  that  learns  a  problem  from  examples  using  a  local  learning  rule,  we  prove  that  the  entropy  of  the  problem becomes  a  lower  bound for  the connectivity of the network.",https://papers.nips.cc/paper_files/paper/1987/file/03afdbd66e7929b125f8597834fa83a4-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/03afdbd66e7929b125f8597834fa83a4-Abstract.html,Machine Learning and Deep Learning
The Hopfield Model with Multi-Level Neurons,Michael Fleisher,1987,"The  Hopfield  neural  network.  model  for  associative  memory  is  generalized.  The  generalization  replaces  two  state  neurons by neurons taking a  richer set of values.  Two  classes  of neuron  input output  relations are developed guaranteeing convergence to stable states.  The first is a class of ""continuous"" rela- tions and the second is a class of allowed quantization rules for the neurons.  The information capacity for  networks from  the second class is fOWld  to be of order N 3 bits for a network with N  neurons.  A generalization of the sum of outer products learning rule is developed and investigated as well.  Â© American Institute of Physics 1988  279",https://papers.nips.cc/paper_files/paper/1987/file/072b030ba126b2f4b2374f342be9ed44-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/072b030ba126b2f4b2374f342be9ed44-Abstract.html,Machine Learning and Deep Learning
How Neural Nets Work,"Alan Lapedes, Robert Farber",1987,"There is  presently great interest in the abilities of neural networks to mimic 
""qualitative reasoning""  by manipulating neural incodings of symbols.  Less work 
has  been performed on using neural networks to process floating  point numbers 
and it is  sometimes stated that neural networks are somehow inherently inaccu(cid:173)
rate  and  therefore  best  suited  for  ""fuzzy""  qualitative reasoning.  Nevertheless, 
the  potential  speed  of massively  parallel  operations  make  neural  net  ""number 
crunching""  an interesting topic  to explore.  In this paper we  discuss some of our 
work in which we  demonstrate that for  certain applications neural networks can 
achieve  significantly  higher  numerical  accuracy  than  more  conventional  tech(cid:173)
niques.  In  particular,  prediction  of future  values  of a  chaotic  time  series  can 
be  performed  with  exceptionally  high  accuracy.  We  analyze  how  a  neural  net 
is  able  to do  this  ,  and in  the process  show  that  a  large class  of functions  from 
Rn.  ~ Rffl  may  be  accurately  approximated  by  a  backpropagation  neural  net 
with just two  ""hidden""  layers.  The network  uses  this functional  approximation 
to perform either interpolation (signal processing applications)  or extrapolation 
(symbol processing applicationsJ.  Neural nets therefore use quite familiar meth(cid:173)
ods to perform. their tasks.  The geometrical viewpoint advocated here seems to 
be a  useful  approach  to analyzing  neural  network  operation  and  relates  neural 
networks  to well  studied topics  in  functional  approximation.",https://papers.nips.cc/paper_files/paper/1987/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/093f65e080a295f8076b1c5722a46aa2-Abstract.html,Machine Learning and Deep Learning
Spatial Organization of Neural Networks: A Probabilistic Modeling Approach,"Andreas Stafylopatis, Marios Dikaiakos, D. Kontoravdis",1987,"The  aim  of  this  paper  is  to  explore  the  spatial  organization  of  neural  networks  under  Markovian  assumptions,  in  what  concerns  the be(cid:173) haviour  of  individual  cells  and  the  interconnection  mechanism.  Space(cid:173) organizational  properties  of  neural  nets  are  very  relevant  in  image  modeling  and  pattern  analysis,  where  spatial  computations  on  stocha(cid:173) stic  two-dimensional  image  fields  are  involved.  As  a  first  approach  we  develop  a  random  neural  network  model,  based  upon  simple  probabi(cid:173) listic  assumptions,  whose  organization  is  studied  by  means  of  dis(cid:173) crete-event  simulation.  We  then  investigate  the  possibility  of  ap(cid:173) proXimating  the  random  network's  behaviour  by  using  an  analytical  ap(cid:173) proach  originating  from  the  theory  of  general  product-form  queueing  networks.  The  neural  network  is  described  by  an  open  network  of  no(cid:173) des,  in  which  customers  moving  from  node  to  node  represent  stimula(cid:173) tions  and  connections  between  nodes  are  expressed  in  terms  of  sui(cid:173) tably  selected  routing  probabilities.  We  obtain  the  solution  of  the  model  under  different  disciplines  affecting  the  time  spent  by  a  sti(cid:173) mulation  at  each  node  visited.  Results  concerning  the  distribution  of  excitation  in  the  network  as  a  function  of  network  topology  and  external  stimulation  arrival  pattern  are  compared  with  measures  ob(cid:173) tained  from  the  simulation  and  validate  the  approach  followed.",https://papers.nips.cc/paper_files/paper/1987/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html,Machine Learning and Deep Learning
A Neural-Network Solution to the Concentrator Assignment Problem,"Gene Tagliarini, Edward Page",1987,Networks  of simple analog  processors  having  neuron-like properties have  been  employed  to  compute  good  solutions  to  a  variety  of optimization  prob(cid:173) lems.  This  paper presents  a  neural-net solution to  a  resource allocation prob(cid:173) lem that arises  in  providing  local  access  to  the  backbone of a  wide-area  com(cid:173) munication  network.  The  problem is  described in  terms of an energy function  that can be  mapped onto an analog computational network.  Simulation results  characterizing  the  performance  of the  neural  computation  are  also  presented.,https://papers.nips.cc/paper_files/paper/1987/file/1679091c5a880faf6fb5e6087eb1b2dc-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/1679091c5a880faf6fb5e6087eb1b2dc-Abstract.html,Machine Learning and Deep Learning
LEARNING BY STATE RECURRENCE DETECTION,"Bruce Rosen, James Goodwin, Jacques Vidal",1987,"This research investigates a new technique for unsupervised learning of nonlinear  control problems. The approach is applied both to Michie and Chambers BOXES  algorithm and to Barto, Sutton and Anderson's extension, the ASE/ACE system, and  has significantly improved the convergence rate of stochastically based learning  automata.  Recurrence learning is a new nonlinear reward-penalty algorithm. It exploits  information found during learning trials to reinforce decisions resulting in the  recurrence of nonfailing states. Recurrence learning applies positive reinforcement  during the exploration of the search space, whereas in the BOXES or ASE algorithms,  only negative weight reinforcement is applied, and then only on failure. Simulation  results show that the added information from recurrence learning increases the learning  rate.  Our empirical results show that recurrence learning is faster than both basic failure  driven learning and failure prediction methods. Although recurrence learning has only  been tested in failure driven experiments, there are goal directed learning applications  where detection of recurring oscillations may provide useful information that reduces  the learning time by applying negative, instead of positive reinforcement.  Detection of cycles provides a heuristic to improve the balance between evidence  gathering and goal directed search.",https://papers.nips.cc/paper_files/paper/1987/file/182be0c5cdcd5072bb1864cdee4d3d6e-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/182be0c5cdcd5072bb1864cdee4d3d6e-Abstract.html,AI and Applications
Stability Results for Neural Networks,"Anthony Michel, Jay Farrell, Wolfgang Porod",1987,"In the present paper we survey and utilize results from the qualitative theory of large  scale interconnected dynamical systems in order to develop  a  qualitative theory for  the  Hopfield model of neural networks.  In our approach we  view such networks as  an inter(cid:173) connection of many single neurons.  Our results  are  phrased in  terms of the  qualitative  properties of the individual neurons and in terms of the properties of the interconnecting  structure of the neural  networks.  Aspects of neural networks which  we  address include  asymptotic stability,  exponential stability,  and instability  of an  equilibrium;  estimates  of trajectory bounds; estimates of the domain of attraction of an asymptotically stable  equilibrium;  and stability of neural networks  under structural perturbations.",https://papers.nips.cc/paper_files/paper/1987/file/19ca14e7ea6328a42e0eb13d585e4c22-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/19ca14e7ea6328a42e0eb13d585e4c22-Abstract.html,Ethics and Fairness in AI
Introduction to a System for Implementing Neural Net Connections on SIMD Architectures,Sherryl Tomboulian,1987,"Neural  networks  have  attracted  much  interest  recently,  and  using  parallel  architectures  to simulate  neural  networks  is  a  natural  and  necessary  applica(cid:173) tion.  The  SIMD  model  of parallel  computation is  chosen,  because systems  of  this  type  can  be  built  with  large  numbers  of processing  elements.  However,  such systems are not naturally suited to generalized communication.  A method  is  proposed  that  allows  an implementation of neural  network  connections  on  massively parallel SIMD  architectures.  The key to this system is an algorithm  that allows  the formation  of arbitrary  connections  between  the ""neurons"".  A  feature  is  the ability  to add  new  connections  quickly.  It also  has error  recov(cid:173) ery  ability and  is  robust  over a  variety  of network  topologies.  Simulations  of  the general  connection system, and its implementation on the Connection Ma(cid:173) chine,  indicate  that  the  time and  space  requirements  are  proportional  to  the  product of the  average number of connections per neuron and the diameter of  the interconnection network.",https://papers.nips.cc/paper_files/paper/1987/file/1c383cd30b7c298ab50293adfecb7b18-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/1c383cd30b7c298ab50293adfecb7b18-Abstract.html,AI and Applications
Optimization with Artificial Neural Network Systems: A Mapping Principle and a Comparison to Gradient Based Methods,Harrison Leong,1987,General  formulae  for  mapping  optimization  problems  into  systems  of  ordinary  differential  equations  associated with artificial  neural  networks  are  presented.  A comparison is  made  to  optim(cid:173) ization using gradient-search methods.  The perfonnance  measure  is  the  settling time  from  an  initial  state  to  a  target  state.  A  simple  analytical  example  illustrates  a situation  where  dynamical  systems  representing  artificial  neural  network  methods  would  settle  faster  than  those  representing  gradient(cid:173) search.  Settling  time  was  investigated  for  a  more  complicated  optimization  problem  using  com(cid:173) puter  simulations.  The  problem  was  a  simplified  version  of a problem  in  medical  imaging:  deter(cid:173) mining  loci  of cerebral  activity  from  electromagnetic  measurements  at  the  scalp.  The  simulations  showed  that  gradient  based  systems  typically  settled  50  to  100  times  faster  than  systems  based  on  current neural  network optimization methods.,https://papers.nips.cc/paper_files/paper/1987/file/1f0e3dad99908345f7439f8ffabdffc4-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/1f0e3dad99908345f7439f8ffabdffc4-Abstract.html,AI and Applications
Optimal Neural Spike Classification,"James Bower, Amir Atiya",1987,"Being able  to record the electrical activities of a  number of neurons simultaneously is  likely  to be important in  the study of the functional organization of networks of real neurons.  Using  one  extracellular  microelectrode  to  record  from  several neurons  is  one  approach  to  studying  the  response  properties  of sets  of  adjacent  and  therefore  likely  related  neurons.  However,  to  do  this,  it  is  necessary  to  correctly  classify  the  signals  generated  by  these  different  neurons.  This paper considers  this problem of classifying the  signals  in  such an  extracellular recording,  based upon their shapes, and specifically considers the classification of signals in the case when  spikes overlap  temporally.",https://papers.nips.cc/paper_files/paper/1987/file/1ff1de774005f8da13f42943881c655f-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/1ff1de774005f8da13f42943881c655f-Abstract.html,Machine Learning and Deep Learning
REFLEXIVE ASSOCIATIVE MEMORIES,Hendricus G. Loos,1987,"In the synchronous discrete model, the average memory capacity of  bidirectional associative memories (BAMs) is compared with that of  Hopfield memories, by means of a calculat10n of the percentage of good  recall for 100 random BAMs of dimension 64x64, for different numbers  of stored vectors. The memory capac1ty Is found to be much smal1er than  the Kosko upper bound, which Is the lesser of the two dimensions of the  BAM. On the average, a 64x64 BAM has about 68 % of the capacity of the  corresponding Hopfield memory with the same number of neurons. Ortho(cid:173) normal coding of the BAM Increases the effective storage capaCity by  only 25 %. The memory capacity limitations are due to spurious stable  states, which arise In BAMs In much the same way as in Hopfleld  memories. Occurrence of spurious stable states can be avoided by  replacing the thresholding in the backlayer of the BAM by another  nonl1near process, here called ""Dominant Label Selection"" (DLS). The  simplest DLS is the wlnner-take-all net, which gives a fault-sensitive  memory. Fault tolerance can be improved by the use of an orthogonal or  unitary transformation. An optical application of the latter is a Fourier  transform, which is implemented simply by a lens.",https://papers.nips.cc/paper_files/paper/1987/file/2838023a778dfaecdc212708f721b788-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/2838023a778dfaecdc212708f721b788-Abstract.html,Ethics and Fairness in AI
The Performance of Convex Set Projection Based Neural Networks,"Robert Marks, Les Atlas, Seho Oh, James Ritcey",1987,and,https://papers.nips.cc/paper_files/paper/1987/file/28dd2c7955ce926456240b2ff0100bde-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/28dd2c7955ce926456240b2ff0100bde-Abstract.html,Machine Learning and Deep Learning
Speech Recognition Experiments with Perceptrons,David Burr,1987,"Artificial  neural  networks  (ANNs)  are  capable  of accurate  recognition  of  simple speech  vocabularies such  as  isolated  digits  [1].  This paper looks  at two  more  difficult  vocabularies,  the  alphabetic  E-set  and  a  set  of  polysyllabic  words.  The  E-set  is  difficult  because  it  contains  weak  discriminants  and  polysyllables  are  difficult  because  of  timing  variation.  Polysyllabic  word  recognition  is  aided  by a  time  pre-alignment technique  based on  dynamic pro(cid:173) gramming  and  E-set  recognition  is  improved  by  focusing  attention.  Recogni(cid:173) tion  accuracies  are  better  than  98%  for  both  vocabularies  when  implemented  with a single  layer perceptron.",https://papers.nips.cc/paper_files/paper/1987/file/2a38a4a9316c49e5a833517c45d31070-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/2a38a4a9316c49e5a833517c45d31070-Abstract.html,Machine Learning and Deep Learning
On Properties of Networks of Neuron-Like Elements,"Pierre Baldi, Santosh Venkatesh",1987,"The  complexity  and  computational  capacity  of multi-layered,  feedforward  neural networks is examined.  Neural networks for special purpose (structured)  functions  are examined  from  the  perspective of circuit  complexity.  Known  re(cid:173) sults in  complexity theory are applied to the special instance of neural network  circuits,  and  in  particular,  classes  of functions  that  can  be  implemented  in  shallow circuits characterised.  Some conclusions are  also  drawn about learning  complexity,  and some  open problems raised.  The dual  problem of determining  the computational capacity of a  class of multi-layered  networks with  dynamics  regulated  by  an  algebraic  Hamiltonian  is  considered.  Formal  results  are  pre(cid:173) sented  on  the  storage  capacities  of programmed  higher-order  structures,  and  a  tradeoff between ease  of programming  and  capacity is  shown.  A  precise  de(cid:173) termination is  made of the static fixed  point structure of random higher-order  constructs,  and phase-transitions (0-1  laws)  are  shown.",https://papers.nips.cc/paper_files/paper/1987/file/3295c76acbf4caaed33c36b1b5fc2cb1-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/3295c76acbf4caaed33c36b1b5fc2cb1-Abstract.html,Ethics and Fairness in AI
Ensemble' Boltzmann Units have Collective Computational Properties like those of Hopfield and Tank Neurons,"Mark Derthick, Joe Tebelskis",1987,Abstract Unavailable,https://papers.nips.cc/paper_files/paper/1987/file/32bb90e8976aab5298d5da10fe66f21d-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/32bb90e8976aab5298d5da10fe66f21d-Abstract.html,AI and Applications
On Tropistic Processing and Its Applications,Manuel FernÃ¡ndez,1987,"The  interaction  of  a  set  of  tropisms  is  sufficient  in  many  cases  to  explain  the  seemingly  complex  behavioral  responses  exhibited  by  varied  classes  of  biological  systems  to  combinations  of  stimuli.  It  can  be  shown  that  a  straightforward  generalization  of  the  tropism  phenomenon  allows  the  efficient  implementation  of  effective  algorithms  which  appear  to  respond  ""intelligently""  to  changing  environmental  conditions.  Examples  of  the  utilization  of  tropistic  processing  techniques  will  be  presented  in  this  paper  in  applications  entailing  simulated  behavior  synthesis,  path-planning,  pattern  analysis  (clustering),  and  engineering  design  optimization.",https://papers.nips.cc/paper_files/paper/1987/file/33e75ff09dd601bbe69f351039152189-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/33e75ff09dd601bbe69f351039152189-Abstract.html,AI and Applications
Neuromorphic Networks Based on Sparse Optical Orthogonal Codes,"Mario Vecchi, Jawad Salehi",1987,Abstract Unavailable,https://papers.nips.cc/paper_files/paper/1987/file/3416a75f4cea9109507cacd8e2f2aefc-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/3416a75f4cea9109507cacd8e2f2aefc-Abstract.html,Machine Learning and Deep Learning
A 'Neural' Network that Learns to Play Backgammon,"Gerald Tesauro, Terrence J. Sejnowski",1987,"We describe a class of connectionist networks that have learned to play back(cid:173) gammon  at  an  intermediate-to-advanced  level.  TIle  networks  were  trained  by  a  supervised  learning  procedure  on  a  large  set  of sample  positions evaluated  by  a  human  expert.  In  actual  match  play  against  humans  and  conventional  computer  programs, the networks demonstrate substantial ability to generalize on the basis of  expert knowledge.  Our study touches on some of the most important issues in net(cid:173) work  learning  theory,  including the  development  of efficient coding schemes  and  training procedures, scaling,  generalization,  the  use  of real-valued inputs  and out(cid:173) puts,  and  techniques  for  escaping  from  local  minima.  Practical  applications  in  games and other domains are also discussed.",https://papers.nips.cc/paper_files/paper/1987/file/34173cb38f07f89ddbebc2ac9128303f-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/34173cb38f07f89ddbebc2ac9128303f-Abstract.html,AI and Applications
Learning Representations by Recirculation,"Geoffrey E. Hinton, James McClelland",1987,"We describe a new learning procedure for networks that contain groups of non(cid:173) linear units  arranged in a closed loop.  The  aim of the learning is  to  discover codes  that  allow  the  activity  vectors  in  a  ""visible""  group  to  be  represented  by  activity  vectors  in  a  ""hidden""  group.  One  way  to  test  whether  a  code  is  an  accurate  representation is to try to reconstruct the visible vector from the hidden vector.  The  difference  between  the  original  and  the  reconstructed  visible  vectors  is  called  the  reconstruction  error,  and  the  learning  procedure  aims  to  minimize  this  error.  The  learning procedure has  two  passes.  On the  fust pass,  the  original  visible  vector is  passed around the loop,  and on the second pass an average of the original vector and  the reconstructed vector is passed around the  loop.  The learning procedure  changes  each weight by  an  amount proportional  to  the product of the  ""presynaptic""  activity  and the difference in the post-synaptic activity on the two passes.  This procedure is  much  simpler  to  implement  than  methods  like  back-propagation.  Simulations  in  simple networks  show that it usually converges rapidly on a good set of codes,  and  analysis  shows  that  in  certain  restricted  cases  it  performs  gradient  descent  in  the  squared reconstruction error.",https://papers.nips.cc/paper_files/paper/1987/file/35f4a8d465e6e1edc05f3d8ab658c551-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/35f4a8d465e6e1edc05f3d8ab658c551-Abstract.html,Machine Learning and Deep Learning
A Computer Simulation of Cerebral Neocortex: Computational Capabilities of Nonlinear Neural Networks,"Alexander Singer, John Donoghue",1987,Abstract Unavailable,https://papers.nips.cc/paper_files/paper/1987/file/37693cfc748049e45d87b8c7d8b9aacd-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/37693cfc748049e45d87b8c7d8b9aacd-Abstract.html,AI and Applications
PATTERN CLASS DEGENERACY IN AN UNRESTRICTED STORAGE DENSITY MEMORY,"Christopher Scofield, Douglas L. Reilly, Charles Elbaum, Leon Cooper",1987,in,https://papers.nips.cc/paper_files/paper/1987/file/3c59dc048e8850243be8079a5c74d079-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/3c59dc048e8850243be8079a5c74d079-Abstract.html,Machine Learning and Deep Learning
Strategies for Teaching Layered Networks Classification Tasks,"Ben Wittner, John Denker",1987,"There is  a widespread misconception  that the delta-rule is in some sense guaranteed to  work  on  networks  without hidden units.  As  previous authors have mentioned,  there is  no such guarantee for  classification tasks.  We  will begin by  presenting explicit counter(cid:173) examples illustrating two different interesting ways in which  the delta rule can fail.  We  go on  to provide conditions  which  do guarantee  that gradient  descent  will  successfully  train  networks  without  hidden  units  to  perform  two-category  classification  tasks.  We  discuss  the  generalization  of our  ideas  to  networks  with  hidden  units  and  to  multi(cid:173) category classification  tasks.  The Classification Task  Consider  networks  of the form  indicated  in  figure  1.  We  discuss  various  methods  for  training such  a  network,  that is  for  adjusting its weight  vector,  w.  If we  call  the input  v, the output is  g(wÂ· v), where 9 is  some function.  The classification  task we  wish  to train the network  to perform is  the following.  Given  two finite sets of vectors,  Fl  and F2, output a number greater than zero when a vector in  Fl  is input, and output a number less than zero when a  vector in  F2  is input.  Without  significant loss of generality, we  assume that 9 is odd (Le.  g( -s) ==  -g( sÂ».  In that case,  the task can be reformulated  as follows.  Define  2  F  :==  Fl  U {-v such  that v  E F2}  (1)  and  output  a  number  greater  than  zero  when  a  vector  in  F  is  input.  The  former  formulation is  more natural in some sense,  but the later formulation is  somewhat  more  convenient  for  analysis  and is  the one we  use.  We  call  vectors in  F,  training  vectors.  A  Class  of Gradient  Descent  Algorithms  We  denote  the solution set  by  W  :==  {w such  that g(wÂ· v) > 0 for  all v  E F},  (2)  lCurrently at  NYNEX  Science  and Technology,  500  Westchester  Ave.,  White  Plains,  NY  10604  2 We  use  both  A  := Band B  =: A  to denote  ""A  is  by definition  B"".  @  American Institute of Physics 1988",https://papers.nips.cc/paper_files/paper/1987/file/3ef815416f775098fe977004015c6193-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/3ef815416f775098fe977004015c6193-Abstract.html,Ethics and Fairness in AI
Invariant Object Recognition Using a Distributed Associative Memory,"Harry Wechsler, George Zimmerman",1987,"This  paper  describes  an  approach  to  2-dimensional  object  recognition.  Complex-log  con(cid:173) formal  mapping  is  combined  with  a  distributed  associative  memory  to  create  a  system  which  recognizes  objects  regardless  of  changes  in  rotation  or  scale.  Recalled  information  from  the  memorized  database  is  used  to  classify  an object,  reconstruct  the  memorized  ver(cid:173) sion  of the  object,  and estimate  the  magnitude of changes in  scale  or rotation.  The system  response  is  resistant  to  moderate  amounts of noise  and occlusion.  Several experiments,  us(cid:173) ing real,  gray scale images,  are  presented to show  the feasibility  of our approach.",https://papers.nips.cc/paper_files/paper/1987/file/43ec517d68b6edd3015b3edc9a11367b-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/43ec517d68b6edd3015b3edc9a11367b-Abstract.html,AI and Applications
Cycles: A Simulation Tool for Studying Cyclic Neural Networks,Michael Gately,1987,"A computer program has been designed and implemented to allow a researcher  to  analyze the oscillatory behavior of simulated neural networks with  cyclic  con(cid:173) nectivity.  The  computer  program,  implemented  on  the  Texas  Instruments  Ex(cid:173) plorer / Odyssey system, and the results of numerous experiments are discussed.  The program, CYCLES, allows a user to construct, operate, and inspect neural  networks containing cyclic connection paths with  the aid of a  powerful graphics(cid:173) based interface.  Numerous cycles have been studied, including cycles with one or  more activation points, non-interruptible cycles, cycles with variable path lengths,  and interacting cycles.  The final  class,  interacting cycles,  is  important due to its  ability to implement time-dependent goal processing in neural networks.",https://papers.nips.cc/paper_files/paper/1987/file/44f683a84163b3523afe57c2e008bc8c-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/44f683a84163b3523afe57c2e008bc8c-Abstract.html,AI and Applications
Learning on a General Network,Amir Atiya,1987,"This paper generalizes the backpropagation method to a  general network containing feed(cid:173) back t;onnections.  The network model considered consists of interconnected groups of neurons,  where each  group could be fully  interconnected  (it could  have feedback connections, with pos(cid:173) sibly  asymmetric weights),  but no loops between the  groups are  allowed.  A stochastic descent  algorithm  is  applied,  under  a  certain inequality constraint  on each  intra-group weight  matrix  which  ensures for  the  network  to  possess  a  unique equilibrium  state for  every input.",https://papers.nips.cc/paper_files/paper/1987/file/45c48cce2e2d7fbdea1afc51c7c6ad26-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/45c48cce2e2d7fbdea1afc51c7c6ad26-Abstract.html,Machine Learning and Deep Learning
Neural Net and Traditional Classifiers,"William Huang, Richard P. Lippmann",1987,"Abstract.  Previous  work on  nets  with  continuous-valued inputs  led  to generative 
procedures  to construct convex decision  regions with two-layer perceptrons (one hidden 
layer) and  arbitrary decision  regions  with  three-layer perceptrons  (two hidden layers). 
Here we demonstrate that two-layer perceptron classifiers trained with back propagation 
can  form  both  convex  and  disjoint  decision  regions.  Such  classifiers  are  robust,  train 
rapidly,  and  provide  good  performance  with  simple  decision  regions.  When  complex 
decision  regions  are  required,  however,  convergence  time  can  be  excessively  long  and 
performance is  often no better than that of k-nearest  neighbor classifiers.  Three neural 
net  classifiers  are  presented  that  provide  more  rapid  training  under  such  situations. 
Two use  fixed  weights in  the  first  one  or  two layers and  are  similar  to classifiers  that 
estimate probability density functions using histograms.  A third ""feature map classifier"" 
uses  both  unsupervised  and  supervised  training.  It provides  good  performance  with 
little supervised training in situations such as speech recognition where much unlabeled 
training data is  available.  The architecture of this classifier can  be  used  to implement 
a  neural net  k-nearest  neighbor classifier.",https://papers.nips.cc/paper_files/paper/1987/file/4e732ced3463d06de0ca9a15b6153677-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/4e732ced3463d06de0ca9a15b6153677-Abstract.html,AI and Applications
Scaling Properties of Coarse-Coded Symbol Memories,"Ronald Rosenfeld, David Touretzky",1987,"Abstract:  Coarse-coded symbol memories have appeared in several neural network 
symbol  processing  models.  In  order  to  determine  how  these  models  would  scale,  one 
must  first  have  some  understanding  of the  mathematics  of coarse-coded  representa(cid:173)
tions.  We  define  the  general  structure  of coarse-coded  symbol  memories  and  derive 
mathematical relationships among  their essential parameters:  memory 8ize,  8ymbol-8et 
size and capacity.  The computed capacity of one of the schemes agrees well with actual 
measurements oC  tbe coarse-coded working memory of DCPS, Touretzky and Hinton's 
distributed connectionist production system.",https://papers.nips.cc/paper_files/paper/1987/file/54229abfcfa5649e7003b83dd4755294-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/54229abfcfa5649e7003b83dd4755294-Abstract.html,Machine Learning and Deep Learning
Synchronization in Neural Nets,"Jacques Vidal, John Haggerty",1987,"The  paper  presents  an  artificial  neural  network  concept  (the  Synchronizable Oscillator Networks)  where the instants of individual  firings  in  the  form  of  point  processes  constitute  the  only  form  of  information  transmitted  between  joining  neurons.  This  type  of  communication contrasts with  that which  is  assumed  in most  other  models  which  typically  are  continuous  or  discrete  value-passing  networks.  Limiting the messages received  by each processing unit to  time  markers that signal  the firing  of other units  presents  significant  implemen tation advantages.  In  our  model,  neurons  fire  spontaneously  and  regularly  in  the  absence of perturbation.  When interaction is  present,  the scheduled  firings  are  advanced  or  delayed  by  the firing  of neighboring  neurons.  Networks  of  such  neurons  become  global  oscillators  which  exhibit  multiple  synchronizing  attractors.  From  arbitrary  initial  states,  energy  minimization  learning  procedures  can  make  the  network  converge  to  oscillatory  modes  that  satisfy  multi-dimensional  constraints  Such  networks  can  directly  represent  routing  and  scheduling problems  that conSist of ordering sequences of events.",https://papers.nips.cc/paper_files/paper/1987/file/6364d3f0f495b6ab9dcf8d3b5c6e0b01-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/6364d3f0f495b6ab9dcf8d3b5c6e0b01-Abstract.html,AI and Applications
A NEURAL NETWORK CLASSIFIER BASED ON CODING THEORY,"Tzi-Dar Chiueh, Rodney Goodman",1987,"The new neural network classifier we propose transforms the  classification problem into the coding theory problem of decoding a noisy  codeword. An input vector in the feature space is transformed into an internal  representation which is a codeword in the code space, and then error correction  decoded in this space to classify the input feature vector to its class. Two classes  of codes which give high performance are the Hadamard matrix code and the  maximal length sequence code. We show that the number of classes stored in an  N-neuron system is linear in N and significantly more than that obtainable by  using the Hopfield type memory as a classifier.",https://papers.nips.cc/paper_files/paper/1987/file/642e92efb79421734881b53e1e1b18b6-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/642e92efb79421734881b53e1e1b18b6-Abstract.html,Machine Learning and Deep Learning
Microelectronic Implementations of Connectionist Neural Networks,"Stuart Mackie, Hans Graf, Daniel Schwartz, John Denker",1987,"In  this  paper  we  discuss  why  special  purpose  chips  are  needed  for  useful  implementations  of connectionist  neural  networks  in  such  applications  as  pattern  recognition  and  classification.  Three  chip  designs  are  described:  a  hybrid  digital/analog programmable  connection  matrix,  an  analog  connection  matrix  with  adjustable connection strengths, and a digital pipe lined best-match chip.  The common  feature  of the designs  is the distribution of arithmetic processing power amongst the  data storage to minimize data movement.",https://papers.nips.cc/paper_files/paper/1987/file/6512bd43d9caa6e02c990b0a82652dca-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/6512bd43d9caa6e02c990b0a82652dca-Abstract.html,AI and Applications
Analysis of Distributed Representation of Constituent Structure in Connectionist Systems,Paul Smolensky,1987,"A  general  method, the  tensor product representation, is described for  the distributed representation of  value/variable bindings.  The method allows the fully distributed representation of symbolic structures:  the roles  in  the structures, as well as the fillers  for  those roles, can be arbitrarily non-local.  Fully and  partially localized  special cases reduce to existing cases of connectionist representations of structured  data;  the  tensor  product  representation  generalizes  these  and  the  few  existing  examples  of  fuUy  distributed  representations  of structures.  The  representation  saturates  gracefully  as  larger  structures  are  represented;  it penn its  recursive  construction  of complex  representations  from  simpler  ones;  it  respects  the  independence  of the  capacities  to generate and  maintain  multiple bindings in  parallel;  it  extends naturally to continuous structures and continuous representational patterns; it pennits values to  also  serve  as  variables;  it  enables  analysis  of  the  interference  of  symbolic  structures  stored  in  associative  memories;  and  it  leads  to characterization  of optimal  distributed representations  of roles  and a recirculation algorithm for learning them.",https://papers.nips.cc/paper_files/paper/1987/file/66f041e16a60928b05a7e228a89c3799-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/66f041e16a60928b05a7e228a89c3799-Abstract.html,AI and Applications
Hierarchical Learning Control - An Approach with Neuron-Like Associative Memories,"Enis ErsÃ¼, Henning Tolle",1987,Advances,https://papers.nips.cc/paper_files/paper/1987/file/67c6a1e7ce56d3d6fa748ab6d9af3fd7-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/67c6a1e7ce56d3d6fa748ab6d9af3fd7-Abstract.html,Machine Learning and Deep Learning
Presynaptic Neural Information Processing,L. Carley,1987,"The  potential  for  presynaptic  information  processing  within  the  arbor  of a  single  axon  will  be  discussed  in  this  paper.  Current  knowledge  about  the  activity  dependence  of  the  firing  threshold,  the  conditions  required  for  conduction  failure,  and  the  similarity  of  nodes  along  a  single  axon  will  be  reviewed.  An  electronic  circuit  model  for  a  site  of low  conduction  safety  in  an  axon  will  be  presented.  In  response to  single  frequency  stimulation  the  electronic circuit acts  as  a  lowpass filter.",https://papers.nips.cc/paper_files/paper/1987/file/68d30a9594728bc39aa24be94b319d21-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/68d30a9594728bc39aa24be94b319d21-Abstract.html,Ethics and Fairness in AI
An Optimization Network for Matrix Inversion,"Ju-Seog Jang, Soo-Young Lee, Sang-Yung Shin",1987,"Inverse  matrix  calculation  can  be  considered  as  an  optimization.  We  have  demonstrated  that  this  problem  can  be  rapidly  solved  by  highly  interconnected  simple  neuron-like  analog  processors.  A  network  for  matrix  inversion  based  on  the  concept  of  Hopfield's  neural  network  was  designed,  and  implemented  with  electronic  hardware.  With  slight  modifications,  the  network  is  readily  applicable  to  solving  a  linear  simultaneous  equation  efficiently.  Notable  features  of  this  circuit  are  potential  speed  due  to  parallel  processing,  and  robustness  against  variations  of  device  parameters.",https://papers.nips.cc/paper_files/paper/1987/file/6c8349cc7260ae62e3b1396831a8398f-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/6c8349cc7260ae62e3b1396831a8398f-Abstract.html,Machine Learning and Deep Learning
Basins of Attraction for Electronic Neural Networks,"Charles Marcus, R. Westervelt",1987,"We  have  studied  the  basins  of  attraction  for  fixed  point  and  oscillatory  attractors  in  an  electronic  analog  neural  network.  Basin  measurement  circuitry  periodically  opens  the  network  feedback  loop,  loads  raster-scanned  initial  conditions  and  examines  the  resulting  attractor.  Plotting  the  basins  for  fixed  points  (memories),  we  show  that  overloading  an  associative  memory  network  leads  to  irregular  basin  shapes.  The  network  also  includes  analog  time  delay  circuitry,  and  we  have  shown  that  delay  in  symmetric  networks  can  introduce  basins  for  oscillatory  attractors.  Conditions  leading  to  oscillation  are  related  to  the  presence  of  frustration;  reducing  frustration  by  diluting  the  connections  can  stabilize  a  delay  network.",https://papers.nips.cc/paper_files/paper/1987/file/6ea9ab1baa0efb9e19094440c317e21b-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/6ea9ab1baa0efb9e19094440c317e21b-Abstract.html,AI and Applications
Programmable Synaptic Chip for Electronic Neural Networks,"Alexander Moopenn, H. Langenbacher, A. Thakoor, S. Khanna",1987,"A binary  synaptic  matrix  chip  has  been  developed  for  electronic  neural  networks.  The  matrix  chip  contains  a  programmable  32X32  array  of  ""long  channel""  NMOSFET  binary  connection  elements  imple(cid:173) mented  in  a  3-um  bulk  CMOS  process.  Since  the  neurons  are  kept  off(cid:173) chip,  the  synaptic  chip  serves  as  a  ""cascadable""  building  block  for  a  multi-chip  synaptic  network  as  large  as  512X512  in  size.  As  an  alternative  the  programmable  NMOSFET  (long  channel)  connection  elements,  tailored  thin  film  resistors  are  deposited,  in  series  with  FET  switches,  on  some  CMOS  test  chips,  to  obtain  the  weak  synaptic  connections.  Although  deposition  and  patterning  of  the  resistors  require  additional  they  promise  substantial  savings  in  silcon  area.  The  performance  of  a  synaptic  chip  in  a  32- neuron  breadboard  system  in  an  associative  memory  test  application  is  discussed.  processing  steps,  to",https://papers.nips.cc/paper_files/paper/1987/file/6f4922f45568161a8cdf4ad2299f6d23-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/6f4922f45568161a8cdf4ad2299f6d23-Abstract.html,AI and Applications
Learning a Color Algorithm from Examples,"Tomaso A. Poggio, Anya Hurlbert",1987,"A lightness algorithm that separates surface reflectance from illumination in a  Mondrian world is synthesized automatically from a set of examples, pairs of input  (image irradiance) and desired output (surface reflectance). The algorithm, which re(cid:173) sembles a new lightness algorithm recently proposed by Land, is approximately equiva(cid:173) lent to filtering the image through a center-surround receptive field in individual chro(cid:173) matic channels. The synthesizing technique, optimal linear estimation, requires only  one assumption, that the operator that transforms input into output is linear. This  assumption is true for a certain class of early vision algorithms that may therefore be  synthesized in a similar way from examples. Other methods of synthesizing algorithms  from examples, or ""learning"", such as backpropagation, do not yield a significantly dif(cid:173) ferent or better lightness algorithm in the Mondrian world. The linear estimation and  backpropagation techniques both produce simultaneous brightness contrast effects.  The problems that a visual system must solve in decoding two-dimensional images  into three-dimensional scenes (inverse optics problems) are difficult: the information  supplied by an image is not sufficient by itself to specify a unique scene. To reduce  the number of possible interpretations of images, visual systems, whether artificial  or biological, must make use of natural constraints, assumptions about the physical  properties of surfaces and lights. Computational vision scientists have derived effective  solutions for some inverse optics problems (such as computing depth from binocular  disparity) by determining the appropriate natural constraints and embedding them in  algorithms. How might a visual system discover and exploit natural constraints on its  own? We address a simpler question: Given only a set of examples of input images and  desired output solutions, can a visual system synthesize. or ""learn"", the algorithm that  converts input to output? We find that an algorithm for computing color in a restricted  world can be constructed from examples using standard techniques of optimal linear  estimation.  The computation of color is a prime example of the difficult problems of inverse  optics. We do not merely discriminate betwN'n different wavelengths of light; we assign  @ American Institute of Physics 1988  623  roughly constant colors to objects even though the light signals they send to our eyes  change as the illumination varies across space and chromatic spectrum. The compu(cid:173) tational goal underlying color constancy seems to be to extract the invariant surface  spectral reflectance properties from the image irradiance, in which reflectance and iI-""  lumination are mixed 1 â¢  Lightness algorithms 2-8, pioneered by Land, assume that the color of an object  can be specified by its lightness, or relative surface reflectance, in each of three inde(cid:173) pendent chromatic channels, and that lightness is computed in the same way in each  channel. Computing color is thereby reduced to extracting surface reflectance from the  image irradiance in a single chromatic channel.  The image irra.diance, s', is proportional to the product of the illumination inten(cid:173) sity e' and the surface reflectance r' in that channel:  (1 )  This form of the image intensity equation is true for a Lambertian reflectance model,  in which the irradiance s' has no specular components, and for appropriately chosen  color channels 9. Taking the logarithm of both sides converts it to a sum:  s' (x, y) = r' (x, y )e' (x, y).  s(x, y) = rex, y) + e(x,y),  (2)  where s = loges'), r = log(r') and e = log(e').  Given s(x,y) alone, the problem of solving Eq. 2 for r(x,y) is underconstrained.  Lightness algorithms constrain the problem by restricting their domain to a world of  Mondrians, two-dimensional surfaces covered with patches of random colors2 and by  exploiting two constraints in that world: (i) r'(x,y) is unifonn within patches but  has sharp discontinuities at edges between patches and (ii) e' (x, y) varies smoothly  across the Mondrian. Under these constraints, lightness algorithms can recover a good  approximation to r( x, y) and so can recover lightness triplets that label roughly constant  colors 10.  We ask whether it is possible to synthesize from examples an algorithm that exÂ·  tracts reflectance from image irradiance. and whether the synthesized algorithm will re(cid:173) semble existing lightness algorithms derived from an explicit analysis of the constraints.  We make one assumption, that the operator that transforms irradiance into reflectance  is linear. Under that assumption, motivated by considerations discussed later, we use  optimal linear estimation techniques to synthesize an operator from examples. The  examples are pairs of images: an input image of a Mondrian under illumination that  varies smoothly across space and its desired output image that displays the reflectance  of the Mondrian without the illumination. The technique finds the linear estimator  that best maps input into desired output. in the least squares sense.  For computational convenience we use one-dimensional ""training vectors"" that  represent vertical scan lines across the ~londrian images (Fig. 1). We generate many",https://papers.nips.cc/paper_files/paper/1987/file/70efdf2ec9b086079795c442636b55fb-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/70efdf2ec9b086079795c442636b55fb-Abstract.html,Theoretical Foundations
Generalization of Back propagation to Recurrent and Higher Order Neural Networks,Fernando Pineda,1987,A general method for deriving backpropagation algorithms for networks  with recurrent and higher order networks is introduced.  The propagation of activation  in these networks is determined by dissipative differential equations.  The error signal  is backpropagated by integrating an associated differential equation.  The method is  introduced by applying it to the recurrent generalization of the feedforward  backpropagation network.  The method is extended to the case of higher order  networks and to a constrained dynamical system for training a content addressable  memory.  The essential feature of the adaptive algorithms is that adaptive equation has  a simple outer product form.  Preliminary experiments suggest that learning can occur very rapidly in  networks with recurrent connections.  The continuous formalism makes the new  approach more suitable for implementation in VLSI.,https://papers.nips.cc/paper_files/paper/1987/file/735b90b4568125ed6c3f678819b6e058-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/735b90b4568125ed6c3f678819b6e058-Abstract.html,Machine Learning and Deep Learning
Neural Network Implementation Approaches for the Connection Machine,Nathan Brown,1987,"The SIMD parallelism of the Connection Machine (eM) allows the construction of 
neural network simulations by the use of simple data and control structures.  Two 
approaches are described which allow parallel computation of a model's nonlinear 
functions, parallel modification of a model's weights, and parallel propagation of a 
model's activation and error.  Each approach also allows a model's interconnect 
structure to be physically dynamic.  A Hopfield model is implemented with each 
approach at six sizes over the same number of CM processors to provide a performance 
comparison.",https://papers.nips.cc/paper_files/paper/1987/file/7647966b7343c29048673252e490f736-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/7647966b7343c29048673252e490f736-Abstract.html,Machine Learning and Deep Learning
On the Power of Neural Networks for Solving Hard Problems,"Jehoshua Bruck, Joseph Goodman",1987,"This  paper deals  with a  neural network model in  which each neuron  performs a  threshold logic function.  An important property of the model  is  that  it always  converges  to a  stable state  when  operating in  a  serial  mode [2,5].  This property is  the basis of the potential applications of the  model such as associative memory devices and combinatorial optimization  [3,6].  One of the motivations for use of the model for solving hard combinatorial  problems  is  the fact  that it can  be implemented  by optical devices  and  thus operate at a  higher speed  than conventional electronics.  The main theme in this work is  to investigate the power of the model for  solving NP-hard problems  [4,8],  and to understand  the relation  between  speed of operation and the size of a  neural network.  In particular, it will  be  shown  that  for  any  NP-hard  problem  the  existence  of a  polynomial  size  network  that  solves  it  implies  that  NP=co-NP.  Also,  for  Traveling  Salesman  Problem  (TSP), even  a  polynomial  size  network  that  gets  an  â¬-approximate  solution does  not exist unless  P=NP.  The above results  are of great  practical interest,  because  right  now it  is  possible  to build neural networks  which will  operate fast  but are limited  in  the number of neurons.",https://papers.nips.cc/paper_files/paper/1987/file/7cbbc409ec990f19c78c75bd1e06f215-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/7cbbc409ec990f19c78c75bd1e06f215-Abstract.html,Machine Learning and Deep Learning
"HOW THE CATFISH TRACKS ITS PREY: AN INTERACTIVE ""PIPELINED"" PROCESSING SYSTEM MAY DIRECT FORAGING VIA RETICULOSPINAL NEURONS",Jagmeet S. Kanwal,1987,of,https://papers.nips.cc/paper_files/paper/1987/file/7f39f8317fbdb1988ef4c628eba02591-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/7f39f8317fbdb1988ef4c628eba02591-Abstract.html,AI and Applications
Phasor Neural Networks,AndrÃ© Noest,1987,"A novel  network  type  is  introduced  which  uses  unit-length  2-vectors  for  local  variables.  As  an  example  of  its  applications,  associative  memory  nets  are  defined  and  their  performance  analyzed.  Real  systems  corresponding  to  such  'phasor'  models  can  be  e.g.  (neuro)biological  networks  of  limit-cycle  oscillators  or  optical  resonators  that  have  a  hologram  in  their  feedback  path.",https://papers.nips.cc/paper_files/paper/1987/file/8613985ec49eb8f757ae6439e879bb2a-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/8613985ec49eb8f757ae6439e879bb2a-Abstract.html,AI and Applications
Computing Motion Using Resistive Networks,"Christof Koch, Jin Luo, Carver Mead, James Hutchinson",1987,Abstract Unavailable,https://papers.nips.cc/paper_files/paper/1987/file/8e296a067a37563370ded05f5a3bf3ec-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/8e296a067a37563370ded05f5a3bf3ec-Abstract.html,Ethics and Fairness in AI
Experimental Demonstrations of Optical Neural Computers,"Ken Hsu, David Brady, Demetri Psaltis",1987,We  describe  two  expriments  in  optical  neural  computing.  In  the  first  a  closed  optical  feedback  loop  is  used  to  implement  auto-associative  image  recall.  In the second a perceptron-Iike learning algorithm is  implemented with  photorefractive holography.,https://papers.nips.cc/paper_files/paper/1987/file/8f14e45fceea167a5a36dedd4bea2543-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/8f14e45fceea167a5a36dedd4bea2543-Abstract.html,Machine Learning and Deep Learning
MURPHY: A Robot that Learns by Doing,Bartlett Mel,1987,"MURPHY consists of a camera looking at a robot arm, with a connectionist network  architecture situated in between. By moving its arm through a small, representative  sample of the 1 billion possible joint configurations, MURPHY learns the relationships,  backwards and forwards, between the positions of its joints and the state of its visual field.  MURPHY can use its internal model in the forward direction to ""envision"" sequences  of actions for planning purposes, such as in grabbing a visually presented object, or in  the reverse direction to ""imitate"", with its arm, autonomous activity in its visual field.  Furthermore, by taking explicit advantage of continuity in the mappings between visual  space and joint space, MURPHY is able to learn non-linear mappings with only a single  layer of modifiable weights.",https://papers.nips.cc/paper_files/paper/1987/file/92cc227532d17e56e07902b254dfad10-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/92cc227532d17e56e07902b254dfad10-Abstract.html,Machine Learning and Deep Learning
SPONTANEOUS AND  INFORMATION-TRIGGERED SEGMENTS OF SERIES OF HUMAN BRAIN ELECTRIC FIELD MAPS,"D. Lehmann, D. Brandeis, A. Horst, H. Ozaki, I. Pal",1987,"The brain works in a state-dependent manner: processin9  strate9ies and access to stored information depends on the momentary  functional state which is continuously re-adjusted. The state is  manifest as spatial confi9uration of the brain electric field.  Spontaneous and information-tri9gered brain electric activity is a  series of momentary field maps. Adaptive segmentation of spontaneous  series into spatially stable epochs (states) exhibited 210 msec mean  segments, discontinuous changes. Different maps imply different  active neural populations, hence expectedly different effects on  information processing: Reaction time differred between map classes  at stimulus arrival. Segments might be units of brain information  processin9 (content/mode/step), possibly operationalizin9  consciousness time. Related units (e.9. tri9gered by stimuli durin9  fi9ure perception and voluntary attention) mi9ht specify brain sub(cid:173) mechanisms of information treatment.  BRAIN FUNCTIONAL STATES AND THEIR CHANGES  The momentary functional state of the brain is reflected by the  confi9uration of the brain's electro-ma9netic field. The state  manifests the strate9Y, mode, step and content of brain information  processing, and the state constrains the choice of strate9ies and  modes and the access to memory material available for processin9 of  incoming information (1). The constraints include the available  range of changes of state in PAVLOV's classical Â·orienting reaction""  as response to new or important informations. Different states mi9ht  be viewed as different functional connectivities between the neural  elements.  The orienting reaction (see 1,2) is the result of the first  (Mpre-attentiveM) stage of information processing. This stage  operates automatically (no involvement of consciousness) and in a  parallel mode, and quickly determines whether (a) the information is  important or unknown and hence requires increased attention and  alertness, i.e. an orienting reaction which means a re-adjustment of  functional state in order to deal adequately with the information  invokin9 consciousness for further processing, or whether (b) the  information is known or unimportant and hence requires no re(cid:173) adjustment of state, i.e. that it can be treated further with well- Â© American Institute of Physics 1988  468  established (Â·automaticÂ·) strategies. Conscious strategies are slow  but flexible (offer wide choice), automatic strategies are fast but  rigid.  Examples for functional states on a gross scale are wakefulness,  drowsin.ss and sleep in adults, or developmental stages as infancy,  childhood and adolesc.nce, or drug states induced by alcohol or  other psychoactive agent â¢â¢ The different states are associated with  distinctly different ways of information processing. For example, in  normal adults, reality-close, abstracting strategies based on causal  relationships predominate during wakefulness, whereas in drowsiness  and sleep (dreams), reality-remote, visualizing, associative  concatenations of contents are used. Other well-known examples are  drug states.  HUMAN BRAIN ELECTRIC FIELD DATA AND STATES  While alive, the brain produces an ever-changing el.ctromagnetic  fi.ld, which very sensitively reflects global and local states as  effected by spontaneous activity, incoming information, metabolism,  drugs, and diseases. The .lectric component of the brain~s electro(cid:173) magnetic field as non-invasively measured from the intact human  scalp shows voltages between 0.1 and 250 microVolts, temporal  fr.quencies between 0.1 and 30, 100 or 3000 Hz depending on the  examined function, and spatial frequencies up to 0.2 cycles/em.  Brain electric field data are traditionally viewed as time series  of potential differences betwe.n two scalp locations (the  electroencephalogram or EE6). Time series analysis has offered an  effective way to class different gross brain functional states,  typically using EE6 power spectral values. Differences between power  spectra during different gross states typically are greater than  between different locations. States of lesser functional complexity  such as childhood vs adult states, sleep vs wakefulness, and many  drug-state. vs non-drug states tend to increased power in slower  frequencies (e.g. 1,4).  Time series analyses of epochs of intermediate durations between  30 and 10 seconds have demonstrated (e.g. 1,5,6) that there are  significant and reliable relations between spectral power or  coh.rency values of EE6 and characteristics of human mentation  (reality-close thoughts vs free associations, visual vs non-visual  thoughts, po.itive vs negative ~otions).  Viewing brain electric field data as series of momentary field  maps (7,8) opens the possibility to investigate the temporal  microstructure of brain functional states in the sub-second range.  The rationale is that the momentary configuration of activated  neural elements represents a given brain functional state, and that  the spatial pattern of activation is reflected by the momentary  brain electric field which is recordable on the scalp as a momentary  field map. Different configurations of activation (different field  maps) are expected to be associated with different modes,  strategies, steps and contents of information processing.  SE(J1ENTATI~ OF BRAIN ELECTRIC HAP SERIES INTO STABLE SE(J1ENTS  469  When Viewing brain electric activity as series of maps of  momentary potential distributions, changes of functional state are  recognizable as changes of the Â·electric landscapesÂ· of these maps.  Typically, several successive maps show similar landscapes, then  quickly change to a new configuration which again tends to persist  for a number of successive maps, suggestive of stable states  concatenated by non-linear transitions (9,10). Stable map landscapes  might be hypothesized to indicate the basic building blocks of  information processing in the brain, the -atoms of thoughtsÂ·. Thus,  the task at hand is the recognition of the landscape configurations;  this leads to the adaptive segmentation of time series of momentary  maps into segments of stable landscapes during varying durations.  We have proposed and used a method which describes the  configuration of a momentary map by the locations of its maximal and  minimal potential values, thus invoking a dipole model. The goal  here is the phenomenological recognition of different momentary  functional states using a very limited number of major map features  as classifiers, and we suggest conservative interpretion of the data  as to real brain locations of the generating processes which always  involve millions of neural elements.  We have studied (11) map series recorded from 16 scalp locations  over posterior skull areas from normal subjects during relaxation  with closed eyes. For adaptive segmentation, the maps at the times  of maximal map relief were selected for optimal signal/nOise  conditions. The locations of the maximal and minimal (extrema)  potentials were extracted in each map as descriptors of the  landscape; taking into account the basically periodic nature of  spontaneous brain electric activity (Fig. 1), extrema locations were  treated disregarding polarity information. If over time an extreme  left its pre-set spatial window (say, one electrode distance), the  segment was terminated. The map series showed stable map  configurations for varying durations (Fig. 2), and discontinuous,  step-wise changes. Over 6 subjects, resting alpha-type EEG showed  210 msec mean segment duration; segments longer than 323 msec  covered 50% of total time; the most prominent segment class (1.5% of  all classes) covered 20% of total time (prominence varied strongly  over classes; not all possible classes occurred). Spectral power and  phase of averages of adaptive and pre-determined segments  demonstrated the adequacy of the strategy and the homogeneity of  adaptive segment classes by their reduced within-class variance.  Segmentation using global map dissimilarity (sum of Euklidian  difference vs average reference at all measured points) emulates the  results of the extracted-characteristics-strategy.  FUNCTIONAL SIGNIFICANCE OF MOMENTARY MICRO STATES  Since different maps of momentary EEG fields imply activity of  different neural populations, different segment classes must  manifest different brain functional states with expectedly different",https://papers.nips.cc/paper_files/paper/1987/file/93db85ed909c13838ff95ccfa94cebd9-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/93db85ed909c13838ff95ccfa94cebd9-Abstract.html,Theoretical Foundations
Simulations Suggest Information Processing Roles for the Diverse Currents in Hippocampal Neurons,Lyle Borg-Graham,1987,"A computer model of the hippocampal pyramidal cell (HPC) is  described  which  integrates  data from  a  variety  of sources  in order  to  develop  a  con(cid:173) sistent description for  this cell  type.  The model  presently includes  descrip(cid:173) tions  of eleven non-linear somatic currents of the HPC, and the electrotonic  structure of the neuron is modelled with a soma/short-cable approximation.  Model simulations qualitatively or quantitatively reproduce a  wide range of  somatic electrical behavior i~ HPCs, and demonstrate possible  roles  for the  various currents in information  processing.  1  The  Computational Properties of Neurons  There  are  several  substrates  for  neuronal  computation,  including  connec(cid:173) tivity, synapses,  morphometries of dendritic  trees,  linear parameters  of cell  membrane, as well as non-linear, time-varying membrane conductances, also  referred  to as  currents or channels.  In  the classical  description  of neuronal  function,  the contribution  of membrane channels  is  constrained  to  that  of  generating the action potential, setting firing  threshold, and establishing the  relationship  between (steady-state)  stimulus  intensity and  firing  frequency.  However,  it is  becoming clear that  the role  of these channels  may  be much  more complex, resulting in a variety of novel ""computational operators"" that  reflect  the information  processing occurring in  the biological neural  net.  Â© American Institute of Physics 1988  83  2  Modelling  Hippocampal Neurons  Over the  past  decade  a  wide  variety of non-linear  ion channels,  have  been  described  for  many  excitable  cells,  in  particular  several  kinds  of neurons.  One  such  neuron  is  the  hippocampal  pyramidal  cell  (HPC).  HPC  chan(cid:173) nels  are  marked  by  their  wide  range  of temporal,  voltage-dependent,  and  chemical-dependent characteristics,  which  results  in very complex  behavior  or  responses  of these  stereotypical  cortical  integrating cells.  For example,  some HPC channels are activated (opened)  transiently and quickly, thus pri(cid:173) marily affecting  the action  potential shape.  Other channels  have longer  ki(cid:173) netics, modulating the response of HPCs over hundreds of milliseconds.  The  measurement  these  channels  is  hampered  by  various  technical  constraints,  including the small size and extended electrotonic structure of HPCs and the  diverse  preparations  used  in experiments.  Modelling the electrical  behavior  of HPCs  with computer simulations is  one method  of integrating data from  a  variety of sources in order to develop  a  consistent description for  this  cell  type.  In the model referred to here putative mechanisms for  voltage-dependent  and  calcium-dependent  channel gating  have  been  used  to generate  simula(cid:173) tions of the somatic electrical behavior of HPCs, and to suggest mechanisms  for  information processing at  the single cell  level.  The model  has  also  been  used  to suggest experimental protocols  designed  to test  the validity of sim(cid:173) ulation results.  Model simulations qualitatively or quantitatively reproduce  a  wide  range of somatic electrical behavior in HPCs, and  explicitly  demon(cid:173) strate possible functional  roles  for  the various currents [1].  The model  presently includes  descriptions  of eleven  non-linear somatic  currents,  including  three  putative  N a+  currents  - INa-trig,  INa-rep,  and  INa-tail;  six  K+  currents  that  have  been  reported  in  the  literature - IDR  (Delayed  Rectifier),  lA,  Ie,  IAHP  (After-hyperpolarization),  1M,  and  IQ;  and two Ca2+ currents, also reported  previously - lea  and  leas.  The  electrotonic  structure  of the  HPC  is  modelled  with  a  soma/short(cid:173) cable approximation, and  the dendrites are assumed to be linear.  While the  conditions  for  reducing  the dendritic  tree  to a  single  cable are  not  met  for  HPC  (the so-called Rall conditions  [3]),  the  Zin  of the cable is  close  to that  of the tree.  In addition, although HPC dendrites have non-linear membrane,  it  assumed  that  as  a  first  approximation  the contribution of currents  from  this membrane may be ignored in the somatic response  to somatic stimulus.  Likewise,  the  model structure assumes  that axon-soma current  under these  conditions can be lumped into the soma circuit.  84  In  part  this  paper  will  address  the  following  question:  if  neural  nets  are realizable using elements that  have simple integrative all-or-nothing re(cid:173) sponses,  connected  to each other  with  regenerative  conductors,  then  what  is  the function  for all  the channels observed experimentally in real neurons?  The results  of this  HPC  model study suggest  some  purpose for  these com(cid:173) plexities, and in this  paper we  shall investigate some of the possible roles  of  non-linear channels in neuronal information processing.  However, given the  speculative  nature  of many of the  currents  that  we  have  presented  in  the  model, it is  important to view  results  based  on  the interaction of the many  model  elements as  preliminary.  3  Defining Neural Information Coding is the First  Step  in  Describing  Biological Computations  Determination of computational properties  of neurons  requires  a  priori as(cid:173) sumptions  as  to how information is  encoded in  neuronal  output.  The clas(cid:173) sical  description  assumes  that  information  is  encoded  as  spike  frequency.  However,  a  single  output variable,  proportional to firing  frequency,  ignores  other potentially information-rich degrees  of freedom,  including:  â¢  Relative phase of concurrent inputs.  â¢  Frequency modulation during single  bursts.  â¢  Cessation of firing  due to intrinsic mechanisms.",https://papers.nips.cc/paper_files/paper/1987/file/9778d5d219c5080b9a6a17bef029331c-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/9778d5d219c5080b9a6a17bef029331c-Abstract.html,Theoretical Foundations
An Artificial Neural Network for Spatio-Temporal Bipolar Patterns: Application to Phoneme Classification,"Les Atlas, Toshiteru Homma, Robert Marks",1987,"An  artificial  neural  network  is  developed  to  recognize  spatio-temporal  bipolar patterns  associatively.  The  function  of a formal  neuron is  generalized by  replacing  multiplication  with  convolution,  weights  with  transfer  functions,  and  thresholding  with  nonlinear  transform  following  adaptation.  The Hebbian  learn(cid:173) ing  rule  and  the  delta  learning  rule  are  generalized  accordingly,  resulting  in  the  learning  of weights  and  delays.  The  neural  network  which  was  first  developed  for  spatial  patterns  was  thus  generalized  for  spatio-temporal  patterns.  It  was  tested  using  a  set  of bipolar input patterns  derived from  speech  signals,  showing  robust classification of 30 model phonemes.",https://papers.nips.cc/paper_files/paper/1987/file/98f13708210194c475687be6106a3b84-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/98f13708210194c475687be6106a3b84-Abstract.html,
Teaching Artificial Neural Systems to Drive: Manual Training Techniques for Autonomous Systems,"J. F. Shepanski, S. A. Macy",1987,Abstract Unavailable,https://papers.nips.cc/paper_files/paper/1987/file/9a1158154dfa42caddbd0694a4e9bdc8-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/9a1158154dfa42caddbd0694a4e9bdc8-Abstract.html,Machine Learning and Deep Learning
Correlational Strength and Computational Algebra of Synaptic Connections Between Neurons,Eberhard Fetz,1987,"Intracellular  recordings  in  spinal  cord  motoneurons  and  cerebral  cortex neurons have provided new evidence on the correlational strength of  monosynaptic  connections,  and  the  relation  between  the  shapes  of  postsynaptic  potentials  and  the  associated  increased  firing  probability.  In  these  cells,  excitatory  postsynaptic  potentials  (EPSPs)  produce  cross(cid:173) correlogram peaks  which resemble  in large part the derivative of  the EPSP.  Additional  synaptic  noise broadens  the peak,  but the  peak  area  -- i.e.,  the  number of above-chance firings triggered per EPSP -- remains proportional to  the EPSP  amplitude.  A typical EPSP of 100  ~v triggers about .01  firings per  EPSP.  The  consequences  of  these  data  for  information  processing  by  polysynaptic connections is discussed.  The effects of sequential polysynaptic  links  can  be  calculated  by  convolving  the  effects  of  the  underlying  monosynaptic connections.  The net effect of parallel pathways is the sum of  the individual contributions.",https://papers.nips.cc/paper_files/paper/1987/file/9bf31c7ff062936a96d3c8bd1f8f2ff3-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/9bf31c7ff062936a96d3c8bd1f8f2ff3-Abstract.html,Machine Learning and Deep Learning
"Discovering Structure from Motion in Monkey, Man and Machine",Ralph Siegel,1987,"The ability to obtain three-dimensional structure from visual motion is  important for survival of human and non-human primates. Using a parallel process(cid:173) ing model, the current work explores how the biological visual system might solve  this problem and how the neurophysiologist might go about understanding the  solution.",https://papers.nips.cc/paper_files/paper/1987/file/9f61408e3afb633e50cdf1b20de6f466-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/9f61408e3afb633e50cdf1b20de6f466-Abstract.html,Ethics and Fairness in AI
Static and Dynamic Error Propagation Networks with Application to Speech Coding,"A. Robinson, F. Fallside",1987,"Error propagation nets have been shown to be able to learn a variety of tasks in  which a static input pattern is mapped outo a static output pattern. This paper  presents a generalisation of these nets to deal with time varying, or dynamic  patterns, and three possible architectures are explored. As an example, dynamic  nets are applied to tbe problem of speech coding, in which a time sequence of  speech data are coded by one net and decoded by another. The use of dynamic  nets gives a better signal to noise ratio than that achieved using static nets.",https://papers.nips.cc/paper_files/paper/1987/file/a1d0c6e83f027327d8461063f4ac58a6-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/a1d0c6e83f027327d8461063f4ac58a6-Abstract.html,
Schema for Motor Control Utilizing a Network Model of the Cerebellum,James Houk,1987,"This  paper  outlines  a  schema  for  movement  control  based  on  two  stages  of  signal  processing.  The  higher  stage  is  a  neural  network  model  that  treats  the  cerebellum  as  an  array  of  adjustable  motor  pattern  generators.  This  network  uses  sensory  input  to  preset  and  to  trigger  elemental  pattern  generators  and  to  evaluate  their  performance.  The  actual  patterned  outputs,  however,  are  produced  by  intrin(cid:173) sic  circuitry  that  includes  recurrent  loops  and  is  thus  capable  of  self-sustained  activity.  These  patterned  outputs  are  sent  as  motor  commands  to  local  feedback  systems  called  motor  servos.  The  latter  control  the  forces  and  lengths  of  individual  muscles.  Overall  control  is  thus  achieved  in  two  stages:  (1)  an  adaptive  cerebellar  network  generates  an  array  of  feedforward  motor  commands  and  (2)  a  set  of  local  feedback  systems  translates  these  commands  into  actual  movements.",https://papers.nips.cc/paper_files/paper/1987/file/a3f390d88e4c41f2747bfa2f1b5f87db-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/a3f390d88e4c41f2747bfa2f1b5f87db-Abstract.html,
Distributed Neural Information Processing in the Vestibulo-Ocular System,"Clifford Lau, Vicente Honrubia",1987,"A new distributed neural information-processing  model is proposed to explain the response characteristics  of the vestibulo-ocular system and to reflect more  accurately the latest anatomical and neurophysiological  data on the vestibular afferent fibers and vestibular nuclei.  In this model, head motion is sensed topographically by hair  cells in the semicircular canals. Hair cell signals are then  processed by multiple synapses in the primary afferent  neurons which exhibit a continuum of varying dynamics. The  model is an application of the concept of ""multilayered""  neural networks to the description of findings in the  bullfrog vestibular nerve, and allows us to formulate  mathematically the behavior of an assembly of neurons  whose physiological characteristics vary according to their  anatomical properties.",https://papers.nips.cc/paper_files/paper/1987/file/a5771bce93e200c36f7cd9dfd0e5deaa-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/a5771bce93e200c36f7cd9dfd0e5deaa-Abstract.html,
Time-Sequential Self-Organization of Hierarchical Neural Networks,"Ronald Silverman, Andrew Noetzel",1987,"Self-organization  of  multi-layered  networks  can  be  realized  time-sequential  organization  of  successive  neural  layers.  by  Lateral  inhibition  operating  in  the  surround  of  firing  cells  in  each  for  unsupervised  capture  of  excitation  patterns  presented  by  the  previous  layer.  By  presenting  patterns  of  organization,  higher  implicit  in  the  pattern  set.  in  co-ordination  with  network  self(cid:173) the  hierarchy  capture  concepts  increasing  complexity,",https://papers.nips.cc/paper_files/paper/1987/file/a5bfc9e07964f8dddeb95fc584cd965d-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/a5bfc9e07964f8dddeb95fc584cd965d-Abstract.html,
A Method for the Design of Stable Lateral Inhibition Networks that is Robust in the Presence of Circuit Parasitics,"John Wyatt, D. Standley",1987,"In  the  analog  VLSI  implementation  of  neural  systems,  it is  sometimes  convenient  to  build  lateral  inhibition  networks  by  using  a  locally  connected  on-chip  resistive  grid.  A  serious  problem  of  unwanted  spontaneous  oscillation  often  arises  with  these  circuits  and  renders  them  unusable  in  practice.  This  paper  reports  a  design  approach  that  guarantees  such  a  system  will  be  stable,  even  though  the  values  of  designed  elements  and  parasitic  elements  in  the  resistive  grid  may  be  unknown.  The  method  is  based  on  a  rigorous,  somewhat  novel  mathematical  analysis  using  Tellegen's  theorem  and  the  idea  of  Popov  multipliers  from  control  theory.  It  is  thoroughly  practical  because  the  criteria  are  local  in  the  sense  that  no  overall  analysis  of  the  interconnected  system  is  required,  empirical  in  the  sense  that  they  involve  only  measurable  frequency  response  data  on  the  individual  cells,  and  robust  in  the  sense  that  unmodelled  parasitic  resistances  and  capacitances  in  the  inter(cid:173) connection  network  cannot  affect  the  analysis.  I.",https://papers.nips.cc/paper_files/paper/1987/file/a684eceee76fc522773286a895bc8436-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/a684eceee76fc522773286a895bc8436-Abstract.html,
Constrained Differential Optimization,"John Platt, Alan Barr",1987,"Many optimization models of neural  networks need constraints to restrict the space of outputs to  a subspace which satisfies external criteria.  Optimizations using energy methods yield ""forces"" which  act upon  the  state of the  neural  network.  The penalty method, in which quadratic  energy  constraints  are  added  to  an  existing  optimization  energy,  has  become  popular  recently,  but  is  not  guaranteed  to satisfy  the  constraint conditions  when  there  are  other forces  on  the  neural  model  or when  there  are  multiple constraints.  In this paper, we present the basic differential multiplier method (BDMM),  which  satisfies constraints exactly;  we  create forces  which gradually apply  the constraints over time,  using ""neurons"" that estimate Lagrange multipliers.  The  basic  differential  multiplier  method  is  a  differential  version  of the  method  of multipliers  from  Numerical Analysis.  We  prove  that the differential  equations locally converge  to  a constrained  minimum.  Examples of applications of the differential method of multipliers include enforcing permutation  codewords in the analog decoding problem and enforcing valid tours in the traveling salesman problem.",https://papers.nips.cc/paper_files/paper/1987/file/a87ff679a2f3e71d9181a67b7542122c-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/a87ff679a2f3e71d9181a67b7542122c-Abstract.html,
Encoding Geometric Invariances in Higher-Order Neural Networks,"C. Giles, R. Griffin, T. Maxwell",1987,"We  describe  a  method  of  constructing  higher-order  neural  networks  that  respond  invariantly  under  geometric  transformations  on  the  input  space.  By  requiring  each  unit  to  satisfy  a  set  of  constraints  on  the  interconnection  weights,  a  particular  structure  is  imposed  on  the  network.  A  network  built  using  such  an  architecture  maintains  its  invariant  performance  independent  of  the  values  the  weights  assume,  of  the  learning  rules  used,  and  of  the  form  of  the  nonlinearities  in  the  network.  The  invariance  exhibited  by  a  first(cid:173) order  network  is  usually  of  a  trivial  sort,  e.g.,  responding  only  to  the  average  input  in  the  case  of  translation  invariance,  whereas  higher-order  networks  can  perform  useful  functions  and  still  exhibit  the  invariance.  We  derive  the  weight  constraints  for  translation,  rotation,  scale,  and  several  combinations  of  these  transformations,  and  report  results  of  simulation  studies.",https://papers.nips.cc/paper_files/paper/1987/file/aab3238922bcc25a6f606eb525ffdc56-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/aab3238922bcc25a6f606eb525ffdc56-Abstract.html,
A Novel Net that Learns Sequential Decision Process,"Guo-Zheng Sun, Yee-Chun Lee, Hsing-Hen Chen",1987,We propose a  new  scheme  to construct  neural networks  to classify  pat(cid:173) terns.  The new scheme has several novel features  :,https://papers.nips.cc/paper_files/paper/1987/file/ad61ab143223efbc24c7d2583be69251-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/ad61ab143223efbc24c7d2583be69251-Abstract.html,
Mathematical Analysis of Learning Behavior of Neuronal Models,"John Cheung, Massoud Omidvar",1987,"In  this  paper,  we  wish  to  analyze  the  convergence  behavior  of a  number  of neuronal plasticity models.  Recent neurophysiological research suggests that  the neuronal behavior is adaptive.  In particular, memory stored within a neuron  is  associated with the synaptic weights which are varied or adjusted to achieve  learning.  A  number  of adaptive  neuronal  models  have  been  proposed  in  the  literature.  Three specific models will be analyzed in this paper, specifically the  Hebb model, the Sutton-Barto model, and the most recent trace model.  In this  paper we  will  examine  the conditions  for  convergence,  the  position  of conver(cid:173) gence and the rate at convergence,  of these models  as they applied to classical  conditioning.  Simulation results  are also presented to verify the analysis.",https://papers.nips.cc/paper_files/paper/1987/file/b53b3a3d6ab90ce0268229151c9bde11-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/b53b3a3d6ab90ce0268229151c9bde11-Abstract.html,
New Hardware for Massive Neural Networks,"Darryl Coon, A. Perera",1987,"Transient phenomena associated with forward biased silicon p + - n - n + struc(cid:173) tures at 4.2K show remarkable similarities with biological neurons.  The devices  play  a  role  similar to the  two-terminal switching elements in  Hodgkin-Huxley  equivalent  circuit  diagrams.  The  devices  provide simpler  and  more  realistic  neuron  emulation  than  transistors  or op-amps.  They  have  such  low  power  and  current  requirements  that  they  could  be  used  in  massive  neural  networks.  Some  observed  properties  of  simple  circuits  containing  the  devices  include  action  potentials,  refractory  periods,  threshold behavior, excitation, inhibition, summation over synaptic inputs, synaptic  weights,  temporal  integration, memory,  network  connectivity modification  based  on  experience, pacemaker activity, firing  thresholds, coupling to sensors with graded sig(cid:173) nal  outputs  and  the  dependence  of firing  rate  on input  current.  Transfer functions  for simple artificial neurons with spiketrain inputs and spiketrain outputs have been  measured  and correlated with input coupling.",https://papers.nips.cc/paper_files/paper/1987/file/b6d767d2f8ed5d21a44b0e5886680cb9-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/b6d767d2f8ed5d21a44b0e5886680cb9-Abstract.html,
An Adaptive and Heterodyne Filtering Procedure for the Imaging of Moving Objects,"F. Schuling, H. Mastebroek, W. Zaagman",1987,"Recent experimental work on the stimulus velocity dependent time resolving  power of the neural units, situated in the highest order optic ganglion of the  blowfly, revealed the at first sight amazing phenomenon that at this high level of  the fly visual system, the time constants of these units which are involved in the  processing of neural activity evoked by moving objects, are -roughly spoken(cid:173) inverse proportional to the velocity of those objects over an extremely wide range.  In this paper we will discuss the implementation of a two dimensional heterodyne  adaptive filter construction into a computer simulation model. The features of this  simulation model include the ability to account for the experimentally observed  stimulus-tuned adaptive temporal behaviour of time constants in the fly visual  system. The simulation results obtained, clearly show that the application of such  an adaptive processing procedure delivers an improved imaging technique of  moving patterns in the high velocity range.  A FEW REMARKS ON THE FLY VISUAL SYSTEM",https://papers.nips.cc/paper_files/paper/1987/file/c0c7c76d30bd3dcaefc96f40275bdc0a-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/c0c7c76d30bd3dcaefc96f40275bdc0a-Abstract.html,
Phase Transitions in Neural Networks,Joshua Chover,1987,"Various  simulat.ions  of  cort.ical  subnetworks  have  evidenced  something  like  phase  transitions  with  respect  to  key  parameters.  We  demonstrate  that.  such  transi t.ions  must.  indeed  exist.  in  analogous  infinite  array  models.  For  related  finite  array  models  classical  phase  transi t.ions  (which  describe  steady-state  behavior)  may  not.  exist.,  but.  there  can  be  distinct. quali tative  changes  in  (""metastable"")  transient  behavior  as  key  system  parameters  pass  through  crit.ical  values .",https://papers.nips.cc/paper_files/paper/1987/file/c16a5320fa475530d9583c34fd356ef5-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/c16a5320fa475530d9583c34fd356ef5-Abstract.html,
Using Neural Networks to Improve Cochlear Implant Speech Perception,Manoel Tenorio,1987,"- An  increasing  number  of  profoundly  deaf  patients  suffering  from  sen- sorineural  deafness  are  using  cochlear  implants  as  prostheses.  Mter  the  implant,  sound  can  be  detected  through  the  electrical  stimulation  of  the  remaining  peripheral  auditory  nervous  system.  Although  great  progress  has  been  achieved  in  this  area,  no  useful  speech  recognition  has  been  attained  with either single  or multiple  channel cochlear implants.  Coding  evidence  suggests  that  it  is  necessary  for  any  implant  which  would  effectively  couple  with  the  natural  speech  perception  system  to simu(cid:173) late  the  temporal  dispersion  and  other  phenomena  found  in  the  natural  receptors,  and  currently  not  implemented  in  any  cochlear  implants.  To  this  end,  it  is  presented  here  a  computational  model  using  artificial  neural  net(cid:173) works  cochlear.  the  natural  phenomena",https://papers.nips.cc/paper_files/paper/1987/file/c20ad4d76fe97759aa27a0c99bff6710-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/c20ad4d76fe97759aa27a0c99bff6710-Abstract.html,
Self-Organization of Associative Database and Its Applications,"Hisashi Suzuki, Suguru Arimoto",1987,"An  efficient  method  of self-organizing  associative  databases  is  proposed  together  with  applications  to  robot  eyesight  systems.  The  proposed  databases  can  associate  any  input  with  some  output.  In  the  first  half part  of discussion,  an  algorithm of self-organization  is  proposed.  From  an  aspect  of  hardware,  it  produces  a  new  style  of  neural  network.  In  the  latter half part, an applicability to handwritten letter recognition and that to an autonomous  mobile  robot system are demonstrated.",https://papers.nips.cc/paper_files/paper/1987/file/c4ca4238a0b923820dcc509a6f75849b-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/c4ca4238a0b923820dcc509a6f75849b-Abstract.html,
Temporal Patterns of Activity in Neural Networks,Paolo Gaudiano,1987,"Patterns  of activity  over  real  neural  structures  are  known  to  exhibit  time(cid:173) dependent  behavior.  It  would  seem  that  the  brain  may  be  capable  of utilizing  temporal  behavior of activity in neural  networks as  a  way  of performing functions  which cannot otherwise be easily implemented.  These might include the origination  of sequential behavior and  the  recognition  of time-dependent  stimuli.  A  model  is  presented  here  which  uses  neuronal  populations  with  recurrent  feedback  connec(cid:173) tions in an attempt to observe and describe  the resulting time-dependent behavior.  Shortcomings and problems inherent  to  this model are  discussed.  Current models  by other  researchers  are  reviewed  and their similarities and differences  discussed.  METHODS  /  PRELIMINARY RESULTS  In previous  papers,[2,3]  computer models  were  presented  that  simulate a  net  con(cid:173) sisting of two spatially organized populations of realistic neurons.  The populations are  richly  interconnected  and  are  shown  to  exhibit  internally  sustained  activity.  It  was  shown that if the neurons have response times significantly shorter than the typical unit  time characteristic of the input  patterns  (usually  1 msec),  the populations will exhibit  time-dependent behavior.  This will typically result in the net falling into a  limit cycle.  By a limit cycle, it is meant that the population falls into activity patterns during which  all of the active cells  fire  in  a  cyclic,  periodic  fashion.  Although  the period of firing  of  the individual  cells  may be  different,  after  a  fixed  time  the  overall population activity  will  repeat  in  a  cyclic,  periodic  fashion.  For  populations  organized  in  7x7  grids,  the  limit  cycle  will  usually  start  20~200 msec  after  the input  is  turned off,  and  its period  will be in  the order of 20-100  msec.  The point ofinterest is that ifthe net is allowed to undergo synaptic modifications by  means  of a  modified  hebbian learning rule  while being presented with a  specific  spatial  pattern (i.e., cells at  specific  spatial locations within the net are externally stimulated),  subsequent  presentations  of the  same  pattern  with  different  temporal  characteristics  will cause the population to recall patterns which are spatially identical (the same cells  will be active)  but which have different  temporal qualities.  In other words, the net can  fall  into a  different  limit  cycle.  These limit cycles  seem  to behave as  attractors in that  similar input  patterns will result  in  the  same limit cycle,  and hence  each distinct limit  cycle  appears  to have a  basin of attraction.  Hence a  net  which can  only learn a  small  Â© American Institute of Physics 1988  298  number  of spatially  distinct  patterns  can recall  the  patterns in a  number  of temporal  modes.  If it were possible to quantitatively discriminate between such temporal modes,  it  would  seem  reasonable  to  speculate  that  different  limit  cycles  could  correspond  to  different  memory traces.  This would  significantly increase estimates on the  capacity of  memory  storage in the net.  It has  also been shown that a net being presented with a  given pattern will fall and  stay  into  a  limit  cycle  until another  pattern is  presented  which  will  cause  the  system  to fall  into a  different  basin of attraction.  If no other  patterns  are presented,  the  net  will remain in  the  same limit  cycle  indefinitely.  Furthermore, the net  will fall  into the  same  limit  cycle  independently  of the  duration  of the  input  stimulus,  so  long  as  the  input stimulus is  presented for  a  long enough time to raise the population activity level  beyond  a  minimum necessary  to  achieve  self-sustained  activity.  Hence,  if we  suppose  that  the  net  ""recognizes""  the input  when  it  falls  into the  corresponding  limit  cycle,  it  follows  that the net will recognize a string of input patterns regardless of the duration of  each input pattern, so long as each input is presented long enough for the net  to fall into  the  appropriate limit  cycle.  In particular,  our  system is  capable of falling  into  a  limit  cycle within some tens of milliseconds.  This can be fast enough to encode, for example, a  string of phonemes as would typically be found in continuous speech.  It may be possible,  for  instance,  to create a  model  similar  to Rumelhart  and McClelland's  1981  model  on  word recognition by appropriately connecting multiple layers  of these networks.  If the  response  time  of the  cells  were  increased  in  higher  layers,  it may  be  possible  to have  the  lowest  level  respond  to  stimuli  quickly  enough  to  distinguish  phonemes  (or  some  sub-phonemic basic linguistic unit), then have populations from this first  level feed  into  a  slower,  word-recognizing population layer,  and  so  On.  Such a  model  may  be  able  to  perform word  recognition from an input  consisting of continuous phoneme strings even  when  the phonemes  may vary in duration of presentation.",https://papers.nips.cc/paper_files/paper/1987/file/c51ce410c124a10e0db5e4b97fc2af39-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/c51ce410c124a10e0db5e4b97fc2af39-Abstract.html,
"Network Generality, Training Required, and Precision Required","John Denker, Ben Wittner",1987,"We  show  how  to estimate  (1)  the  number  of functions  that  can  be implemented  by  a  particular  network  architecture,  (2)  how  much  analog  precision  is  needed  in  the  con(cid:173) nections in the network, and (3) the number of training examples the network must see  before it can  be expected  to form  reliable  generalizations.  Generality versus Training  Data Required  Consider  the following  objectives:  First, the network  should be very  powerful and ver(cid:173) satile,  i.e.,  it  should  implement  any  function  (truth  table)  you  like,  and  secondly,  it  should learn easily, forming  meaningful generalizations from  a small number of training  examples.  Well, it is  information-theoretically impossible to create such a  network.  We  will  present  here a  simplified  argument; a  more complete and sophisticated version can  be found  in  Denker et al.  (1987).  It is  customary to regard learning as  a  dynamical process:  adjusting the weights  (etc.)  in  a  single  network.  In  order  to  derive  the  results  of  this  paper,  however,  we  take  a  different  viewpoint,  which  we  call  the  ensemble  viewpoint.  Imagine  making  a  very  large  number of replicas of the network.  Each  replica has  the same architecture as  the  original,  but  the  weights  are  set  differently  in  each  case.  No  further  adjustment  takes  place;  the  ""learning process""  consists  of winnowing the ensemble of replicas,  searching  for  the one( s)  that satisfy our requirements.  Training proceeds as follows:  We  present each item in  the training set to every network  in  the  ensemble.  That  is,  we  use  the  abscissa of the  training  pattern  as  input  to  the  network,  and  compare  the  ordinate of the  training  pattern  to see  if it  agrees  with  the  actual output  of the  network.  For  each  network,  we  keep  a  score  reflecting  how  many  times (and how badly) it disagreed with a  training item.  Networks with the lowest score  are  the  ones  that  agree  best  with  the  training data.  If we  had  complete  confidence  in  lCurrently  at  NYNEX  Science  and Technology,  500  Westchester Ave.,  White  Plains,  NY  10604  @)  American Institute of Physics 1988  220  the reliability of the training set, we  could at each step simply throwaway all  networks  that disagree.  For definiteness, let us  consider a typical network architecture, with  No input wires and  Nt  units in  each processing layer  I, for  I  E {IÂ·Â· Â·L}.  For simplicity  we  assume NL  =  1.  We  recognize  the  importance of networks  with  continuous-valued  inputs  and  outputs,  but  we  will  concentrate  for  now  on  training  (and  testing)  patterns  that  are  discrete,  with N  ==  No  bits of abscissa and N L =  1 bit of ordinate.  This allows  us to classify  the  networks  into  bins  according  to  what  Boolean  input-output  relation  they  implement,  and simply  consider the ensemble of bins.  If the  network  architecture  is  completely  general  and  There  are  22N  jossible  bins.  powerful,  all  22  functions  will  exist  in  the ensemble of bins.  On  average,  one  expects  that each  training item  will  throwaway  at  most  half of the  bins.  Assuming  maximal  efficiency,  if m  training items are  used,  then  when  m  ~ 2N  there  will  be only one  bin  remaining,  and  that  must  be  the  unique  function  that  consistently  describes  all  the  data.  But there  are  only  2N  possible abscissas  using N  bits.  Therefore a  truly general  network cannot possibly exhibit meaningful generalization - 100% of the possible data  is  needed for  training.  N ow  suppose that the network is not  completely general, so that even  with all  possible  settings of the weights we can only create functions in 250  bins, where So  < 2N.  We call  So  the initial entropy of the network.  A more formal  and general  definition is  given  in  Denker et al.  (1987).  Once again, we  can use the training data to winnow the ensemble,  and when  m  ~ So,  there will be only one remaining bin.  That function  will presumably  generalize correctly to the remaining 2N - m  possible patterns.  Certainly that function  is  the best we can do with the network architecture and the training data we were given.  The  usual  problem  with  automatic  learning  is  this:  If the  network  is  too  general,  So  will  be large, and an inordinate amount of training data will be required.  The required  amount of data may be simply unavailable, or it may be so large that training would be  prohibitively time-consuming.  The shows the critical importance of building a  network  that is  not more general  than  necessary.  Estimating the Entropy  In real engineering situations, it is  important to be able to estimate the initial entropy  of various proposed designs, since that determines the amount of training data that will  be required.  Calculating So  directly from  the definition is prohibitively difficult, but we  can  use the definition to derive useful  approximate expressions.  (You  wouldn't want to  calculate the thermodynamic entropy of a  bucket of water directly  from  the definition,  either. )  221  Suppose  that  the  weights  in  the  network  at  each  connection  i  were  not  continuously  adjustable real numbers, but rather were specified by a  discrete code with bi  bits.  Then  the total number of bits required  to specify  the configuration  of the network is  (1)  Now the total number offunctions that could possibly be implemented by such a network  architecture would  be at most 2B.  The actual number will  always be smaller than this,  since there are various ways in which different settings of the weights can lead to identical  functions  (bins).  For one  thing, for  each  hidden layer 1 E {1Â·Â·Â· L-1}, the numbering of  the  hidden units can be permuted, and the polarity of the hidden units can be flipped,  which  means  that  250  is  less  than  2B  by  a  factor  (among  others)  of III Nl! 2N ,.  In  addition,  if there  is  an  inordinately  large  number  of bits  bi  at  each  connection,  there  will  be  many  settings  where  small  changes  in  the connection  will  be immaterial.  This  will  make 2so  smaller by an additional factor.  We expect  aSO/abi  ~ 1 when bi is small,  and aSO/abi  ~ 0 when  bi  is  large;  we  must now  figure  out  where the crossover occurs.  The number of ""useful and significant"" bits of precision, which we designate b   ~ log(N/S).  Note:  our  calculation  does  not  involve  the  dynamics  of  the  learning  process.  Some  numerical methods (including versions of back propagation) commonly require a number  of temporary  ""guard bits""  on  each  weight,  as  pointed  out  by  llichard  Durbin  (private  communication).  Another log N  bits ought  to suffice.  These bits  are  not  needed  after  learning is  complete, and do not contribute to So.  If we combine these ideas and apply them to a  network with N  units in each layer, fully  connected,  we  arrive  at  the  following  expression  for  the  number  of  different  Boolean  functions  that  can be implemented by such a  network:",https://papers.nips.cc/paper_files/paper/1987/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/c74d97b01eae257e44aa9d5bade97baf-Abstract.html,
High Order Neural Networks for Efficient Associative Memory Design,"GÃ©rard Dreyfus, Isabelle Guyon, Jean-Pierre Nadal, LÃ©on Personnaz",1987,We  propose  learning  rules  for  recurrent  neural  networks  with  high-order  interactions  between  some or all  neurons.  The designed  networks  exhibit the  desired associative  memory  function: perfect  storage  and  retrieval  of pieces  of information and/or sequences of information of any complexity.,https://papers.nips.cc/paper_files/paper/1987/file/c7e1249ffc03eb9ded908c236bd1996d-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/c7e1249ffc03eb9ded908c236bd1996d-Abstract.html,
The Capacity of the Kanerva Associative Memory is Exponential,Philip Chou,1987,"The  capacity  of  an  associative  memory  is  defined  as  the  maximum  number  of  vords  that  can  be  stored  and  retrieved  reliably  by  an  address  vithin  a  given  sphere  of  attraction.  It  is  shown  by  sphere  packing  arguments  that  as  the  address  length  increases.  the  capacity  of  any  associati ve  memory  is  limited  to  an  exponential  grovth  rate  of  1 - h2 ( 0).  vhere  h2(0)  is  the  binary  entropy  function  in  bits.  and  0  is  the  radius  of  the  sphere  of  attraction.  This  exponential  grovth  in  capacity  can  actually  be  achieved  by  the  Kanerva  associative  memory.  if  its  parameters  are  optimally  set .  Formulas  for  these  op.timal  values  are  provided.  The  exponential  grovth  in  capacity  for  the  Kanerva  associative  memory  contrasts  sharply  vith  the  sub-linear  grovth  in  capacity  for  the  Hopfield  associative  memory.  ASSOCIATIVE  MEMORY  AND  ITS  CAPACITY  Our  model  of  an  associative  memory  is  the  folloving.  Let  ()(,Y)  be  an  (address.  datum)  pair.  vhere  )(  is  a  vector  of  n  Â±ls  and  Y  is  a  vector  of  m  Â±ls.  and  let  ()(l),y(I)), ... ,()(M) , y(M)).  be  M  (address,  datum)  pairs  stored  in  an  associative  memory.  is  presented  at  the  input  vith  an  address  )(  that  is  close  to  some  stored  address  )(W.  then  it should  produce  at  the  output  a  vord  Y  that  is  close  to  the  corresponding  contents  y(j).  To  be  specific,  let  us  say  that  an  associative  memory  can  correct fraction  0  errors  if  an  )(  vi thin  Hamming  distance  no  of  )((j)  retrieves  Y  equal  to  y(j).  The  Hamming  sphere  around  each  )(W  vill  be  called  the  sphere  of  attraction,  and  0  viII  be  called  the  radius  of  attraction.  If  the  associative  memory  One  notion  of  the  capacity  of  this  associative  memory  is  the  maximum  number  of  vords  that  it  can  store  vhile  correcting  fraction  0  errors .  Unfortunately.  this  notion  of  capacity  is  ill-defined.  because  it  depends  on  exactly  vhich  (address.  datum)  pairs  have  been  stored.  Clearly.  no  associative  memory  can  correct  fraction  0  errors  for  every  sequence  of  stored  (address,  datum)  pairs.  Consider.  for  example,  a  sequence  in  vhich  several  different  vords  are  vritten  to  the  same  address .  No  memory  can  reliably  retrieve  the  contents  of  the  overvritten  vords.  At  the  other  extreme.  any  associative  memory ' can  store  an  unlimited  number  of  vords  and  retrieve  them  all  reliably.  if  their  contents  are  identical.  A useful  definition  of  capacity  must  lie  somevhere  betveen  these  tvo  extremes.  that  for  most  sequences  of  addresses  XU), .. . , X(M)  and  most  sequences  of  data  y(l), ... , y(M).  the  memory  can  correct  fraction  0  errors.  We  define  In  this  paper.  ve  are  interested  in  the  largest  M  such  IThis  vork  vas  supported  by  the  National  Science  Foundation  under  NSF  grant  IST-8509860  and  by  an  IBM  Doctoral  Fellovship.  Â© American Institute of Physics 1988  185  I  most  sequences'  in  a  probabilistic  sense,  as  some  set  of  sequences  yi th  total  probability  greater  than  say,  .99.  When  all  sequences  are  equiprobab1e,  this  reduces  to  the  deterministic  version:  sequences.",https://papers.nips.cc/paper_files/paper/1987/file/c81e728d9d4c2f636f067f89cc14862c-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/c81e728d9d4c2f636f067f89cc14862c-Abstract.html,
The Sigmoid Nonlinearity in Prepyriform Cortex,Frank Eeckman,1987,Abstract Unavailable,https://papers.nips.cc/paper_files/paper/1987/file/c9f0f895fb98ab9159f51fd0297e236d-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/c9f0f895fb98ab9159f51fd0297e236d-Abstract.html,
Probabilistic Characterization of Neural Model Computations,Richard Golden,1987,"Information  retrieval  in  a  neural  network  is  viewed  as  a  procedure  in  which  the  network  computes  a  ""most  probable""  or  MAP estimate  of the  unk(cid:173) nown information.  This viewpoint allows the class of probability distributions,  P, the neural network can acquire to be explicitly  specified. Learning algorithms  for the  neural  network  which  search  for  the  ""most probable""  member of P can  then  be  designed.  Statistical  tests  which  decide if the  ""true""  or  environmental  probability  distribution  is in  P can also  be developed.  Example applications of  the theory to  the  highly nonlinear back-propagation learning algorithm,  and  the  networks of Hopfield and Anderson are discussed.",https://papers.nips.cc/paper_files/paper/1987/file/d09bf41544a3365a46c9077ebb5e35c3-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/d09bf41544a3365a46c9077ebb5e35c3-Abstract.html,
Learning in Networks of Nondeterministic Adaptive Logic Elements,Richard Windecker,1987,from,https://papers.nips.cc/paper_files/paper/1987/file/d1fe173d08e959397adf34b1d77e88d7-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/d1fe173d08e959397adf34b1d77e88d7-Abstract.html,
HIGH DENSITY ASSOCIATIVE MEMORIES,"Amir Dembo, Ofer Zeitouni",1987,"from a description of desired properties  A class of high dens ity assoc iat ive memories  is constructed,  starting  should  exhib it. These propert ies include high capac ity, controllable bas ins  of attraction and fast speed of convergence. Fortunately enough, the  resulting memory is implementable by an artificial Neural Net.",https://papers.nips.cc/paper_files/paper/1987/file/d2ddea18f00665ce8623e36bd4e3c7c5-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/d2ddea18f00665ce8623e36bd4e3c7c5-Abstract.html,
A Mean Field Theory of Layer IV of Visual Cortex and Its Application to Artificial Neural Networks,Christopher Scofield,1987,"A  single  cell  theory  for  the  development  of  selectivity  and  ocular  dominance  in  visual  cortex  has  been  presented  previously  by  Bienenstock,  Cooper  and  Munrol.  This  has  been  extended  to  a  network  applicable  to  layer  IV  of  visual  cortex2 .  In  this  paper  we  present  a  mean  field  approximation  that  captures  in  a  fairly  transparent  manner  the  quantitative,  results  of  the  network  theory.  Finally,  we  consider  the  application  of  this  theory  to  artificial  neural  networks  and  show  that  a  significant  reduction  in  architectural  complexity  is  possible.  the  qualitative,",https://papers.nips.cc/paper_files/paper/1987/file/d3d9446802a44259755d38e6d163e820-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/d3d9446802a44259755d38e6d163e820-Abstract.html,
Neural Networks for Template Matching: Application to Real-Time Classification of the Action Potentials of Real Neurons,"James Bower, Yiu-Fai Wong, Jashojiban Banik",1987,"Much  experimental study  of  real  neural  networks  relies  on  the  proper  classification  of  extracellulary  sampled  neural  signals  (i .e.  action  potentials)  recorded  from  the  brains  of ex(cid:173) perimental animals.  In  most  neurophysiology  laboratories this classification  task  is  simplified  by  limiting  investigations  to  single,  electrically  well-isolated  neurons recorded  one  at  a  time.  However, for those interested in sampling the activities of many single neurons simultaneously,  waveform  classification  becomes  a  serious  concern.  In  this  paper  we  describe  and  constrast  three  approaches  to  this  problem  each  designed  not  only  to  recognize  isolated  neural  events,  but also  to separately classify temporally overlapping events in real time.  First we  present two  formulations  of  waveform  classification  using  a  neural network  template  matching  approach.  These  two  formulations  are  then  compared  to  a  simple  template  matching  implementation.  Analysis with real neural signals reveals  that simple template matching is  a  better solution to  this  problem  than either neural network  approach.",https://papers.nips.cc/paper_files/paper/1987/file/d645920e395fedad7bbbed0eca3fe2e0-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/d645920e395fedad7bbbed0eca3fe2e0-Abstract.html,
Capacity for Patterns and Sequences in Kanerva's SDM as Compared to Other Associative Memory Models,James Keeler,1987,"The  information  capacity  of Kanerva's  Sparse,  Distributed Memory  (SDM)  and  Hopfield-type  neural networks  is  investigated.  Under  the  approximations  used here,  it  is shown  that  the  to(cid:173) tal  information  stored in  these  systems  is proportional  to  the  number  connections  in  the  net(cid:173) work.  The  proportionality  constant  is  the  same  for  the  SDM  and  HopJreld-type  models  in(cid:173) dependent  of  the  particular  model,  or  the  order  of  the  model.  The  approximations  are  checked  numerically.  This  same  analysis  can  be  used  to  show  that  the  SDM  can  store  se(cid:173) quences  of spatiotemporal patterns,  and  the  addition  of time-delayed  connections  allows  the  retrieval  of context  dependent  temporal  patterns.  A  minor  modification  of the  SDM  can  be  used to store correlated patterns.",https://papers.nips.cc/paper_files/paper/1987/file/d67d8ab4f4c10bf22aa353e27879133c-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/d67d8ab4f4c10bf22aa353e27879133c-Abstract.html,
The Connectivity Analysis of Simple Association,Dan Hammerstrom,1987,"The efficient realization, using current silicon technology, of Very Large Connection  Networks (VLCN) with more than a billion connections requires that these networks exhibit  a high degree of communication locality. Real neural networks exhibit significant locality,  yet most connectionist/neural network models have little. In this paper, the connectivity  requirements of a simple associative network are analyzed using communication theory.  Several techniques based on communication theory are presented that improve the robust(cid:173) ness of the network in the face of sparse, local interconnect structures. Also discussed are  some potential problems when information is distributed too widely.",https://papers.nips.cc/paper_files/paper/1987/file/d82c8d1619ad8176d665453cfb2e55f0-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/d82c8d1619ad8176d665453cfb2e55f0-Abstract.html,
Performance Measures for Associative Memories that Learn and Forget,Anthony Kuh,1987,"Recently,  many  modifications  to  the  McCulloch/Pitts  model  have  been  proposed  where  both  learning  and  forgetting  occur.  Given  that  the  network  never saturates  (ceases  to  function  effectively  due  to  an  overload  of  information),  the  learning  updates  can  con(cid:173) tinue indefinitely.  For these networks,  we  need  to introduce  performance  measmes in  addi(cid:173) tion  to  the  information  capacity  to  evaluate  the  different  networks.  We  mathematically  define  quantities such  as  the  plasticity  of  a  network,  the  efficacy  of an  information  vector,  and the  probability  of network  saturation.  From  these  quantities  we  analytically  compare  different networks.",https://papers.nips.cc/paper_files/paper/1987/file/d9d4f495e875a2e075a1a4a6e1b9770f-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/d9d4f495e875a2e075a1a4a6e1b9770f-Abstract.html,
Centric Models of the Orientation Map in Primary Visual Cortex,"William Baxter, Bruce Dow",1987,"In  the  visual  cortex  of  the  monkey  the  horizontal  organization  of  the  preferred  orientations of orientation-selective  cells  follows  two opposing  rules:  1) neighbors  tend  to  have similar orientation preferences, and  2) many different orientations are  observed  in a  local  region.  Several  orientation  models  which satisfy these  constraints are found  to  differ  in  the spacing  and  the  topological  index  of their singularities.  Using  the  rate  of orientation change as  a  measure,  the  models  are  compared to published experimental  results.",https://papers.nips.cc/paper_files/paper/1987/file/e2c420d928d4bf8ce0ff2ec19b371514-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/e2c420d928d4bf8ce0ff2ec19b371514-Abstract.html,
A Computer Simulation of Olfactory Cortex with Functional Implications for Storage and Retrieval of Olfactory Information,"James Bower, Matthew Wilson",1987,"Based  on  anatomical  and  physiological  data,  we  have  developed  a  computer  simulation  of piri(cid:173) form  (olfactory)  cortex  which  is  capable  of reproducing  spatial  and  temporal  patterns  of actual  cortical  activity  under  a  variety  of conditions.  Using  a  simple  Hebb-type  learning  rule  in  conjunc(cid:173) tion  with  the  cortical  dynamics  which  emerge  from  the  anatomical  and  physiological  organiza(cid:173) tion  of  the  model,  the  simulations  are  capable  of establishing  cortical  representations  for  differ(cid:173) ent  input  patterns.  The  basis  of  these  representations  lies  in  the  interaction  of  sparsely  distribut(cid:173) ed,  highly  divergent/convergent  interconnections  between  modeled  neurons.  We  have  shown  that  different  representations  can  be  stored  with  minimal  interference.  and  that  following  learning  these  representations  are  resistant  to  input  degradation,  allowing  reconstruction  of  a  representa(cid:173) tion  following  only  a  partial  presentation  of  an  original  training  stimulus.  Further,  we  have  demonstrated  that  the  degree  of  overlap  of  cortical  representations  for  different  stimuli  can  also  be  modulated.  For  instance  similar  input  patterns  can  be  induced  to generate  distinct  cortical  representations  (discrimination).  while  dissimilar  inputs  can  be  induced  to  generate  overlapping  representations  (accommodation).  Both  features  are  presumably  important  in  classifying  olfacto(cid:173) ry stimuli.",https://papers.nips.cc/paper_files/paper/1987/file/e369853df766fa44e1ed0ff613f563bd-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/e369853df766fa44e1ed0ff613f563bd-Abstract.html,
Towards an Organizing Principle for a Layered Perceptual Network,Ralph Linsker,1987,"An information-theoretic optimization principle is  proposed for  the development  of  each  processing  stage  of  a  multilayered  perceptual  network.  This  principle  of  ""maximum information preservation""  states that the signal transformation that is to be  realized at each stage is one that maximizes the information that the output signal values  (from that stage) convey about the input signals values (to that stage), subject to certain  constraints and in  the presence of processing noise.  The quantity being maximized is  a  Shannon information rate.  I provide motivation for this principle and -- for some simple  model cases -- derive some of its consequences, discuss an algorithmic implementation,  and  show  how  the  principle  may  lead  to  biologically  relevant  neural  architectural  features  such  as  topographic  maps,  map  distortions,  orientation  selectivity,  and  extraction of spatial and temporal signal correlations.  A  possible  connection between  this  information-theoretic principle  and  a  principle  of minimum  entropy production in  nonequilibrium thermodynamics is suggested.",https://papers.nips.cc/paper_files/paper/1987/file/e4da3b7fbbce2345d7772b0674a318d5-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/e4da3b7fbbce2345d7772b0674a318d5-Abstract.html,
A Trellis-Structured Neural Network,"Thomas Petsche, Bradley Dickinson",1987,"We have developed a neural network which consists of cooperatively inter(cid:173) connected Grossberg on-center off-surround subnets and which can be used to  optimize a function related to the log likelihood function for decoding convolu(cid:173) tional codes or more general FIR signal deconvolution problems. Connections in  the network are confined to neighboring subnets, and it is representative of the  types of networks which lend themselves to VLSI implementation. Analytical and  experimental results for convergence and stability of the network have been found.  The structure of the network can be used for distributed representation of data  items while allowing for fault tolerance and replacement of faulty units.  1",https://papers.nips.cc/paper_files/paper/1987/file/ea5d2f1c4608232e07d3aa3d998e5135-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/ea5d2f1c4608232e07d3aa3d998e5135-Abstract.html,
Supervised Learning of Probability Distributions by Neural Networks,"Eric Baum, Frank Wilczek",1987,"We propose that the back propagation algorithm for super(cid:173) vised learning can be generalized, put on a satisfactory conceptual 
footing, and very likely made more efficient by defining the val(cid:173)
ues of the output and input neurons as probabilities and varying 
the synaptic weights in the gradient direction of the log likelihood, 
rather than the 'error'.  In the past thirty years many researchers have studied the 
question of supervised learning in 'neural'-like networks. Recently 
a learning algorithm called 'back propagation H - 4 or the 'general(cid:173)
ized delta-rule' has been applied to numerous problems including 
the mapping of text to phonemes 5 , the diagnosis of illnesses6 and 
the classification of sonar targets 7 â¢ In these applications, it would 
often be natural to consider imperfect, or probabilistic informa(cid:173)
tion. We believe that by considering supervised learning from this 
slightly larger perspective, one can not only place back propaga-",https://papers.nips.cc/paper_files/paper/1987/file/eccbc87e4b5ce2fe28308fd9f2a7baf3-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/eccbc87e4b5ce2fe28308fd9f2a7baf3-Abstract.html,
Stochastic Learning Networks and their Electronic Implementation,"Joshua Alspector, Robert Allen, Victor Hu, Srinagesh Satyanarayana",1987,Abstract Unavailable,https://papers.nips.cc/paper_files/paper/1987/file/f033ab37c30201f73f142449d037028d-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/f033ab37c30201f73f142449d037028d-Abstract.html,
Connecting to the Past,Bruce MacDonald,1987,"Recently  there  has  been  renewed  interest  in  neural-like  processing  systems,  evidenced  for  ex(cid:173) ample in  the two volumes  Parallel Distributed Processing edited by Rumelhart and McClelland,  and  discussed  as  parallel  distributed  systems,  connectionist  models,  neural  nets,  value  passing  systems  and  multiple  context  systems.  Dissatisfaction  with  symbolic  manipulation  paradigms  for  artificial  intelligence seems  partly  responsible  for  this  attention, encouraged by  the promise  of massively  parallel  systems  implemented  in  hardware.  This  paper  relates  simple  neural-like  systems  based  on  multiple  context  to  some  other  well-known  formalisms-namely  production  systems, k-Iength sequence prediction, finite-state  machines and Turing machines-and presents  earlier sequence  prediction  results  in  a  new  light.",https://papers.nips.cc/paper_files/paper/1987/file/f457c545a9ded88f18ecee47145a72c0-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/f457c545a9ded88f18ecee47145a72c0-Abstract.html,
PARTITIONING OF SENSORY DATA BY A CORTICAL NETWORK,"Richard Granger, Jose Ambros-Ingerson, Howard Henry, Gary Lynch",1987,Abstract Unavailable,https://papers.nips.cc/paper_files/paper/1987/file/f7177163c833dff4b38fc8d2872f1ec6-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/f7177163c833dff4b38fc8d2872f1ec6-Abstract.html,
A Dynamical Approach to Temporal Pattern Processing,"W. Stornetta, Tad Hogg, Bernardo Huberman",1987,"Recognizing  patterns  with  temporal  context  is  important for  such  tasks  as  speech  recognition,  motion  detection  and  signature  verification.  We  propose  an  architecture  in  which  time  serves as its  own representation, and temporal context is encoded in the state of the  nodes. We contrast this with the approach of replicating portions of the  architecture to represent time.  As one example of these ideas, we demonstrate an architecture  with  capacitive  inputs  serving  as  temporal  feature  detectors  in  an  otherwise  standard  back  propagation  model.  Experiments  involving  motion  detection  and  word  discrimination  serve  to  illustrate  novel  features  of the system.  Finally, we discuss possible  extensions of the  architecture.",https://papers.nips.cc/paper_files/paper/1987/file/fbd7939d674997cdb4692d34de8633c4-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/fbd7939d674997cdb4692d34de8633c4-Abstract.html,
Minkowski-r Back-Propagation: Learning in Connectionist Models with Non-Euclidian Error Signals,"Stephen Hanson, David Burr",1987,"Many connectionist learning models are implemented using a gradient descent  in a least squares error function of the output and teacher signal.  The present model  Fneralizes. in particular. back-propagation [1]  by using Minkowski-r power metrics.  For  small  r's  a  ""city-block""  error  metric  is  approximated  and  for  large  r's  the  ""maximum"" or ""supremum""  metric is  approached.  while  for r=2  the  standard  back(cid:173) propagation  model  results.  An  implementation  of Minkowski-r back-propagation  is  described.  and  several  experiments  are  done  which  show  that  different values  of r  may be desirable for various purposes. Different r values may be appropriate for the  reduction  of  the  effects  of outliers  (noise).  modeling  the  input  space  with  more  compact clusters. or modeling  the statistics of a particular domain more naturally or  in a way that may be more perceptually or psychologically meaningful (e.g. speech or  vision).",https://papers.nips.cc/paper_files/paper/1987/file/fc490ca45c00b1249bbe3554a4fdf6fb-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/fc490ca45c00b1249bbe3554a4fdf6fb-Abstract.html,
Analysis and Comparison of Different Learning Algorithms for Pattern Association Problems,J. Bernasconi,1987,"We  investigate the behavior of different learning algorithms  for networks of neuron-like units. As test cases we use simple pat(cid:173) tern association problems, such as the XOR-problem and symmetry de(cid:173) tection problems. The algorithms considered are either versions of  the Boltzmann machine learning rule or based on the backpropagation  of errors. We also propose and analyze a generalized delta rule for  linear threshold units. We  find that the performance of a given  learning algorithm depends strongly on the type of units used. In  particular, we observe that networks with Â±1 units quite generally  exhibit a significantly better learning behavior than the correspon(cid:173) ding 0,1 versions. We also demonstrate that an adaption of the  weight-structure to  the symmetries of the problem can lead to a  drastic increase in learning speed.",https://papers.nips.cc/paper_files/paper/1987/file/fe9fc289c3ff0af142b6d3bead98a923-Paper.pdf,https://papers.nips.cc/paper_files/paper/1987/hash/fe9fc289c3ff0af142b6d3bead98a923-Abstract.html,
Neural Control of Sensory Acquisition: The Vestibulo-Ocular Reflex,"Michael Paulin, Mark Nelson, James Bower",1988,"We present a new hypothesis that the cerebellum plays a key role in ac(cid:173) tively controlling the acquisition of sensory infonnation by the nervous  system.  In this paper we explore this idea by examining the function of  a  simple  cerebellar-related  behavior,  the  vestibula-ocular  reflex  or  VOR, in  which  eye movements  are generated to minimize image slip  on  the  retina  during  rapid  head  movements.  Considering  this  system  from  the point of view of statistical estimation theory, our results  sug(cid:173) gest that the transfer function of the VOR, often regarded as a static or  slowly  modifiable  feature  of the  system,  should  actually  be  continu(cid:173) ously and rapidly changed during head movements. We further suggest  that these changes are under the direct control of the cerebellar cortex  and propose experiments to test this hypothesis.",https://papers.nips.cc/paper_files/paper/1988/file/006f52e9102a8d3be2fe5614f42ba989-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/006f52e9102a8d3be2fe5614f42ba989-Abstract.html,
Modeling the Olfactory Bulb - Coupled Nonlinear Oscillators,"Zhaoping Li, John J. Hopfield",1988,"The olfactory  bulb of mammals  aids  in  the  discrimination  of  odors.  A  mathematical  model  based  on  the  bulbar  anatomy  and  electrophysiology  is  described.  Simulations  produce  a  35-60  Hz  modulated activity coherent across the bulb, mimicing the observed  field  potentials.  The  decision  states  (for  the  odor  information)  here  can  be  thought  of as  stable  cycles,  rather  than  point  stable  states  typical  of simpler  neuro-computing  models.  Analysis  and  simulations show that a  group of coupled non-linear oscillators are  responsible for the oscillatory activities determined by the odor in(cid:173) put, and that the bulb, with appropriate inputs from higher centers,  can  enhance  or suppress  the  sensitivity  to  partiCUlar  odors.  The  model provides a framework  in which to understand the transform  between odor input and the bulbar output to olfactory cortex.",https://papers.nips.cc/paper_files/paper/1988/file/013d407166ec4fa56eb1e1f8cbe183b9-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/013d407166ec4fa56eb1e1f8cbe183b9-Abstract.html,
Efficient Parallel Learning Algorithms for Neural Networks,"Alan Kramer, Alberto Sangiovanni-Vincentelli",1988,"Parallelizable optimization techniques are applied to the problem of  learning in feedforward neural networks. In addition to having supe(cid:173) rior convergence properties, optimization techniques such as the Polak(cid:173) Ribiere method are also significantly more efficient than the Back(cid:173) propagation algorithm. These results are based on experiments per(cid:173) formed on small boolean learning problems and the noisy real-valued  learning problem of hand-written character recognition.",https://papers.nips.cc/paper_files/paper/1988/file/02522a2b2726fb0a03bb19f2d8d9524d-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/02522a2b2726fb0a03bb19f2d8d9524d-Abstract.html,
The Boltzmann Perceptron Network: A Multi-Layered Feed-Forward Network Equivalent to the Boltzmann Machine,"Eyal Yair, Allen Gersho",1988,"The  concept  of  the  stochastic  Boltzmann  machine  (BM)  is  auractive  for  decision  making  and  pattern  classification  purposes  since  the  probability  of  attaining  the network  states  is a  function  of the network energy.  Hence,  the  probability of attaining particular energy minima  may be associated  with  the  probabilities  of  making  certain  decisions  (or  classifications).  However,  because of its stochastic  nature,  the complexity of the BM is fairly  high and  therefore  such  networks  are  not  very  likely  to  be  used  in  practice.  In  this  paper  we  suggest  a  way  to  alleviate  this  drawback  by  converting  the  sto(cid:173) chastic  BM into  a  deterministic  network  which  we  call  the  Boltzmann  Per(cid:173) ceptron  Network  (BPN).  The BPN is functionally  equivalent  to  the  BM but  has  a  feed-forward  structure  and  low  complexity.  No annealing  is required.  The  conditions  under  which  such  a  convmion  is  feasible  are  given.  A  learning  algorithm  for  the  BPN based  on  the  conjugate  gradient  method  is  also provided which is somewhat akin  to the backpropagation algorithm.",https://papers.nips.cc/paper_files/paper/1988/file/045117b0e0a11a242b9765e79cbf113f-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/045117b0e0a11a242b9765e79cbf113f-Abstract.html,
Neural Networks that Learn to Discriminate Similar Kanji Characters,"Yoshihiro Mori, Kazuhiko Yokosawa",1988,is,https://papers.nips.cc/paper_files/paper/1988/file/06409663226af2f3114485aa4e0a23b4-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/06409663226af2f3114485aa4e0a23b4-Abstract.html,
Computer Modeling of Associative Learning,"Daniel Alkon, Francis Quek, Thomas Vogl",1988,Abstract Unavailable,https://papers.nips.cc/paper_files/paper/1988/file/069059b7ef840f0c74a814ec9237b6ec-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/069059b7ef840f0c74a814ec9237b6ec-Abstract.html,
Links Between Markov Models and Multilayer Perceptrons,"HervÃ© Bourlard, C. J. Wellekens",1988,"Hidden Markov models are widely used for automatic speech recog(cid:173) nition. They inherently incorporate the sequential character of the  speech signal and are statistically trained. However, the a-priori  choice of the model topology limits their flexibility. Another draw(cid:173) back of these models is their weak discriminating power. Multilayer  perceptrons are now promising tools in the connectionist approach  for classification problems and have already been successfully tested  on speech recognition problems. However, the sequential nature of  the speech signal remains difficult to handle in that kind of ma(cid:173) chine. In this paper, a discriminant hidden Markov model is de(cid:173) fined and it is shown how a particular multilayer perceptron with  contextual and extra feedback input units can be considered as a  general form of such Markov models.",https://papers.nips.cc/paper_files/paper/1988/file/0777d5c17d4066b82ab86dff8a46af6f-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/0777d5c17d4066b82ab86dff8a46af6f-Abstract.html,
Skeletonization: A Technique for Trimming the Fat from a Network via Relevance Assessment,"Michael C. Mozer, Paul Smolensky",1988,"This paper proposes a means of using the knowledge in a network to  determine the functionality or relevance of individual units, both for  the purpose of understanding the network's behavior and improving its  performance. The basic idea is to iteratively train the network to a cer(cid:173) tain performance criterion, compute a measure of relevance that identi(cid:173) fies which input or hidden units are most critical to performance, and  automatically trim the least relevant units. This skeletonization tech(cid:173) nique can be used to simplify networks by eliminating units that con(cid:173) vey redundant information; to improve learning performance by first  learning with spare hidden units and then trimming the unnecessary  ones away, thereby constraining generalization; and to understand the  behavior of networks in terms of minimal ""rules.""",https://papers.nips.cc/paper_files/paper/1988/file/07e1cd7dca89a1678042477183b7ac3f-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/07e1cd7dca89a1678042477183b7ac3f-Abstract.html,
A Passive Shared Element Analog Electrical Cochlea,"David Feld, Joe Eisenberg, Edwin Lewis",1988,"We present a  simplified model  of the  micromechanics of the human  cochlea,  realized  with  electrical  elements.  Simulation  of the  model  shows that it retains four signal processing features whose importance  we argue on the basis of engineering logic and evolutionary evidence.  Furthermore, just as  the cochlea does,  the  model  achieves  massively  parallel signal processing in a structurally economic way, by means of  shared elements.  By extracting what we believe are the five essential  features of the cochlea, we hope to design a useful  front-end  filter to  process  acoustic  images and to  obtain  a  better understanding  of the  auditory system.",https://papers.nips.cc/paper_files/paper/1988/file/0a09c8844ba8f0936c20bd791130d6b6-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/0a09c8844ba8f0936c20bd791130d6b6-Abstract.html,
Speech Production Using A Neural Network with a Cooperative Learning Mechanism,"Mitsuo Komura, Akio Tanaka",1988,"We  propose  a  new  neural  network  model  and  its  learning  algorithm. The proposed neural network consists of four layers  - input, hidden, output and final output layers. The hidden and  output layers are multiple.  Using the proposed  SICL(Spread  Pattern Information and Cooperative  Learning)  algorithm,  it  is  possible  to  learn  analog  data  accurately  and  to  obtain  smooth outputs. Using this neural network, we have developed  a  speech production system consisting of a  phonemic  symbol  production  subsystem  and  a  speech  parameter  production  subsystem.  We  have  succeeded  in  producing  natural  speech  waves with high accuracy.",https://papers.nips.cc/paper_files/paper/1988/file/0f28b5d49b3020afeecd95b4009adf4c-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/0f28b5d49b3020afeecd95b4009adf4c-Abstract.html,
Associative Learning via Inhibitory Search,David Ackley,1988,"ALVIS is  a  reinforcement-based  connectionist  architecture  that  learns  associative  maps  in  continuous  multidimensional  environ(cid:173) ments.  The  discovered  locations  of  positive  and  negative  rein(cid:173) forcements  are  recorded  in  ""do be""  and  ""don't  be""  subnetworks,  respectively.  The outputs of the subnetworks relevant  to the cur(cid:173) rent goal are combined and compared with the current location to  produce  an  error  vector.  This  vector  is  backpropagated  through  a  motor-perceptual  mapping  network.  to  produce  an  action  vec(cid:173) tor that leads the system towards do-be locations  and  away from  don 't-be locations.  AL VIS is  demonstrated with a simulated robot  posed a  target-seeking task.",https://papers.nips.cc/paper_files/paper/1988/file/1385974ed5904a438616ff7bdb3f7439-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/1385974ed5904a438616ff7bdb3f7439-Abstract.html,
Performance of a Stochastic Learning Microchip,"Joshua Alspector, Bhusan Gupta, Robert Allen",1988,Abstract Unavailable,https://papers.nips.cc/paper_files/paper/1988/file/140f6969d5213fd0ece03148e62e461e-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/140f6969d5213fd0ece03148e62e461e-Abstract.html,
Song Learning in Birds,M. Konishi,1988,Abstract Unavailable,https://papers.nips.cc/paper_files/paper/1988/file/149e9677a5989fd342ae44213df68868-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/149e9677a5989fd342ae44213df68868-Abstract.html,
Modeling Small Oscillating Biological Networks in Analog VLSI,"Sylvie Ryckebusch, James Bower, Carver Mead",1988,"We  have used analog VLSI technology to model a class of small os(cid:173) cillating  biological neural circuits  known  as  central pattern  gener(cid:173) ators  (CPG). These circuits generate rhythmic patterns of activity  which drive locomotor behaviour in the animal.  We  have designed,  fabricated,  and tested a model neuron circuit which relies on many  of the  same  mechanisms  as  a  biological  central pattern  generator  neuron,  such  as  delays  and  internal feedback.  We  show  that  this  neuron can be used  to build several small circuits based on known  biological CPG circuits, and that these circuits produce patterns of  output which  are very similar to the observed biological patterns.  To  date,  researchers  in  applied  neural  networks  have  tended  to  focus  on  mam(cid:173) malian systems  as  the  primary source  of  potentially  useful  biological  information.  However,  invertebrate systems may represent  a source  of ideas  in many ways  more  appropriate, given current levels of engineering sophistication in building neural-like  systems, and given the state of biological understanding of mammalian circuits.  In(cid:173) vertebrate  systems  are  based  on  orders  of magnitude  smaller  numbers  of neurons  than  are  mammalian  systems.  The  networks  we  will  consider  here,  for  example,  are  composed  of  about  a  dozen  neurons,  which  is  well  within  the  demonstrated  capabilities  of current  hardware  fabrication  techniques.  Furthermore,  since  much  more detailed structural information is  available about these systems than for  most  systems in  higher animals, insights can be guided by real information rather than by  guesswork.  Finally, even though they are constructed of small numbers of neurons,  these networks have  numerous interesting and  potentially even useful properties.  CENTRAL PATTERN  GENERATORS  Of  all  the  invertebrate  neural  networks  currently  being  investigated  by  neurobi(cid:173) ologists,  the  class  of  networks  known  as  central  pattern  generators  (CPGs)  may  be  especially  worthy  of attention.  A  CPG is  responsible  for  generating  oscillatory  neural  activity  that  governs  specific  patterns  of  motor  output,  and  can  generate  its  pattern  of activity  when  isolated  from  its  normal neuronal  inputs.  This  prop- Modeling Small Oscillating Biological Networks  385  erty, which greatly facilitates experiments, has enabled biologists to describe several  CPGs in  detail at  the cellular and synaptic level.  These networks have  been found  in all animals,  but have been extensively studied in invertebrates [Selverston,  1985].  We  chose to model several small CPG networks using analog VLSI technology.  Our  model differs from  most computer simulation models of biological networks [Wilson  and Bower, in press] in that we did not attempt to model the details of the individual  ionic currents,  nor did we  attempt to model each known connection in the networks.  Rather, our aim  was  to determine the basic  functionality  of a  set of CPG networks  by modeling them as  the minimum set of connections required to reproduce output  qualitatively similar to that produced by the real network under certain conditions.  MODELING  CPG NEURONS  The  basic  building  block  for  our  model  is  a  general  purpose  CPG  neuron  circuit.  This circuit, shown  in  Figure  1,  is  our model  for  a  typical neuron  found  in central  pattern  generators,  and  contains  some  of  the  essential  elements  of real  biological  neurons.  Like real neurons, this model integrates current and uses positive feedback  to  output  a  train  of pulses,  or  action  potentials,  whose  frequency  depends  on  the  magnitude of the current input.  The part of the circuit which generates these pulses  is  shown  in  Figure 2a [Mead,  19891.  The second element in  the CPG neuron circuit is the synapse.  In Figure 1,  each pair  of transistors functions as a synapse.  The p-well transistors are. excitatory synapses,  whereas the n-well transistors are inhibitory synapses.  One of the transistors in the  pair sets the strength of the synapse,  while  the other transistor is  the  input of the  synapse.  Each  CPG neuron  has four  different synapses.  The  third element of our model  CPG neuron  involves  temporal delays.  Delays  are  an essential element in the function of CPGs, and biology has evolved many different  mechanisms  to  introduce delays  into neural networks.  The membrane capacitance  of the  cell  body,  different  rates of chemical reactions,  and  axonal transmission  are  just  a  few  of the  mechanisms which  have  time constants  associated with  them.  In  our  model we  have  included  synaptic  delay  as  the  principle source of delay  in  the  network.  This  is  modeled  as  an  RC  delay,  implemented  by  the follower-integrator  circuit shown in Figure 2b [Mead,  19891.  The time constant of the delay is a function  of the  conductance  of  the  amplifier,  set  by  the  bias  G.  A  multiple  time  constant  delay line is  formed  by cascading several of these elements.  Our neuron circuit uses  a  delay  line  with  three  time  constants.  The  synapses  which  are  before  the  delay  element  are  slow synapses, whereas the  undelayed synapses  are  fa.st  synapses.  We fabricated the circuit shown in Figure 1 using CMOS, VLSI technology.  Several  of these  circuits  were  put  on  each  chip,  with  all  of  the  inputs  and  controls  going  out  to pads, so  that these  cells  could  be externally connected  to form  the  network  of interest.  386  Ryckebusch, Bower, and Mead",https://papers.nips.cc/paper_files/paper/1988/file/1afa34a7f984eeabdbb0a7d494132ee5-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/1afa34a7f984eeabdbb0a7d494132ee5-Abstract.html,
Comparing Biases for Minimal Network Construction with Back-Propagation,"Stephen Hanson, Lorien Pratt",1988,"learning  representations during  Rumelhart (1987). has proposed a method for choosing minimal or  ""simple""  in Back-propagation  networks. This approach can be used to (a) dynamically select the  number of hidden units. (b) construct a representation that is  appropriate for the problem and (c) thus improve the generalization  ability of Back-propagation networks. The method Rumelhart suggests  involves adding penalty terms to the usual error function. In this paper  we introduce RumelhartÂ·s minimal networks idea and compare two  possible biases on the weight search space. These biases are compared  in both simple counting problems and a speech recognition problem.  In general. the constrained search does seem to minimize the number of  hidden units required with an expected increase in local minima.",https://papers.nips.cc/paper_files/paper/1988/file/1c9ac0159c94d8d0cbedc973445af2da-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/1c9ac0159c94d8d0cbedc973445af2da-Abstract.html,
What Size Net Gives Valid Generalization?,"Eric Baum, David Haussler",1988,"We  address  the  question  of when  a  network  can  be  expected  to  generalize from m  random training examples chosen from some ar(cid:173) bitrary probability distribution, assuming that future test examples  are drawn from  the same  distribution.  Among  our  results are  the  following  bounds on appropriate sample vs.  network size.  Assume  o <  Â£  $  1/8.  We  show  that  if m  >  O( ~log~) random  exam(cid:173) ples  can  be  loaded  on  a  feedforward  network  of linear  threshold  functions  with N  nodes and W  weights,  so that at least a  fraction  1 - t of the examples  are  correctly  classified,  then one  has confi(cid:173) dence approaching certainty that the network will correctly classify  a  fraction  1 - Â£  of future  test  examples drawn from  the same  dis(cid:173) tribution.  Conversely,  for  fully-connected  feedforward  nets  with  one  hidden  layer,  any learning  algorithm  using  fewer  than  O( '!')  random training examples  will,  for  some distributions of examples  consistent  with  an  appropriate  weight  choice,  fail  at  least  some  fixed fraction of the time to find  a  weight choice that will correctly  classify more  than a 1 - Â£  fraction of the future  test examples.",https://papers.nips.cc/paper_files/paper/1988/file/1d7f7abc18fcb43975065399b0d1e48e-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/1d7f7abc18fcb43975065399b0d1e48e-Abstract.html,
A Low-Power CMOS Circuit Which Emulates Temporal Electrical Properties of Neurons,"Jack Meador, Clint Cole",1988,"This  paper  describes  a  CMOS  artificial  neuron.  The  circuit  is 
directly  derived  from  the  voltage-gated  channel  model  of  neural 
membrane,  has  low  power  dissipation,  and  small  layout  geometry. 
The principal motivations behind this work include a desire for high 
performance,  more  accurate  neuron  emulation,  and  the  need  for 
higher density in practical neural network implementations.",https://papers.nips.cc/paper_files/paper/1988/file/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Abstract.html,
Linear Learning: Landscapes and Algorithms,Pierre Baldi,1988,Abstract Unavailable,https://papers.nips.cc/paper_files/paper/1988/file/202cb962ac59075b964b07152d234b70-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/202cb962ac59075b964b07152d234b70-Abstract.html,
Applications of Error Back-Propagation to Phonetic Classification,"Hong Leung, Victor W. Zue",1988,"This paper is concerced with the use of error back-propagation  in phonetic classification. Our objective is to investigate the ba(cid:173) sic characteristics of back-propagation, and study how the frame(cid:173) work of multi-layer perceptrons can be exploited in phonetic recog(cid:173) nition. We explore issues such as integration of heterogeneous  sources of information, conditioll~ that can affect performance of  phonetic classification, internal representations, comparisons with  traditional pattern classification techniques, comparisons of differ(cid:173) ent error metrics, and initialization of the network. Our investiga(cid:173) tion is performed within a set of experiments that attempts to rec(cid:173) ognize the 16 vowels in American English independent of speaker.  Our results are comparable to human performance.  Early approaches in phonetic recognition fall into two major extremes: heuristic  and algorithmic. Both approaches have their own merits and shortcomings. The  heuristic approach has the intuitive appeal that it focuses on the linguistic informa(cid:173) tion in the speech signal and exploits acoustic-phonetic knowledge. HO'fever, the  weak control strategy used for utilizing our knowledge has been grossly inadequate.  At the other extreme, the algorithmic approach relies primarily on the powerful con(cid:173) trol strategy offered by well-formulated pattern recognition techniques. However,  relatively little is known about how our speech knowledge accumulated over the  past few decades can be incorporated into the well-formulated algorithms. We feel  that artificial neural networks (ANN) have some characteristics that can potentially  enable them to bridge the gap between these two extremes. On the one hand, our  speech knowledge can provide guidance to the structure and design of the network.  On the other hand, the self-organizing mechanism of ANN can provide a control  strategy for utilizing our knowledge.  In this paper, we extend our earlier work on the use of artificial neural networks  for phonetic recognition [2]. Specifically, we focus our investigation on the following  sets of issues. First, we describe the use of the network to integrate heterogeneous  sources of information. We will see how classification performance improves as more  Error Back-Propagation to Phonetic Classification  207  information is available. Second, we discuss several important factors that can sub(cid:173) stantially affect the performance of phonetic classification. Third, we examine the  internal representation of the network. Fourth, we compare the network with two  traditional classification techniques: K-nearest neighbor and Gaussian classifica(cid:173) tion. Finally, we discuss our specific implementations of back-propagation that  yield improved performance and more efficient learning time.",https://papers.nips.cc/paper_files/paper/1988/file/2723d092b63885e0d7c260cc007e8b9d-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/2723d092b63885e0d7c260cc007e8b9d-Abstract.html,
An Analog Self-Organizing Neural Network Chip,"James Mann, Sheldon Gilbert",1988,"A design for a fully analog version of a self-organizing feature map neural  network has been completed. Several parts of this design are in fabrication.  The feature map algorithm was modified to accommodate circuit solutions  to the various computations required. Performance effects were measured  by simulating the design as part of a frontend for a speech recognition  system. Circuits are included to implement both activation computations and  weight adaption 'or learning. External access to the analog weight values is  provided to facilitate weight initialization, testing and static storage. This  fully analog implementation requires an order of magnitude less area than  a comparable digital/analog hybrid version developed earlier.",https://papers.nips.cc/paper_files/paper/1988/file/2a79ea27c279e471f4d180b08d62b00a-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/2a79ea27c279e471f4d180b08d62b00a-Abstract.html,
A Bifurcation Theory Approach to the Programming of Periodic Attractors in Network Models of Olfactory Cortex,Bill Baird,1988,"A  new  learning  algorithm  for  the  storage  of  static  and  periodic  attractors  in  biologically  inspired  recurrent  analog  neural  networks  is  introduced.  For  a  network  of  n  nodes,  n  static  or  n/2  periodic  attractors  may  be  stored.  The  algorithm  allows  programming  of  the  network  vector  field  indepen(cid:173) dent  of  the  patterns  to  be  stored.  Stability  of  patterns,  basin  geometry,  and  rates  of  convergence  may  be  controlled.  For  orthonormal  patterns,  the  l~grning operation  reduces  to  a  kind  of  periodic  outer  product  rule  that  allows  local,  additive,  commutative,  incremental  learning.  Standing  or  traveling  wave  cycles  may  be  stored  to  mimic  the  kind  of  oscillating  spatial  patterns  that  appear  in  the  neural  activity  of  the  olfactory  bulb  and  prepyriform  cortex  during  inspiration  and  suffice,  in  the  bulb,  to  predict  the  pattern  recognition  behavior  of  rabbits  in  classical  conditioning  ex(cid:173) periments.  These  attractors  arise,  during  simulat(cid:173) ed  inspiration,  through  a  multiple  Hopf  bifurca(cid:173) tion,  which  can  act  as  a  critical  ""decision  pOint""  for  their  selection  by  a  very  small  input  pattern.",https://papers.nips.cc/paper_files/paper/1988/file/2b24d495052a8ce66358eb576b8912c8-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/2b24d495052a8ce66358eb576b8912c8-Abstract.html,
Heterogeneous Neural Networks for Adaptive Behavior in Dynamic Environments,"Randall Beer, Hillel Chiel, Leon S. Sterling",1988,"Research  in artificial neural networks has genera1ly emphasized  homogeneous architectures. In contrast, the nervous systems of natural  animals exhibit great heterogeneity in both their elements and patterns  of interconnection. This heterogeneity is crucial to the flexible  generation of behavior which is essential for survival in a complex,  dynamic environment. It may also provide powerful insights into the  design of artificial neural networks.  In this paper, we describe a  heterogeneous neural network for controlling  the wa1king of a  simulated insect. This controller is inspired by the neuroethological  It exhibits a  and neurobiological literature on insect locomotion.  variety of statically stable gaits at different speeds simply by varying  the tonic activity of a single cell. It can also adapt to perturbations as a  natural consequence of its design.",https://papers.nips.cc/paper_files/paper/1988/file/2b44928ae11fb9384c4cf38708677c48-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/2b44928ae11fb9384c4cf38708677c48-Abstract.html,
Programmable Analog Pulse-Firing Neural Networks,"Alister Hamilton, Alan Murray, Lionel Tarassenko",1988,"We  describe  pulse  - stream  firing  integrated  circuits  that  imple(cid:173) ment asynchronous analog neural networks.  Synaptic weights are  stored  dynamically,  and  weighting  uses  time-division  of  the  neural  pulses  from  a  signalling  neuron  to  a  receiving  neuron.  MOS  transistors  in  their  ""ON""  state  act  as  variable  resistors  to  control  a  capacitive  discharge,  and  time-division  is  thus  achieved  by  a  small  synapse  circuit  cell.  The  VLSI  chip  set  design  uses  2.5J.1.m  CMOS technology.",https://papers.nips.cc/paper_files/paper/1988/file/31fefc0e570cb3860f2a6d4b38c6490d-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/31fefc0e570cb3860f2a6d4b38c6490d-Abstract.html,
An Analog VLSI Chip for Thin-Plate Surface Interpolation,John Harris,1988,"Reconstructing a surface from sparse sensory data is a well-known  problem iIi computer vision. This paper describes an experimental  analog VLSI chip for smooth surface interpolation from sparse depth  data. An eight-node ID network was designed in 3J.lm CMOS and  successfully tested. The network minimizes a second-order or ""thin(cid:173) plate"" energy of the surface. The circuit directly implements the cou(cid:173) pled depth/slope model of surface reconstruction (Harris, 1987). In  addition, this chip can provide Gaussian-like smoothing of images.",https://papers.nips.cc/paper_files/paper/1988/file/3636638817772e42b59d74cff571fbb3-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/3636638817772e42b59d74cff571fbb3-Abstract.html,
Simulation and Measurement of the Electric Fields Generated by Weakly Electric Fish,"Brian Rasnow, Christopher Assad, Mark Nelson, James Bower",1988,"The weakly electric fish, Gnathonemus peters;;, explores its environment by gener(cid:173) ating pulsed elecbic fields  and detecting small pertwbations in the fields  resulting from  nearby objects.  Accordingly, the fISh  detects and discriminates objects on  the basis of a  sequence of elecbic ""images"" whose temporal and spatial properties depend on  the  tim(cid:173) ing of the fish's electric organ discharge and its body position relative to objects in its en(cid:173) vironmenl  We are interested in investigating how these fish utilize timing and body-po(cid:173) sition during exploration to aid in object discrimination.  We have developed a fmite-ele(cid:173) ment simulation of the fish's self-generated electric  fields  so as  to  reconstruct the elec(cid:173) trosensory consequences of body position and electric organ discharge timing in the fish.  This paper describes this finite-element simulation system and presents preliminary elec(cid:173) tric field measurements which are being used to tune the simulation.",https://papers.nips.cc/paper_files/paper/1988/file/37a749d808e46495a8da1e5352d03cae-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/37a749d808e46495a8da1e5352d03cae-Abstract.html,
Training Multilayer Perceptrons with the Extended Kalman Algorithm,"Sharad Singhal, Lance Wu",1988,"trained with  A large fraction of recent work in artificial neural nets uses  multilayer perceptrons  the back-propagation  algorithm described by Rumelhart et. a1. This algorithm  converges slowly for large or complex problems such as  speech recognition, where thousands of iterations may be  needed for convergence even with small data sets. In this  paper, we show that training multilayer perceptrons is an  identification problem for a nonlinear dynamic system which  can be solved using  the Extended Kalman Algorithm.  Although computationally complex, the Kalman algorithm  usually converges in a few  the  algorithm and compare it with back-propagation using two(cid:173) dimensional examples.  iterations. We describe",https://papers.nips.cc/paper_files/paper/1988/file/38b3eff8baf56627478ec76a704e9b52-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/38b3eff8baf56627478ec76a704e9b52-Abstract.html,
An Electronic Photoreceptor Sensitive to Small Changes in Intensity,"Tobi DelbrÃ¼ck, C. A. Mead",1988,"We describe an electronic photoreceptor circuit that is sensitive to  small changes in incident light intensity. The sensitivity to change8  in the intensity is achieved by feeding back to the input a filtered  version of the output. The feedback loop includes a hysteretic el(cid:173) ement. The circuit behaves in a manner reminiscent of the gain  control properties and temporal responses of a variety of retinal  cells, particularly retinal bipolar cells. We compare the thresholds  for detection of intensity increments by a human and by the cir(cid:173) cuit. Both obey Weber's law and for both the temporal contrast  sensitivities are nearly identical.  We previously described an electronic photoreceptor that outputs a voltage that is  logarithmic in the light intensity (Mead, 1985). This report describes an extension  of this circuit which was based on a suggestion by Frank Werblin that biological  retinas may achieve greater sensitivity to change8 in the illumination by feeding  back a filtered version of the output.  OPERATION OF THE CIRCUIT  The circuit (Figure 1) consists of a phototransistor (P), exponential feedback to  P (Ql, Q2, and Q3), a transconductance amplifier (A), and the hysteretic element  (Q4 and Qs). In general terms the operation of the circuit consists of two stages of  amplification with hysteresis in the feedback loop. The light falls on the parasitic  bipolar transistor P. (The rest of the circuit is shielded by metal.) P's collector  is the substrate and the base is an isolated well. P and Ql form the first stage of  amplification. The light produces a base current Is for P. The emitter current IE  is PIs, neglecting collector resistance for now. P is typically a few hundred. The  feedback current IQl is set by the gate voltage on QdQ2' which is set by the current  through Q3, which is set by the feedback voltage Vjb. In equilibrium Vjb will be  such that IQl = IE and some voltage Vp will be the output of the first stage. The  An Electronic Photoreceptor Sensitive to Small Changes  721  negative feedback through the transconductance amplifier A will make Vp ~ V/ bâ¢  This voltage is logarithmic in the light intensity, since in subthreshold operation  the currents through Q2 and Q3 are exponential in their gate to source voltages.  The DC output of the circuit will be Vout ~ V/b = Vdd - (2kT /q) log IE, neglecting  the back-gate effect for Q2. Figure Sa (DC output) shows that the assumption of  subthreshold operation is valid over about 4 orders of magnitude.",https://papers.nips.cc/paper_files/paper/1988/file/3988c7f88ebcb58c6ce932b957b6f332-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/3988c7f88ebcb58c6ce932b957b6f332-Abstract.html,
Training a 3-Node Neural Network is NP-Complete,"Avrim Blum, Ronald Rivest",1988,"We consider  a  2-layer,  3-node,  n-input neural network whose  nodes  compute linear threshold functions  of their inputs.  We  show  that it  is NP-complete to decide whether there exist weights and thresholds  for the three nodes of this network so that it will produce output con(cid:173) sistent  with  a  given set of training examples.  We  extend  the result  to other simple networks.  This result suggests that those looking for  perfect  training  algorithms  cannot  escape  inherent  computational  difficulties just by  considering only simple or very  regular networks.  It also suggests the importance, given a training problem, of finding  an  appropriate network  and input encoding for  that problem.  It is  left as an open problem to extend our result to nodes with non-linear  functions such as  sigmoids.",https://papers.nips.cc/paper_files/paper/1988/file/3def184ad8f4755ff269862ea77393dd-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/3def184ad8f4755ff269862ea77393dd-Abstract.html,
Automatic Local Annealing,Jared Leinbach,1988,This research involves a method for finding global maxima  in  constraint  satisfaction  networks.  It  is  an  annealing  process  butt  unlike  most  otherst  requires  no  annealing  schedule.  Temperature  is  instead  determined  locally  by  units at each updatet and thus all processing is done at the  unit  level.  There  are  two  major  practical  benefits  to  processing  this  way:  1)  processing  can continue  in  'bad t  areas of the networkt while 'good t areas remain stablet and  2)  processing  continues  in  the  'bad t  areast as  long  as  the  constraints  remain  poorly  satisfied  (i.e.  it  does  not  stop  after  some  predetermined  number of cycles).  As a  resultt  this  method  not  only  avoids  the  kludge  of requiring  an  externally determined annealing schedulet but it also finds  global  maxima  more  quickly  and  consistently  than  externally  scheduled  systems  the  to  Boltzmann machine (Ackley et alt 1985) is made).  FinallYt  implementation of this method is computationally trivial.,https://papers.nips.cc/paper_files/paper/1988/file/42a0e188f5033bc65bf8d78622277c4e-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/42a0e188f5033bc65bf8d78622277c4e-Abstract.html,
Adaptive Neural Net Preprocessing for Signal Detection in Non-Gaussian Noise,"Richard P. Lippmann, Paul Beckman",1988,"A  nonlinearity  is  required  before  matched  filtering  in  mInimum  error  receivers  when  additive  noise  is  present  which  is  impulsive  and  highly  non-Gaussian.  Experiments  were  performed  to  determine  whether  the  correct clipping  nonlinearity  could  be provided  by  a  single-input  single(cid:173) output  multi-layer  perceptron  trained  with  back  propagation.  It  was  found  that a  multi-layer perceptron with one input and output node,  20  nodes  in  the  first  hidden  layer,  and  5  nodes  in  the  second  hidden  layer  could be trained to provide a  clipping nonlinearity with fewer than 5,000  presentations  of noiseless  and  corrupted  waveform  samples.  A  network  trained  at  a  relatively  high  signal-to-noise  (SIN)  ratio  and  then  used  as  a  front  end  for  a  linear  matched  filter  detector greatly  reduced  the  probability  of error.  The  clipping  nonlinearity  formed  by  this  network  was similar to that used in current receivers designed for impulsive  noise  and  provided  similar substantial  improvements in  performance.",https://papers.nips.cc/paper_files/paper/1988/file/47d1e990583c9c67424d369f3414728e-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/47d1e990583c9c67424d369f3414728e-Abstract.html,
Learning the Solution to the Aperture Problem for Pattern Motion with a Hebb Rule,Martin Sereno,1988,"to  The  primate  visual  system  learns  to  recognize  the  true  direction  of  pattern  motion  using  local  detectors  only  capable  of  detecting  the  component  of  motion  perpendicular  the  orientation  of  the  moving  edge.  A  multilayer  feedforward  network  model  similar  to  Linsker's  model  was  presented  with  input  patterns  each  consisting  of  randomly  oriented  contours  moving  in  a  particular  direction.  Input  layer  units  are  granted  component  direction  and  speed  tuning  curves  similar  to  those  recorded  from  neurons  in  primate  visual  area  VI  that  project  to  area  MT.  The  network  is  trained  on  many  such  patterns  until  most  weights  saturate.  A  proportion  of  the  units  in  the  second  layer  solve  the  aperture  problem  (e.g.,  show  the  to  gratings),  same  direction-tuning  curve  peak  resembling  pattern-direction  selective  neurons,  which  ftrst  appear  inareaMT.",https://papers.nips.cc/paper_files/paper/1988/file/4c56ff4ce4aaf9573aa5dff913df997a-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/4c56ff4ce4aaf9573aa5dff913df997a-Abstract.html,
GENESIS: A System for Simulating Neural Networks,"Matthew Wilson, Upinder Bhalla, John Uhley, James Bower",1988,"support  simulations  at  many  it  is  We  have  developed  a  graphically  oriented,  general  purpose  simulation  system  to  facilitate  the  modeling  of  neural  networks.  The  simulator  is  implemented  under  UNIX  and  X-windows  and  is  levels  of  detail.  designed  to  in  both  applied  network  Specifically,  modeling  and  in  the  simulation  of  detailed,  realistic,  biologically(cid:173) based  models.  Examples  of  current  models  developed  under  this  system  include  mammalian  olfactory  bulb  and  cortex,  invertebrate  central  pattern  generators,  as  well  as  more  abstract  connectionist  simulations.  intended  for  use",https://papers.nips.cc/paper_files/paper/1988/file/4c5bde74a8f110656874902f07378009-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/4c5bde74a8f110656874902f07378009-Abstract.html,
Neural Architecture,Valentino Braitenberg,1988,Abstract Unavailable,https://papers.nips.cc/paper_files/paper/1988/file/5878a7ab84fb43402106c575658472fa-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/5878a7ab84fb43402106c575658472fa-Abstract.html,
Learning by Choice of Internal Representations,"Tal Grossman, Ronny Meir, Eytan Domany",1988,"We  introduce  a  learning algorithm for  multilayer neural  net(cid:173) works  composed of binary linear threshold elements.  Whereas ex(cid:173) isting algorithms reduce  the learning process  to minimizing a  cost  function  over  the  weights,  our  method  treats  the  internal  repre(cid:173) sentations  as  the fundamental entities  to  be  determined.  Once  a  correct set of internal representations is  arrived at, the weights are  found  by  the  local  aild  biologically plausible Perceptron  Learning  Rule  (PLR).  We tested  our  learning algorithm on  four  problems:  adjacency, symmetry, parity and  combined symmetry-parity.",https://papers.nips.cc/paper_files/paper/1988/file/5ef059938ba799aaa845e1c2e8a762bd-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/5ef059938ba799aaa845e1c2e8a762bd-Abstract.html,
Adaptive Neural Networks Using MOS Charge Storage,"Daniel Schwartz, R. Howard, Wayne Hubbard",1988,"MOS charge storage has been demonstrated as an effective method to store  the  weights  in  VLSI  implementations  of  neural  network  models  by  several  workers  2 .  However,  to  achieve  the  full  power  of a  VLSI  implementation  of  an adaptive algorithm, the learning operation must built into the circuit.  We  have  fabricated  and  tested  a  circuit  ideal  for  this  purpose  by  connecting  a  pair of capacitors with  a  CCD like structure, allowing for  variable size  weight  changes  as  well  as  a  weight  decay  operation.  A  2.51-'  CMOS  version  achieves  better  than  10  bits  of dynamic  range  in  a  140/'  X  3501-'  area.  A  1.25/'  chip  based  upon  the  same  cell  has  1104  weights  on  a  3.5mm  x  6.0mm  die  and  is  capable of peak learning rates  of at least  2  x  109  weight  changes  per second.  1  Adaptive  Networks  Much  of the  recent  excitement  about  neural  network  models  of computation  has  been  driven  by  the  prospect  of new  architectures  for  fine  grained  parallel  compu(cid:173) tation using analog VLSI.  Adaptive systems are espescially good targets for  analog  VLSI because the ada.ptive process  can compensate for  the inaccuracy of individual  devices  as easily as for  the variability of the signal.  However, silicon  VLSI  does  not  provide  us  with  an  ideal solution  for  weight  storage.  Among  the  properties  of an  ideal storage technology for  analog VLSI  adaptive systems  are:  â¢  The minimum available weight change ~w must be small.  The simplest adap(cid:173) tive  algorithms optimize  the  weights  by  minimizing  the  output  error  with  a  steepest  descent  search  in weight space  [1].  Iterative improvement algorithms  such  as  steepest  descent  are  based  on  the  heuristic  assumption  of  'better'  weights being found  in  the neighborhood of 'good' ones;  a  heuristic that fails  when  the granularity of the weights is  not fine  enough.  In the worst  case,  the  resolution required just to represent  a  function  can  grow exponentially in  the  dimension of the input space .  â¢  The  weights  must be able  to represent  both  positive and negative values  and  the  changes  must  be easily  reversible.  Frequently,  the  weights  may cycle  up  and down while the adaptive process is converging and millions of incremental  changes  during  a  single  training session  is  not  unreasonable.  If the  weights  cannot easily  follow  all  of these  changes,  then  the  learning  must  be  done  off  chip.  1 Now  at GTE Laboratories, 40 Sylvan Rd., Waltham, Mass 02254  dbs@gte.com%relay.cs.net  2For example, see  the  papers by Mann and Gilbert, Walker and Akers,  and Murray  et.  al.  in",https://papers.nips.cc/paper_files/paper/1988/file/5f93f983524def3dca464469d2cf9f3e-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/5f93f983524def3dca464469d2cf9f3e-Abstract.html,
Implications of Recursive Distributed Representations,Jordan Pollack,1988,"I  will  describe  my  recent  results  on  the  automatic  development  of fixed(cid:173) width recursive  distributed representations  of variable-sized  hierarchal data  structures.  One  implication  of this  wolk  is  that  certain  types  of AI-style  data-structures can now be represented in fixed-width analog vectors. Simple  inferences  can  be  perfonned  using  the  type  of pattern  associations  that  neural  networks excel  at  Another implication arises from  noting that these  representations  become  self-similar in  the  limit Once  this door to  chaos is  opened.  many  interesting new  questions  about  the  representational  basis  of  intelligence emerge, and can (and will) be discussed.",https://papers.nips.cc/paper_files/paper/1988/file/5fd0b37cd7dbbb00f97ba6ce92bf5add-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/5fd0b37cd7dbbb00f97ba6ce92bf5add-Abstract.html,
Backpropagation and Its Application to Handwritten Signature Verification,"Timothy Wilkinson, Dorothy Mighell, Joseph Goodman",1988,"A  pool  of handwritten  signatures  is  used  to  train  a  neural  net(cid:173) work for the task of deciding whether or not a  given signature is a  forgery.  The network is  a feedforward  net, with a binary image as  input.  There is a hidden layer, with a single unit output layer.  The  weights are  adjusted according to the backpropagation algorithm.  The signatures are entered into a C  software program through the  use of a Datacopy Electronic Digitizing Camera.  The binary signa(cid:173) tures  are normalized  and centered.  The performance is  examined  as  a  function of the training set  and network structure.  The  best  scores  are  on  the  order of 2%  true signature  rejection  with  2-4%  false  signature acceptance.",https://papers.nips.cc/paper_files/paper/1988/file/65b9eea6e1cc6bb9f0cd2a47751a186f-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/65b9eea6e1cc6bb9f0cd2a47751a186f-Abstract.html,
Theory of Self-Organization of Cortical Maps,Shigeru Tanaka,1988,"We  have  mathematically  shown  that  cortical  maps  in  the  primary sensory  cortices  can  be  reproduced  by  using  three  hypotheses  which  have  physiological  basis  and  meaning.  Here, our main focus is on ocular.dominance column formation  in the primary visual cortex.  Monte Carlo simulations on the  segregation of ipsilateral and contralateral afferent terminals  are carried out.  Based on  these,  we  show that almost all  the  physiological  experimental  results  concerning  the  ocular  dominance patterns of cats and monkeys reared under normal  or various abnormal visual conditions can be explained from a  viewpoint of the phase transition phenomena.  ROUGH SKETCH OF OUR THEORY  In order to describe the use-dependent self-organization of neural connections  {Singer,1987 and Frank,1987},  we  have  proposed  a  set of coupled  equations  involving  the  electrical  activities  and  neural  connection  density  {Tanaka,  1988}, by using the following physiologically based hypotheses: (1) Modifiable  synapses grow or collapse due to  the competition among themselves for  some  trophic factors,  which are secreted retrogradely from  the postsynaptic side to  the  presynaptic  side.  (2)  Synapses  also  sprout or  retract  according  to  the  concurrence  of presynaptic  spike  activity  and  postsynaptic  local  membrane  depolarization.  (3)  There already exist lateral connections within the  layer,  into  which  the  modifiable  nerve  fibers  are  destined  to  project,  before  the  synaptic modification begins.  Considering this set of equations, we  find  that  the  time  scale  of electrical  activities  is  much  smaller  than  time  course  necessary  for  synapses  to  grow  or  retract.  So  we  can  apply  the  adiabatic  approximation to the equations.  Furthermore, we identify the input electrical  activities,  i.e.,  the  firing  frequency  elicited  from  neurons  in  the  projecting  neuronal layer, with the stochastic process which is specialized by the spatial  correlation function  Ckp;k'  p'.  Here,  k  and  k'  represent  the  positions  of the  neurons  in  the  projecting  layer.  II  stands  for  different  pathways  such  as  ipsilateral  or  contralateral,  on-center  or  off-center,  colour  specific  or  nonspecific  and  so  on.  From  these  approximations,  we  have  a  nonlinear",https://papers.nips.cc/paper_files/paper/1988/file/65ded5353c5ee48d0b7d48c591b8f430-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/65ded5353c5ee48d0b7d48c591b8f430-Abstract.html,
An Adaptive Network That Learns Sequences of Transitions,C. Winter,1988,"We describe  an  adaptive  network,  TIN2,  that learns  the  transition  function of a sequential system from  observations of its behavior.  It  integrates two subnets, TIN-I  (Winter, Ryan and Turner,  1987) and  TIN-2.  TIN-2  constructs  state  representations  from  examples  of  system behavior, and  its  dynamics are the main  topics of the paper.  TIN-I abstracts transition functions from  noisy state representations  and environmental data during training, while in operation it produces  sequences of transitions in response to variations in input.  Dynamics  of both nets are based on the Adaptive Resonance Theory of Carpenter  and Grossberg (1987).  We give results from an experiment in which  TIN2 learned the behavior of a system that recognizes strings with an  even number of l's .",https://papers.nips.cc/paper_files/paper/1988/file/6974ce5ac660610b44d9b9fed0ff9548-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/6974ce5ac660610b44d9b9fed0ff9548-Abstract.html,
Dynamics of Analog Neural Networks with Time Delay,"Charles Marcus, R. Westervelt",1988,"A time delay in the response of the neurons in a network can  induce sustained oscillation and chaos. We present a stability  criterion based on local stability analysis to prevent sustained  oscillation  in  symmetric  delay  networks,  and  show  an  example  of chaotic  dynamics  in  a  non-symmetric  delay  network.",https://papers.nips.cc/paper_files/paper/1988/file/698d51a19d8a121ce581499d7b701668-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/698d51a19d8a121ce581499d7b701668-Abstract.html,
On the K-Winners-Take-All Network,"E. Majani, Ruth Erlanson, Yaser Abu-Mostafa",1988,"We present  and  rigorously  analyze a generalization of the Winner(cid:173) Take-All  Network:  the  K-Winners-Take-All  Network.  This  net(cid:173) work  identifies  the  K  largest  of a  set  of N  real  numbers.  The  network  model used  is  the continuous Hopfield  model.  I  - INTRODUCTION  The Winner-Take-All  Network  is  a  network  which  identifies  the  largest  of N  real  numbers.  Winner-Take-All  Networks  have  been  developed  using  various  neural  networks models (Grossberg-73, Lippman-87, Feldman-82, Lazzaro-89).  We present  here  a  generalization  of the  Winner-Take-All  Network:  the  K-Winners-Take-All  (KWTA) Network.  The KWTA Network identifies the K  largest of N  real numbers.  The neural network model we  use throughout the paper is  the continuous Hopfield  network  model  (Hopfield-84).  If the states of the  N  nodes are initialized  to the N  real numbers, then, if the gain of the sigmoid is large enough, the network converges  to the state with  K  positive real numbers in  the positions of the nodes with  the K  largest  initial states, and  N  - K  negative  real  numbers everywhere else.  Consider  the  following  example:  N  = 4,  K  = 2.  There  are  6  =  (~)  stable  states:(++- +  ++)T, ( +)T, and ( )T.  If the initial  state of the  network  is  (0.3,  -0.4,  0.7,  O.l)T,  then  the network  will  converge to (Vi,V2,V3,v4)T  where Vi> 0,  V2  < 0,  V3  > 0,  V4  < 0  ((+ _ +_)T).  In Section II, we  define  the KWTA  Network (connection weights, external inputs).  In Section III,  we  analyze  the equilibrium states and  in  Section  IV,  we  identify  all  the stable equilibrium states of the KWTA Network.  In Section V, we  describe the  dynamics  of the  KWTA  Network.  In  Section  VI,  we  give  two  important examples  of the KWTA  Network and comment on an alternate implementation of the KWTA  Network.  On the K-Winners-Take-All Network  635  II - THE K-WINNERS-TAKE-ALL  NETWORK  The continuous Hopfield network model (Hopfield-84)  (also known as the Grossberg  additive model  (Grossberg-88)), is characterized by a system of first order differen(cid:173) tial equations which governs the evolution of the state of the network (i = 1, .. . , N) :  The  sigmoid  function  g(u)  is  defined  by:  g(u)  = f(GÂ·  u),  where  G  >  0  is  the  gain  of  the  sigmoid,  and  f(u)  is  defined  by:  1.  ""f/u,  0  <  f'(u)  <  f'(O)  = 1,  2.  limu .... +oo  f( u) = 1,  3.  limu .... -oo f( u) = -l.  The KWTA Network is characterized by mutually inhibitory interconnections Taj  =  -1  for  i  Â¥=  j, a self connection Tai  = a,  (Ial < 1) and'an external input (identical  for  every node) which  depends on the number  K  of winners desired and the size of  the network  N  : ti = 2K - N.  The differential equations for  the KWTA  Network are therefore:  for  all  i,  Cd~i = -Aui + (a + l)g(ui) - (tg(u j )  - t) ,",https://papers.nips.cc/paper_files/paper/1988/file/6c4b761a28b734fe93831e3fb400ce87-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/6c4b761a28b734fe93831e3fb400ce87-Abstract.html,
"Does the Neuron ""Learn"" like the Synapse?",Raoul Tawel,1988,"An improved learning paradigm that offers a significant reduction in com(cid:173)
putation time during the supervised learning phase is described. 
It is based on 
extending the role that the neuron plays in artificial neural systems. Prior work 
has regarded the neuron as a strictly passive, non-linear processing element, and 
the synapse on the other hand as the primary source of information processing and 
knowledge retention. In this work, the role of the neuron is extended insofar as allow(cid:173)
ing its parameters to adaptively participate in the learning phase. The temperature 
of the sigmoid function is an example of such a parameter. During learning, both the 
synaptic interconnection weights w[j and the neuronal temperatures Tr are opti(cid:173)
mized so as to capture the knowledge contained within the training set. The method 
allows each neuron to possess and update its own characteristic local temperature. 
This algorithm has been applied to logic type of problems such as the XOR or parity 
problem, resulting in a significant decrease in the required number of training cycles.",https://papers.nips.cc/paper_files/paper/1988/file/6cdd60ea0045eb7a6ec44c54d29ed402-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/6cdd60ea0045eb7a6ec44c54d29ed402-Abstract.html,
Use of Multi-Layered Networks for Coding Speech with Phonetic Features,"Yoshua Bengio, RÃ©gis Cardin, Renato de Mori, Piero Cosi",1988,"Preliminary  results  on  speaker-independant  speech  recognition  are  reported.  A method  that combines  expertise  on  neural  networks  with  expertise  on  speech  recognition  is  used  to  build  the  recognition  systems.  For  transient  sounds,  event(cid:173) driven  property  extractors  with  variable  resolution  in  the  time  and  frequency  domains  are  used.  For  sonorant  speech,  a  model  of the  human  auditory  system  is  preferred  to  FFT  as  a  front-end  module.",https://papers.nips.cc/paper_files/paper/1988/file/73278a4a86960eeb576a8fd4c9ec6997-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/73278a4a86960eeb576a8fd4c9ec6997-Abstract.html,
Electronic Receptors for Tactile/Haptic Sensing,Andreas Andreou,1988,"We discuss synthetic receptors for  haptic sensing. These are based on  magnetic field sensors (Hall effect structures) fabricated using standard  CMOS technologies.  These receptors, biased with a small permanent  magnet can detect the presence of ferro or ferri-magnetic objects in the  vicinity of the sensor. They can also detect the magnitude and direction  of the magnetic field.",https://papers.nips.cc/paper_files/paper/1988/file/76dc611d6ebaafc66cc0879c71b5db5c-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/76dc611d6ebaafc66cc0879c71b5db5c-Abstract.html,
A Programmable Analog Neural Computer and Simulator,"Paul Mueller, Jan Van der Spiegel, David Blackman, Timothy Chiu, Thomas Clare, Joseph Dao, Christopher Donham, Tzu-pu Hsieh, Marc Loinaz",1988,"This  report describes  the  design  of a  programmable general  purpose analog neural computer and simulator.  It is intended  primarily  for  real-world  real-time  computations  such  as  analysis  of visual  or  acoustical patterns, robotics  and the development of  special purpose  neural nets.  The machine is scalable and  composed of interconnected  modules containing arrays of neurons, modifiable synapses and switches.  It runs  entirely in analog  mode but connection architecture, synaptic  gains and time  constants as well as neuron parameters are set digitally.  Each  neuron has a limited number of inputs and can be connected to any  but not all other neurons. For the determination of synaptic gains and the  implementation  of  learning  algorithms  the  neuron  outputs  are  multiplexed, AID  converted and stored in digital  memory.  Even at  moderate size of 1()3 to IDS neurons  computational speed is expected to  exceed that of any current  digital computer.",https://papers.nips.cc/paper_files/paper/1988/file/7e7757b1e12abcb736ab9a754ffb617a-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/7e7757b1e12abcb736ab9a754ffb617a-Abstract.html,
An Information Theoretic Approach to Rule-Based Connectionist Expert Systems,"Rodney Goodman, John Miller, Padhraic Smyth",1988,"We discuss in this paper architectures for executing probabilistic rule-bases in a par(cid:173) allel manner,  using  as  a theoretical basis recently introduced information-theoretic  models.  We will begin by describing our (non-neural) learning algorithm and theory  of quantitative rule  modelling, followed  by  a discussion on  the exact nature of two  particular models.  Finally we work through an example of our approach, going from  database to rules to inference network, and compare the network's performance with  the theoretical limits for  specific  problems.",https://papers.nips.cc/paper_files/paper/1988/file/7ef605fc8dba5425d6965fbd4c8fbe1f-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/7ef605fc8dba5425d6965fbd4c8fbe1f-Abstract.html,
A Connectionist Expert System that Actually Works,"Richard Fozzard, Gary Bradshaw, Louis Ceci",1988,Abstract Unavailable,https://papers.nips.cc/paper_files/paper/1988/file/7f1de29e6da19d22b51c68001e7e0e54-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/7f1de29e6da19d22b51c68001e7e0e54-Abstract.html,
Cricket Wind Detection,John Miller,1988,Abstract Unavailable,https://papers.nips.cc/paper_files/paper/1988/file/7f6ffaa6bb0b408017b62254211691b5-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/7f6ffaa6bb0b408017b62254211691b5-Abstract.html,
ALVINN: An Autonomous Land Vehicle in a Neural Network,Dean A. Pomerleau,1988,"ALVINN (Autonomous Land Vehicle In a Neural Network) is a 3-layer  back-propagation network designed for the task of road following. Cur(cid:173) rently ALVINN takes images from a camera and a laser range finder as input  and produces as output the direction the vehicle should travel in order to  follow the road. Training has been conducted using simulated road images.  Successful tests on the Carnegie Mellon autonomous navigation test vehicle  indicate that the network can effectively follow real roads under certain field  conditions. The representation developed to perfOIm the task differs dra(cid:173) matically when the networlc is trained under various conditions, suggesting  the possibility of a novel adaptive autonomous navigation system capable of  tailoring its processing to the conditions at hand.",https://papers.nips.cc/paper_files/paper/1988/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/812b4ba287f5ee0bc9d43bbf5bbe87fb-Abstract.html,
Fast Learning in Multi-Resolution Hierarchies,John Moody,1988,"A class of fast, supervised learning algorithms is presented. They use lo(cid:173) cal representations, hashing, atld multiple scales of resolution to approximate  functions which are piece-wise continuous. Inspired by Albus's CMAC model,  the algorithms learn orders of magnitude more rapidly than typical imple(cid:173) mentations of back propagation, while often achieving comparable qualities of  generalization. Furthermore, unlike most traditional function approximation  methods, the algorithms are well suited for use in real time adaptive signal  processing. Unlike simpler adaptive systems, such as linear predictive cod(cid:173) ing, the adaptive linear combiner, and the Kalman filter, the new algorithms  are capable of efficiently capturing the structure of complicated non-linear  systems. As an illustration, the algorithm is applied to the prediction of a  chaotic timeseries.",https://papers.nips.cc/paper_files/paper/1988/file/82161242827b703e6acf9c726942a1e4-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/82161242827b703e6acf9c726942a1e4-Abstract.html,
Mapping Classifier Systems Into Neural Networks,Lawrence Davis,1988,"Classifier systems  are  machine  learning  systems  incotporating  a  genetic  al(cid:173) gorithm  as the learning mechanism.  Although they  respond to inputs that neural  networks can respond to,  their internal  structure, representation  fonnalisms,  and  learning mechanisms differ marlcedly from those employed by neural network re(cid:173) searchers in the same sorts of domains.  As a result, one might conclude that these  two types  of machine learning fonnalisms are intrinsically different.  This is  one  of two papers that, taken together, prove instead that classifier systems and neural  networks  are  equivalent.  In this  paper, half of the  equivalence is  demonstrated  through  the  description  of  a  transfonnation  procedure  that  will  map  classifier  systems into neural networks that  are isomotphic in behavior.  Several alterations  on  the  commonly-used paradigms  employed by  neural  networlc  researchers  are  required  in  order to make  the  transfonnation  worlc.  These alterations  are  noted  and their appropriateness is discussed.  The paper concludes with a  discussion  of  the practical import  of these  results,  and with comments on  their extensibility.",https://papers.nips.cc/paper_files/paper/1988/file/82aa4b0af34c2313a562076992e50aa3-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/82aa4b0af34c2313a562076992e50aa3-Abstract.html,
A Network for Image Segmentation Using Color,"Anya Hurlbert, Tomaso Poggio",1988,"We propose a parallel network of simple processors to find  color boundaries irrespective of spatial changes in illumi(cid:173) nation, and to spread uniform colors within marked re-",https://papers.nips.cc/paper_files/paper/1988/file/8d5e957f297893487bd98fa830fa6413-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/8d5e957f297893487bd98fa830fa6413-Abstract.html,
"Training a Limited-Interconnect, Synthetic Neural IC","M. Walker, S. Haghighi, A. Afghan, Larry Akers",1988,"Hardware implementation of neuromorphic algorithms is hampered by  high  degrees of connectivity.  Functionally equivalent feedforward  networks may be formed by using limited fan-in nodes and additional  layers.  but  this  complicates  procedures  for  determining  weight  magnitudes.  No direct mapping of weights exists between fully and  limited-interconnect  nets.  Low-level  nonlinearities  prevent  the  formation  of internal  representations  of widely  separated  spatial  features and the use of gradient descent methods to minimize output  error is hampered by error magnitude dissipation.  The judicious use  of linear summations or collection units is proposed as a solution.  HARDWARE IMPLEMENTATIONS OF FEEDFORWARD,  SYNTHETIC NEURAL SYSTEMS  The pursuit of hardware implementations of artificial neural network models is motivated  by the need to develop  systems which are capable of executing neuromorphic algorithms  in  real  time.  The most significant barrier is  the  high  degree  of connectivity  required  between the processing elements.  Current interconnect technology does not support the  direct  implementation  of  large-scale  arrays  of  this  type.  In  particular.  the  high  fan-in/fan-outs  of biology  impose  connectivity  requirements  such  that  the  electronic  implementation  of a  highly  interconnected biological  neural  networks  of just a  few  thousand neurons would require a level of connectivity which exceeds the current or even  projected interconnection density ofULSI systems (Akers et al.  1988).  Highly layered. limited-interconnected architectures are however. especially well suited for  VLSI  implementations.  In  previous  works.  we  analyzed  the  generalization  and  fault-tolerance characteristics of a limited-interconnect perceptron architecture applied in  three simple mappings between binary input space and binary output space and proposed a  CMOS architecture (Akers and Walker. 1988).  This paper concentrates on developing an  understanding  of the limitations on  layered  neural  network  architectures  imposed by  hardware implementation and a proposed solution.  778  Walker, Haghighi, Afghan and Akers  TRAINING CONSIDERATIONS FOR  LIMITED .. INTERCONNECT FEEDFORWARD NETWORKS  The symbolic layout of the limited fan-in  network is shown in Fig.  1.  Re-arranging of  the individual input components is done to eliminate edge effects.  Greater detail on the  actual  hardware architecture may be found  in  (Akers and Walker,  1988)  As  in  linear  filters,  the  total  number  of connections  which  fan-in  to  a  given  processing  element  determines the degrees of freedom available for forming a hypersurface which implements  the desired node output function (Widrow and Stearns, 1985).  When processing elements  with fixed, low fan-in are employed, the affects of reduced  degrees of freedom  must be  considered in order to develop workable training methods which permit generalization of  novel  inputs.  First. no  direct or indirect relation  exists  between  weight  magnitudes  obtained for a limited-interconnect, multilayered perceptron, and those obtained for  the  fully connected case.  Networks of these types adapted with identical exemplar sets must  therefore fonn  completely different functions  on  the input  space.  Second,  low-level  nonlinearities prevent direct internal  coding of widely  separated spatial features  in  the  input set.  A related problem  arises  when hyperplane nonlinearities are used.  Multiple  hyperplanes required on a subset of input space are impossible when no two second level  nodes  address  identical positions  in  the  input  space.  Finally,  adaptation  methods  like  backpropagation which minimize output error with gradient descent are hindered since the  magnitude of the error is dissipated as it back-propagates through large numbers of hidden  layers.  The appropriate placement of linear summation elements or collection  units is a  proposed solution.",https://papers.nips.cc/paper_files/paper/1988/file/8f53295a73878494e9bc8dd6c3c7104f-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/8f53295a73878494e9bc8dd6c3c7104f-Abstract.html,
A Model for Resolution Enhancement (Hyperacuity) in Sensory Representation,"Jun Zhang, John Miller",1988,Heiligenberg (1987)  recently proposed a  model  to explain how sen(cid:173) sory  maps  could  enhance  resolution  through  orderly  arrangement  of  broadly tuned receptors.  We  have  extended  this  model  to the general  case  of polynomial  weighting  schemes  and  proved  that  the  response  function  is  also  a  polynomial  of the  same  order.  We  further  demon(cid:173) strated  that  the  Hermitian  polynomials  are  eigenfunctions  of the  sys(cid:173) tem.  Finally we suggested  a  biologically  plausible mechanism for  sen(cid:173) sory representation of external stimuli with resolution far exceeding the  inter-receptor separation.,https://papers.nips.cc/paper_files/paper/1988/file/8f85517967795eeef66c225f7883bdcb-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/8f85517967795eeef66c225f7883bdcb-Abstract.html,
Temporal Representations in a Connectionist Speech System,Erich Smythe,1988,"SYREN  is  a  connectionist model  that uses  temporal  information  in  a  speech signal  for  syllable  recognition.  It classifies  the  rates  and directions of formant center transitions,  and uses an adaptive  method  to  associate  transition  events  with  each  syllable.  The  system  uses  explicit  spatial  temporal  representations through  de(cid:173) lay  lines.  SYREN  uses  implicit  parametric  temporal  representa(cid:173) tions  in  formant  transition  classification  through  node  activation  onset,  decay,  and transition delays  in sub-networks analogous to  visual  motion detector cells.  SYREN  recognizes 79% of six repe(cid:173) titions  of  24  consonant-vowel  syllables  when  tested  on  unseen  data,  and  recognizes  100%  of  its  training  syllables.",https://papers.nips.cc/paper_files/paper/1988/file/903ce9225fca3e988c2af215d4e544d3-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/903ce9225fca3e988c2af215d4e544d3-Abstract.html,
Neural Network Star Pattern Recognition for Spacecraft Attitude Determination and Control,"Phillip Alvelda, A. San Martin",1988,"computational  bottlenecks  Currently,  the  most  complex  spacecraft  attitude  determination  and  control  tasks  are  ultimately  governed  by  ground-based  systems  and  personnel.  Conventional  on-board  systems  face  severe  serial  microprocessors operating on inherently parallel problems.  New  computer architectures based on the anatomy of the human brain  seem  to  promise  high  speed  and  fault-tolerant  solutions  to  the  limitations  of serial  processing.  This  paper  discusses  the  latest  applications of artificial neural networks to the  problem of star  pattern recognition  for spacecraft attitude determination.",https://papers.nips.cc/paper_files/paper/1988/file/96da2f590cd7246bbde0051047b0d6f7-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/96da2f590cd7246bbde0051047b0d6f7-Abstract.html,
A Massively Parallel Self-Tuning Context-Free Parser,Eugene Santos,1988,The  Parsing  and  Learning  System(PALS)  is  a  massively  parallel  self-tuning context-free  parser.  It  is capable  of  parsing sentences of unbounded length mainly due to its  parse-tree representation scheme. The system is capable  of  improving  its  parsing  performance  through  the  presentation of training examples.,https://papers.nips.cc/paper_files/paper/1988/file/9766527f2b5d3e95d4a733fcfb77bd7e-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/9766527f2b5d3e95d4a733fcfb77bd7e-Abstract.html,
Neural Net Receivers in Multiple Access-Communications,"Bernd-Peter Paris, Geoffrey Orsak, Mahesh Varanasi, Behnaam Aazhang",1988,"The application of neural networks to the demodulation of  spread-spectrum signals in a multiple-access environment is  considered. This study is motivated in large part by the fact  that, in a multiuser system, the conventional (matched fil(cid:173) ter) receiver suffers severe performance degradation as the  relative powers of the interfering signals become large (the  ""near-far"" problem). Furthermore, the optimum receiver,  which alleviates the near-far problem, is too complex to be  of practical use. Receivers based on multi-layer perceptrons  are considered as a simple and robust alternative to the opti(cid:173) mum solution. The optimum receiver is used to benchmark  the performance of the neural net receiver; in particular, it is  proven to be instrumental in identifying the decision regions  of the neural networks. The back-propagation algorithm and  a modified version of it are used to train the neural net. An  importance sampling technique is introduced to reduce the  number of simulations necessary to evaluate the performance  of neural nets. In all examples considered the proposed neu(cid:173) ral ~et receiver significantly outperforms the conventional  recelver.",https://papers.nips.cc/paper_files/paper/1988/file/9872ed9fc22fc182d371c3e9ed316094-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/9872ed9fc22fc182d371c3e9ed316094-Abstract.html,
A Computationally Robust Anatomical Model for Retinal Directional Selectivity,"Norberto Grzywacz, Franklin Amthor",1988,"We analyze a mathematical model for  retinal directionally selective  cells  based  on  recent  electrophysiological  data,  and  show  that  its  computation of motion direction is  robust against  noise  and speed.",https://papers.nips.cc/paper_files/paper/1988/file/98dce83da57b0395e163467c9dae521b-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/98dce83da57b0395e163467c9dae521b-Abstract.html,
Statistical Prediction with Kanerva's Sparse Distributed Memory,David Rogers,1988,"A  new  viewpoint  of  the  processing  performed  by  Kanerva's  sparse  distributed  memory  (SDM)  is  presented.  In  conditions  of  near- or  over- capacity,  where  the  associative-memory  behavior  of the  mod(cid:173) el  breaks  down,  the  processing  performed by  the  model  can  be  inter(cid:173) preted  as  that  of  a  statistical  predictor.  Mathematical  results  are  presented  which  serve  as  the  framework  for  a  new  statistical  view(cid:173) point  of  sparse  distributed  memory  and  for  which  the  standard  for(cid:173) mulation  of SDM  is  a  special  case.  This  viewpoint  suggests  possi(cid:173) ble  enhancements  to  the  SDM  model,  including  a  procedure  for  improving  the  predictiveness  of  the  system  based  on  Holland's  work  with  'Genetic  Algorithms',  and  a  method  for  improving  the  capacity of SDM even when used as an associative memory.",https://papers.nips.cc/paper_files/paper/1988/file/9b8619251a19057cff70779273e95aa6-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/9b8619251a19057cff70779273e95aa6-Abstract.html,
Learning Sequential Structure in Simple Recurrent Networks,"David Servan-Schreiber, Axel Cleeremans, James McClelland",1988,"We explore a network architecture introduced by Elman (1988) for  predicting successive elements of a sequence. The network uses the  pattern of activation over a set of hidden units from time-step t-l,  together with element t, to predict element t+ 1. When the network is  trained with strings from a particular finite-state grammar, it can learn  to be a perfect finite-state recognizer for the grammar. Cluster analyses  of the hidden-layer patterns of activation showed that they encode  prediction-relevant information about the entire path traversed through  the network. We illustrate the phases of learning with cluster analyses  performed at different points during training.  Several connectionist architectures that are explicitly constrained to capture  sequential infonnation have been developed. Examples are Time Delay  Networks (e.g. Sejnowski & Rosenberg. 1986) -- also called 'moving  window' paradigms -- or algorithms such as back-propagation in time  (Rumelhart. Hinton & Williams. 1986), Such architectures use explicit  representations of several consecutive events. if not of the entire history of  past inputs. Recently. Elman (1988) has introduced a simple recurrent  network (SRN) that has the potential to master an infinite corpus of  sequences with the limited means of a learning procedure that is completely  local in time (see Figure I.).",https://papers.nips.cc/paper_files/paper/1988/file/9dcb88e0137649590b755372b040afad-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/9dcb88e0137649590b755372b040afad-Abstract.html,
A Back-Propagation Algorithm with Optimal Use of Hidden Units,Yves Chauvin,1988,"This  paper  presents  a  variation  of  the  back-propagation  algo(cid:173) rithm  that makes  optimal  use  of  a  network  hidden units  by  de(cid:173) cr~asing an  ""energy""  term written  as  a  function  of  the  squared  activations  of  these  hidden units.  The  algorithm  can automati(cid:173) cally  find  optimal  or  nearly  optimal  architectures  necessary  to  solve  known  Boolean  functions,  facilitate  the  interpretation  of  the  activation  of  the  remaining  hidden  units  and  automatically  estimate the complexity of architectures appropriate for phonetic  labeling  problems.  The  general  principle  of the  algorithm  can  also be adapted to different tasks:  for  example,  it can be used to  eliminate the  [0,  0]  local minimum  of the  [-1.  +1]  logistic  acti(cid:173) vation  function  while  preserving  a  much  faster  convergence  and  forcing  binary  activations  over the  set of hidden  units.",https://papers.nips.cc/paper_files/paper/1988/file/9fc3d7152ba9336a670e36d0ed79bc43-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/9fc3d7152ba9336a670e36d0ed79bc43-Abstract.html,
GEMINI: Gradient Estimation Through Matrix Inversion After Noise Injection,"Yann Le Cun, Conrad Galland, Geoffrey E. Hinton",1988,"Learning procedures that measure how random perturbations of unit ac(cid:173) tivities correlate with changes in reinforcement are inefficient but simple  to implement in hardware. Procedures like back-propagation (Rumelhart,  Hinton and Williams, 1986) which compute how changes in activities af(cid:173) fect the output error are much more efficient, but require more complex  hardware. GEMINI is a hybrid procedure for multilayer networks, which  shares many of the implementation advantages of correlational reinforce(cid:173) ment procedures but is more efficient. GEMINI injects noise only at the  first hidden layer and measures the resultant effect on the output error.  A linear network associated with each hidden layer iteratively inverts the  matrix which relates the noise to the error change, thereby obtaining  the error-derivatives. No back-propagation is involved, thus allowing un(cid:173) known non-linearities in the system. Two simulations demonstrate the  effectiveness of GEMINI.",https://papers.nips.cc/paper_files/paper/1988/file/a0a080f42e6f13b3a2df133f073095dd-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/a0a080f42e6f13b3a2df133f073095dd-Abstract.html,
A Self-Learning Neural Network,"Allan Hartstein, R. Koch",1988,Abstract Unavailable,https://papers.nips.cc/paper_files/paper/1988/file/a2557a7b2e94197ff767970b67041697-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/a2557a7b2e94197ff767970b67041697-Abstract.html,
Neural Networks for Model Matching and Perceptual Organization,"Eric Mjolsness, Gene Gindi, P. Anandan",1988,"We introduce an optimization approach for solving problems in com(cid:173) puter vision that involve multiple levels of abstraction. Our objective  functions include compositional and specialization hierarchies. We cast  vision problems as inexact graph matching problems, formulate graph  matching in terms of constrained optimization, and use analog neural  networks to perform the optimization. The method is applicable to per(cid:173) ceptual grouping and model matching. Preliminary experimental results  are shown.",https://papers.nips.cc/paper_files/paper/1988/file/a3c65c2974270fd093ee8a9bf8ae7d0b-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/a3c65c2974270fd093ee8a9bf8ae7d0b-Abstract.html,
Neuronal Maps for Sensory-Motor Control in the Barn Owl,"Clay D. Spence, John Pearson, J. Gelfand, R. Peterson, W. Sullivan",1988,"The bam owl has fused visual/auditory/motor representations of  space in its midbrain which are used to orient the head so that visu(cid:173) al or auditory stimuli are centered in the visual field of view. We  present models and computer simulations of these structures which  address various problems, inclu",https://papers.nips.cc/paper_files/paper/1988/file/a4a042cf4fd6bfb47701cbc8a1653ada-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/a4a042cf4fd6bfb47701cbc8a1653ada-Abstract.html,
Performance of Synthetic Neural Network Classification of Noisy Radar Signals,"Stanley Ahalt, F. Garber, I. Jouny, Ashok Krishnamurthy",1988,"This study evaluates the performance of the multilayer-perceptron  and  the frequency-sensitive  competitive  learning network  in  iden(cid:173) tifying  five  commercial  aircraft  from  radar  backscatter  measure(cid:173) ments.  The performance  of the neural network  classifiers  is  com(cid:173) pared  with  that of the  nearest-neighbor  and  maximum-likelihood  classifiers.  Our  results  indicate  that  for  this  problem,  the  neural  network  classifiers  are  relatively  insensitive  to  changes  in  the  net(cid:173) work  topology,  and  to the noise  level  in  the  training data.  While,  for  this  problem,  the traditional algorithms outperform these sim(cid:173) ple neural classifiers,  we  feel  that neural networks show  the poten(cid:173) tial for  improved performance.",https://papers.nips.cc/paper_files/paper/1988/file/a5e00132373a7031000fd987a3c9f87b-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/a5e00132373a7031000fd987a3c9f87b-Abstract.html,
Connectionist Learning of Expert Preferences by Comparison Training,Gerald Tesauro,1988,"A  new  training  paradigm,  caned  the  ""eomparison  pa.radigm,""  is  introduced  for  tasks in which  a. network must  learn  to choose  a  prdcrred  pattern from  a  set of n  alternatives,  based on  examplcs of Imma.n  expert  prderences.  In this  pa.radigm,  the inpu t  to  the network consists of t.wo  uf the  n  alterna tives,  and  the  trained  output is  the expert's judgement of which  pa.ttern is  better.  This  para.digm is  applied  to  the lea,rning  of hackgammon,  a  difficult  board ga.me in  wllieh  the  expert selects  a  move  from  a. set,  of legal  mmÂ·es.  \Vith  compa.rison  training,  much  higher  levels  of performance  can  hc  a.chiew~d, with  networks  that  are  much  smaller,  and  with  coding  sehemes  t.hat  are  much  simpler  and  easier  to  understand.  Furthermorf',  it  is  possible  to  set  up  the  network  so  tha.t  it  always  produces  consisten t  rank-orderings .  1.",https://papers.nips.cc/paper_files/paper/1988/file/a8baa56554f96369ab93e4f3bb068c22-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/a8baa56554f96369ab93e4f3bb068c22-Abstract.html,
Winner-Take-All Networks of O(N) Complexity,"J. Lazzaro, S. Ryckebusch, M.A. Mahowald, C. A. Mead",1988,"We have designed, fabricated, and tested a series of compact CMOS  integrated circuits that realize the winner-take-all function. These  analog, continuous-time circuits use only O(n) of interconnect to  perform this function. We have also modified the winner-take-all  circuit, realizing a circuit that computes local nonlinear inhibition.  Two general types of inhibition mediate activity in neural systems: subtractive in(cid:173) hibition, which sets a zero level for the computation, and multiplicative (nonlinear)  inhibition, which regulates the gain of the computation. We report a physical real(cid:173) ization of general nonlinear inhibition in its extreme form, known as winner-take-all.  We have designed and fabricated a series of compact, completely functional CMOS  integrated circuits that realize the winner-take-all function, using the full analog  nature of the medium. This circuit has been used successfully as a component  in several VLSI sensory systems that perform auditory localization (Lazzaro and  Mead, in press) and visual stereopsis (Mahowald and Delbruck, 1988). Winner(cid:173) take-all circuits with over 170 inputs function correctly in these sensory systems.  We have also modified this global winner-take-all circuit, realizing a circuit that  computes local nonlinear inhibition. The circuit allows multiple winners in the net(cid:173) work, and is well suited for use in systems that represent a feature space topograph(cid:173) ically and that process several features in parallel. We have designed, fabricated,  and tested a CMOS integrated circuit that computes locally the winner-take-all  function of spatially ordered input.  704  Lazzaro, Ryckebusch, Mahowald and Mead  THE WINNER-TAKE-ALL CmCUIT  Figure 1 is a schematic diagram of the winner-take-all circuit. A single wire, asso(cid:173) ciated with the potential Vc, computes the inhibition for the entire circuit; for an  n neuron circuit, this wire is O(n) long. To compute the global inhibition, each  neuron k contributes a current onto this common wire, using transistor T2 a.' To  apply this global inhibition locally, each neuron responds to the common wire volt(cid:173) age Vc, using transistor Tla.' This computation is continuous in time; no clocks  are used. The circuit exhibits no hysteresis, and operates with a time constant  related to the size of the largest input. The output representation of the circuit  is not binary; the winning output encodes the logarithm of its associated input.  Figure 1. Schematic diagram of the winner-take-all circuit. Each neuron receives  a unidirectional current input 11;; the output voltages VI â¢.. VB represent the result  of the winner-take-all computation. If II; = max(II â¢â¢â¢ IB ), then VI; is a logarithmic  function of 11;; if Ii <: 11;, then Vi ~ O.  A static and dynamic ana.lysis of the two-neuron circuit illustrates these system  properties. Figure 2 shows a schematic diagram of a two-neuron winner-take-all  circuit. To understand the beha.vior of the circuit, we first consider the input  condition II = 12 = 1m. Transistors TIl ~d T12 have identical potentials at gate  and source, and are both sinking 1m; thus, the drain potentials VI and V2 must be  equal. Transistors T21 and T22 have identical source, drain, and gate potentials,  and therefore must sink the identical current ICI = IC2 = Ic/2. In the subthreshold  region of operation, the equation 1m = 10 exp(Vc/Vo) describes transistors Til and  T12 , where 10 is a fabrication parameter, and Vo = kT/qlt. Likewise, the equation  Ic/2 = 10 exp((Vm - Vel/Volt where Vm = VI = V2, describes transistors T21 and  T22 . Solving for Vm(Im, Ie) yields  Vm = Voln(~:) + Voln(:;).",https://papers.nips.cc/paper_files/paper/1988/file/a8f15eda80c50adb0e71943adc8015cf-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/a8f15eda80c50adb0e71943adc8015cf-Abstract.html,
Neural Network Recognizer for Hand-Written Zip Code Digits,"John Denker, W. Gardner, Hans Graf, Donnie Henderson, R. Howard, W. Hubbard, L. D. Jackel, Henry Baird, Isabelle Guyon",1988,"This paper describes the construction of a system that recognizes hand-printed  digits, using a combination of classical techniques and neural-net methods. The  system has been trained and tested on real-world data, derived from zip codes seen  on actual U.S. Mail. The system rejects a small percentage of the examples as  unclassifiable, and achieves a very low error rate on the remaining examples. The  system compares favorably with other state-of-the art recognizers. While some of  the methods are specific to this task, it is hoped that many of the techniques will  be applicable to a wide range of recognition tasks.",https://papers.nips.cc/paper_files/paper/1988/file/a97da629b098b75c294dffdc3e463904-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/a97da629b098b75c294dffdc3e463904-Abstract.html,
Analog Implementation of Shunting Neural Networks,"Bahram Nabet, Robert Darling, Robert Pinter",1988,"An  extremely  compact,  all  analog  and  fully  parallel  implementa(cid:173) tion  of a  class  of shunting  recurrent  neural  networks  that  is  ap(cid:173) plicable to a  wide variety of FET-based integration  technologies is  proposed.  While the contrast enhancement, data compression, and  adaptation to mean input intensity capabilities of the network  are  well suited for  processing of sensory information or feature  extrac(cid:173) tion for a content addressable memory (CAM) system, the network  also admits a global Liapunov function  and can thus achieve stable  CAM storage  itself.  In  addition  the model  can  readily function  as  a front-end  processor to an analog adaptive resonance  circuit.",https://papers.nips.cc/paper_files/paper/1988/file/ac627ab1ccbdb62ec96e702f07f6425b-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/ac627ab1ccbdb62ec96e702f07f6425b-Abstract.html,
Range Image Restoration Using Mean Field Annealing,"Griff Bilbro, Wesley Snyder",1988,"A  new  optimization strategy,  Mean  Field  Annealing, is  presented.  Its application to MAP restoration of noisy range images is derived  and experimentally verified.",https://papers.nips.cc/paper_files/paper/1988/file/b3e3e393c77e35a4a3f3cbd1e429b5dc-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/b3e3e393c77e35a4a3f3cbd1e429b5dc-Abstract.html,
Analyzing the Energy Landscapes of Distributed Winner-Take-All Networks,David Touretzky,1988,"DCPS  (the  Distributed  Connectionist  Production System)  is  a  neural  network  with  complex  dynamical  properties.  Visualizing  the  energy  landscapes of some of its component modules leads to a  better intuitive  understanding  of the  model,  and  suggests  ways  in  which  its  dynamics  can be controlled in order to improve performance on difficult  cases.",https://papers.nips.cc/paper_files/paper/1988/file/b73ce398c39f506af761d2277d853a92-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/b73ce398c39f506af761d2277d853a92-Abstract.html,
Neural Approach for TV Image Compression Using a Hopfield Type Network,"Martine Naillon, Jean-Bernard Theeten",1988,"A self-organizing Hopfield network has been  developed in the context of Vector Ouantiza(cid:173) -tion, aiming at compression of  television  images. The metastable states of the spin  glass-like network are used as  an extra  the Minimal Overlap  storage resource using  and Mezard 1987) to  rule (Krauth  learning  the organization of the attractors.  optimize  The sel f-organi zi ng  that we have  scheme  devised  the generation of an  in  adaptive codebook for any qiven TV image.",https://papers.nips.cc/paper_files/paper/1988/file/bd4c9ab730f5513206b999ec0d90d1fb-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/bd4c9ab730f5513206b999ec0d90d1fb-Abstract.html,
Speech Recognition: Statistical and Neural Information Processing Approaches,John Bridle,1988,Abstract Unavailable,https://papers.nips.cc/paper_files/paper/1988/file/bf8229696f7a3bb4700cfddef19fa23f-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/bf8229696f7a3bb4700cfddef19fa23f-Abstract.html,
Convergence and Pattern-Stabilization in the Boltzmann Machine,"Moshe Kam, Roger Cheng",1988,"The Boltzmann Machine has been introduced as a means to perform  global optimization for multimodal objective functions using the  principles of simulated annealing. In this paper we consider its utility  as a spurious-free content-addressable memory, and provide bounds on  its performance in this context. We show how to exploit the machine's  ability to escape local minima, in order to use it, at a constant  temperature, for unambiguous associative pattern-retrieval in noisy  environments. An association rule, which creates a sphere of influence  around each stored pattern, is used along with the Machine's dynamics  to match the machine's noisy input with one of the pre-stored patterns.  Spurious fIxed points, whose regions of attraction are not recognized by  the rule, are skipped, due to the Machine's fInite probability to escape  from any state. The results apply to the Boltzmann machine and to the  asynchronous net of binary threshold elements (Hopfield model'). They  provide the network designer with worst-case and best-case bounds for  the network's performance, and allow polynomial-time tradeoff studies  of design parameters.",https://papers.nips.cc/paper_files/paper/1988/file/c45147dee729311ef5b5c3003946c48f-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/c45147dee729311ef5b5c3003946c48f-Abstract.html,
Models of Ocular Dominance Column Formation: Analytical and Computational Results,"Kenneth Miller, Joseph Keller, Michael Stryker",1988,"We  have  previously  developed  a  simple  mathemati(cid:173) cal model  for  formation  of ocular  dominance  columns  in  mammalian  visual  cortex.  The  model  provides  a  com(cid:173) mon framework  in  which a  variety  of activity-dependent  biological machanisms can be studied.  Analytic and com(cid:173) putational  results  together  now  reveal  the  following:  if  inputs  specific  to  each eye  are  locally  correlated  in  their  firing,  and are not  anticorrelated within an arbor radius,  monocular  cells  will  robustly  form  and  be  organized  by  intra-cortical  interactions  into  columns.  Broader  corre(cid:173) lations  withln  each  eye,  or anti-correlations  between the  eyes, create a  more purely monocular cortex; positive cor(cid:173) relation  over  an  arbor  radius  yields  an  almost  perfectly  monocular cortex.  Most features of the model can be un(cid:173) derstood  analytically  through  decomposition  into  eigen(cid:173) functions and linear stability analysis.  This allows predic(cid:173) tion of the widths of the columns and other features from  measurable biological parameters.",https://papers.nips.cc/paper_files/paper/1988/file/c8ffe9a587b126f152ed3d89a146b445-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/c8ffe9a587b126f152ed3d89a146b445-Abstract.html,
Digital Realisation of Self-Organising Maps,"Nigel Allinson, Martin Johnson, Kevin Moon",1988,Kevin J. Moon,https://papers.nips.cc/paper_files/paper/1988/file/c9e1074f5b3f9fc8ea15d152add07294-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/c9e1074f5b3f9fc8ea15d152add07294-Abstract.html,
Further Explorations in Visually-Guided Reaching: Making MURPHY Smarter,Bartlett Mel,1988,"MURPHY  is  a  vision-based  kinematic  controller  and  path  planner  based  on  a  connectionist  architecture,  and  implemented  with  a  video  camera and  Rhino XR-series robot  arm.  Imitative of the layout of sen(cid:173) sory  and motor maps in  cerebral cortex,  MURPHY'S internal representa(cid:173) tions  consist of four  coarse-coded populations of simple units represent(cid:173) ing both static and  dynamic aspects of the sensory-motor environment.  In previously reported work [4],  MURPHY first  learned a direct kinematic  model of his  camera-arm system during  a  period  of extended  practice,  and  then  used  this  ""mental  model""  to  heuristically  guide  his  hand  to  unobstructed  visual  targets.  MURPHY  has  since  been  extended  in  two  ways:  First, he  now  learns the inverse differential-kinematics of his  arm  in  addition to ordinary direct  kinematics, which  allows  him to push  his  hand  directly towards  a  visual  target  without  the need  for  search.  Sec(cid:173) ondly,  he now  deals with the much more difficult problem of reaching in  the presence of obstacles.",https://papers.nips.cc/paper_files/paper/1988/file/cedebb6e872f539bef8c3f919874e9d7-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/cedebb6e872f539bef8c3f919874e9d7-Abstract.html,
Scaling and Generalization in Neural Networks: A Case Study,"Subutai Ahmad, Gerald Tesauro",1988,"The  issues  of scaling  and  generalization  have  emerged  as  key  issues  in  current studies of supervised learning from examples in neural networks.  Questions such  as  how  many  training  patterns  and  training  cycles  are  needed for  a problem of a given size  and difficulty,  how  to represent the  inllUh  and how  to choose useful training exemplars,  are of considerable  theoretical  and  practical  importance.  Several  intuitive  rules  of thumb  have been obtained from empirical studies, but as yet there are few  rig(cid:173) orous  results.  In  this  paper we  summarize  a  study Qf generalization in  the simplest possible case-perceptron networks learning linearly separa(cid:173) ble  functions.  The  task  chosen  was  the majority function  (i.e.  return  a  1  if a  majority  of the  input  units  are  on),  a  predicate  with  a  num(cid:173) ber  of useful  properties.  We  find  that  many  aspects  of.generalization  in  multilayer  networks  learning  large,  difficult  tasks  are  reproduced  in  this simple domain, in which  concrete numerical results and even some  analytic understanding can be achieved.",https://papers.nips.cc/paper_files/paper/1988/file/d1f491a404d6854880943e5c3cd9ca25-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/d1f491a404d6854880943e5c3cd9ca25-Abstract.html,
A Model of Neural Oscillator for a Unified Submodule,"Alexandr Kirillov, G. N. Borisyuk, R. M. Borisyuk, Ye. Kovalenko, V. Makarenko, V. Chulaevsky, V. Kryukov",1988,Abstract Unavailable,https://papers.nips.cc/paper_files/paper/1988/file/da4fb5c6e93e74d3df8527599fa62642-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/da4fb5c6e93e74d3df8527599fa62642-Abstract.html,
An Optimality Principle for Unsupervised Learning,Terence Sanger,1988,"We propose an optimality  principle for  training an unsu(cid:173) pervised feedforward neural network based upon maximal  ability to reconstruct the input data from the network out(cid:173) puts.  We describe an algorithm which can be used to train  either  linear or  nonlinear  networks  with  certain  types  of  nonlinearity.  Examples of applications  to the problems of  image  coding,  feature  detection,  and analysis  of random(cid:173) dot stereograms are presented.",https://papers.nips.cc/paper_files/paper/1988/file/e00da03b685a0dd18fb6a08af0923de0-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/e00da03b685a0dd18fb6a08af0923de0-Abstract.html,
Neural Analog Diffusion-Enhancement Layer and Spatio-Temporal Grouping in Early Vision,"Allen Waxman, Michael Seibert, Robert Cunningham, Jian Wu",1988,"A  new  class of neural  network  aimed  at  early  visual  processing  is  described; we call it a Neural  Analog Diffusion-Enhancement Layer or  ""NADEL."" The  network  consists  of two  levels  which  are  coupled  through feedfoward and shunted feedback connections. The lower level  is  a  two-dimensional  diffusion map which  accepts  visual  features  as  input, and spreads activity over larger scales as a function of time. The  upper layer is periodically fed the  activity from  the diffusion layer and  locates local maxima in it (an extreme form  of contrast enhancement)  using a network of local comparators. These local maxima are fed back  to  the  diffusion  layer  using  an  on-center/off-surround  shunting  anatomy. The maxima are also available  as output of the network.  The  network dynamics  serves  to  cluster features  on  multiple  scales  as  a  function of time, and can be used in a variety of early visual processing  tasks such  as:  extraction of comers  and high  curvature  points  along  edge contours, line end detection, gap filling in contours, generation of  fixation points, perceptual grouping on multiple scales, correspondence  and path impletion  in long-range  apparent  motion,  and building  2-D  shape representations that are invariant to  location, orientation, scale,  and small deformation on the visual field.",https://papers.nips.cc/paper_files/paper/1988/file/e2ef524fbf3d9fe611d5a8e90fefdc9c-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/e2ef524fbf3d9fe611d5a8e90fefdc9c-Abstract.html,
Optimization by Mean Field Annealing,"Griff Bilbro, Reinhold Mann, Thomas Miller, Wesley Snyder, David van den Bout, Mark White",1988,"Nearly optimal solutions to many combinatorial problems can be  found using stochastic simulated annealing. This paper extends  the concept of simulated annealing from its original formulation  as a Markov process to a new formulation based on mean field  theory. Mean field annealing essentially replaces the discrete de(cid:173) grees of freedom in simulated annealing with their average values  as computed by the mean field approximation. The net result is  that equilibrium at a given temperature is achieved 1-2 orders of  magnitude faster than with simulated annealing. A general frame(cid:173) work for the mean field annealing algorithm is derived, and its re(cid:173) lationship to Hopfield networks is shown. The behavior of MFA is  examined both analytically and experimentally for a generic combi(cid:173) natorial optimization problem: graph bipartitioning. This analysis  indicates the presence of critical temperatures which could be im(cid:173) portant in improving the performance of neural networks.  STOCHASTIC VERSUS MEAN FIELD  In combinatorial optimization problems, an objective function or Hamiltonian,  H(s), is presented which depends on a vector of interacting 3pim, S = {81,"" .,8N},  in some complex nonlinear way. Stochastic simulated annealing (SSA) (S. Kirk(cid:173) patrick, C. Gelatt, and M. Vecchi (1983)) finds a global minimum of H by com(cid:173) bining gradient descent with a random process. This combination allows, under  certain conditions, choices of s which actually increa3e H, thus providing SSA with  a mechanism for escaping from local minima. The frequency and severity of these  uphill moves is reduced by slowly decreasing a parameter T (often referred to as  the temperature) such that the system settles into a global optimum.  Two conceptual operationo; are involved in simulated annealing: a thermodatic op(cid:173) eration which schedules decreases in the temperature, and a relazation operation  92",https://papers.nips.cc/paper_files/paper/1988/file/ec5decca5ed3d6b8079e2e7e7bacc9f2-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/ec5decca5ed3d6b8079e2e7e7bacc9f2-Abstract.html,
An Application of the Principle of Maximum Information Preservation to Linear Systems,Ralph Linsker,1988,"This paper addresses the problem of determining the weights for a  set  of  linear  filters  (model  ""cells"")  so  as  to  maximize  the  ensemble-averaged information that the cells' output values jointly  convey about their input values,  given  the  statistical properties of  the ensemble of input vectors.  The quantity that is maximized is the  Shannon  information  rate,  or  equivalently  the  average  mutual  information between input and output.  Several models for the role  of processing noise are analyzed, and the biological motivation for  considering them is described.  For simple models in which nearby  input  signal  values  (in  space  or  time)  are  correlated,  the  cells  resulting  from  this  optimization  process  include  center-surround  cells and cells sensitive to temporal variations in input signal.",https://papers.nips.cc/paper_files/paper/1988/file/ec8956637a99787bd197eacd77acce5e-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/ec8956637a99787bd197eacd77acce5e-Abstract.html,
Spreading Activation over Distributed Microfeatures,James Hendler,1988,"One attÂ·empt at explaining human inferencing is that of spread(cid:173) ing activat,ion, particularly in the st.ructured connectionist para(cid:173) digm. This has resulted in t.he building of systems with semanti(cid:173) cally nameable nodes which perform inferencing by examining  t.he pat,t.erns of activation spread. In this paper we demonst.rate  t.hat simple structured network infert'ncing can be p(>rformed by  passing art.iva.t.ion over the weights learned by a distributed alga(cid:173) rit,hm. Thus , an account, is provided which explains a well(cid:173) behaved rela t ionship bet.ween structured and distri butt'd conn('c(cid:173) t.ionist. a.pproachrs.",https://papers.nips.cc/paper_files/paper/1988/file/ed3d2c21991e3bef5e069713af9fa6ca-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/ed3d2c21991e3bef5e069713af9fa6ca-Abstract.html,
Consonant Recognition by Modular Construction of Large Phonemic Time-Delay Neural Networks,Alex Waibel,1988,In this paperl  we show that neural networks for speech recognition can be constructed in  a  modular  fashion  by  exploiting  the  hidden  structure  of previously  trained  phonetic  subcategory networks.  The performance of resulting larger phonetic nets was found to be  as  good  as  the  performance  of the  subcomponent  nets  by  themselves.  This  approach  avoids the excessive learning times  that would be necessary to  train larger networks and  allows  for  incremental  learning.  Large  time-delay  neural  networks  constructed  incrementally  by  applying  these  modular  training  techniques  achieved  a  recognition  performance of 96.0% for all consonants.,https://papers.nips.cc/paper_files/paper/1988/file/eecca5b6365d9607ee5a9d336962c534-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/eecca5b6365d9607ee5a9d336962c534-Abstract.html,
Constraints on Adaptive Networks for Modeling Human Generalization,"Mark Gluck, M. Pavel, Van Henkle",1988,"The potential of adaptive  networks  to learn categorization rules and to  model  human  performance  is  studied  by  comparing  how  natural  and  artificial systems respond to new inputs, i.e., how they generalize.  Like  humans,  networks  can  learn  a  detenninistic  categorization  task  by  a  variety  of  alternative  individual  solutions.  An  analysis  of  the  con(cid:173) straints imposed by using networks with the minimal number of hidden  units  shows  that  this  ""minimal  configuration""  constraint  is  not  sufficient to explain and predict human performance;  only a few  solu(cid:173) tions  were  found  to be  shared by both  humans and  minimal  adaptive  networks.  A  further  analysis  of human  and  network  generalizations  indicates  that  initial  conditions  may  provide  important constraints  on  generalization.  A new  technique,  which  we  call  ""reversed learning"",  is described for finding appropriate initial conditions.",https://papers.nips.cc/paper_files/paper/1988/file/f0935e4cd5920aa6c7c996a5ee53a70f-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/f0935e4cd5920aa6c7c996a5ee53a70f-Abstract.html,
Self Organizing Neural Networks for the Identification Problem,"Manoel Tenorio, Wei-Tsih Lee",1988,"This  work  introduces  a  new  method called Self Organizing  Neural  Network  (SONN)  algorithm  and  demonstrates  its  use  in  a  system  identification  task.  The  algorithm  constructs  the  network,  chooses the neuron functions, and adjusts the weights. It is compared to  the Back-Propagation algorithm in the identification of the chaotic time  series.  The  results  shows  that  SONN  constructs  a  simpler,  more  accurate model. requiring less training data and epochs. The algorithm  can be applied and generalized to appilications as a classifier.",https://papers.nips.cc/paper_files/paper/1988/file/f2217062e9a397a1dca429e7d70bc6ca-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/f2217062e9a397a1dca429e7d70bc6ca-Abstract.html,
Learning with Temporal Derivatives in Pulse-Coded Neuronal Systems,"David Parker, Mark Gluck, Eric Reifsnider",1988,"A number of learning models have recently been proposed which  involve calculations of temporal differences (or derivatives in  continuous-time models). These models. like most adaptive network  models. are formulated in tenns of frequency (or activation), a useful  abstraction of neuronal firing rates. To more precisely evaluate the  implications of a neuronal model. it may be preferable to develop a  model which transmits discrete pulse-coded information. We point out  that many functions and properties of neuronal processing and learning  may depend. in subtle ways. on the pulse-coded nature of the informa(cid:173) tion coding and transmission properties of neuron systems. When com(cid:173) pared to formulations in terms of activation. computing with temporal  derivatives (or differences) as proposed by Kosko (1986). Klopf  (1988). and Sutton (1988). is both more stable and easier when refor(cid:173) mulated for a more neuronally realistic pulse-coded system. In refor(cid:173) mulating these models in terms of pulse-coding. our motivation has  been to enable us to draw further parallels and connections between  real-time behavioral models of learning and biological circuit models  of the substrates underlying learning and memory.",https://papers.nips.cc/paper_files/paper/1988/file/f4b9ec30ad9f68f89b29639786cb62ef-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html,
Using Backpropagation with Temporal Windows to Learn the Dynamics of the CMU Direct-Drive Arm II,"Kenneth Goldberg, Barak Pearlmutter",1988,"Computing the inverse dynamics  of a robot ann is an active area of research  in the control literature.  We hope to  learn the  inverse dynamics  by training  a neural network on the  measured  response of a physical ann.  The  input  to  the  network is  a  temporal  window of measured positions;  output is  a vector  of torques.  We  train  the  network on  data measured from  the  first  two joints  of the CMU Direct-Drive Arm II  as  it moves  through a randomly-generated  sample  of ""pick-and-place""  trajectories.  We  then  test  generalization  with  a  new  trajectory  and  compare  its  output  with  the  torque  measured  at  the  physical arm.  The network  is  shown  to  generalize with  a root mean  square  error/standard deviation  (RMSS)  of 0.10.  We  interpreted the weights  of the  network in tenns of the velocity and acceleration filters  used in  conventional  control  theory.",https://papers.nips.cc/paper_files/paper/1988/file/f7e6c85504ce6e82442c770f7c8606f0-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/f7e6c85504ce6e82442c770f7c8606f0-Abstract.html,
Storing Covariance by the Associative Long-Term Potentiation and Depression of Synaptic Strengths in the Hippocampus,"Patric Stanton, Terrence J. Sejnowski",1988,"In modeling studies or memory based on neural networks, both the selective  enhancement and depression or synaptic strengths are required ror effident storage  or inrormation (Sejnowski, 1977a,b; Kohonen, 1984; Bienenstock et aI, 1982;  Sejnowski and Tesauro, 1989). We have tested this assumption in the hippocampus,  a cortical structure or the brain that is involved in long-term memory. A brier,  high-frequency activation or excitatory synapses in the hippocampus produces an  increase in synaptic strength known as long-term potentiation, or L TP (BUss and  Lomo, 1973), that can last ror many days. LTP is known to be Hebbian since it  requires the simultaneous release or neurotransmitter from presynaptic terminals  coupled with postsynaptic depolarization (Kelso et al, 1986; Malinow and Miller,  1986; Gustatrson et al, 1987). However, a mechanism ror the persistent reduction or  synaptic strength that could balance LTP has not yet been demonstrated. We stu(cid:173) died the associative interactions between separate inputs onto the same dendritic  trees or hippocampal pyramidal cells or field CAl, and round that a low-frequency  input which, by itselr, does not persistently change synaptic strength, can either  increase (associative L TP) or decrease in strength (associative long-term depression  or LTD) depending upon whether it is positively or negatively correlated in time  with a second, high-frequency bursting input. LTP or synaptic strength is Hebbian,  and LTD is anti-Hebbian since it is elicited by pairing presynaptic firing with post(cid:173) synaptic hyperpolarization sufficient to block postsynaptic activity. Thus, associa(cid:173) tive L TP and associative L TO are capable or storing inrormation contained in the  covariance between separate, converging hippocampal inputs â¢  â¢ Present address: Dep~ents of NeW'Oscience and Neurology, Albert Einstein College  of Medicine, 1410 Pelham Parkway South, Bronx, NY 10461 USA.  tPresent address: Computational Neurobiology Laboratory, The Salk Institute, P.O. Box  85800, San Diego, CA 92138 USA.  Storing Covariance by Synaptic Strengths in the Hippocampus  395",https://papers.nips.cc/paper_files/paper/1988/file/f899139df5e1059396431415e770c6dd-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/f899139df5e1059396431415e770c6dd-Abstract.html,
"Dynamic, Non-Local Role Bindings and Inferencing in a Localist Network for Natural Language Understanding","Trent Lange, Michael Dyer",1988,"This  paper introduces  a means to  handle the critical problem  of non(cid:173) local  role-bindings  in  localist  spreading-activation  networks.  Every  conceptual node in the network broadcasts a stable, uniquely-identifying  activation pattern, called its signature.  A dynamic role-binding is cre(cid:173) ated  when  a  role's  binding  node  has  an  activation  that  matches  the  bound concept's signature.  Most importantly, signatures are propagated  across long paths of nodes to handle the non-local role-bindings neces(cid:173) sary  for  inferencing.  Our  localist  network  model,  ROBIN  (ROle  Binding  and  Inferencing  Network),  uses  signature  activations  to  ro(cid:173) bustly represent schemata role-bindings and thus perfonn the inferenc(cid:173) ing, plan/goal analysis,  schema instantiation, word-sense disambigua(cid:173) tion, and dynamic re-interpretation portions of the natural language un(cid:173) derstanding process.",https://papers.nips.cc/paper_files/paper/1988/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Abstract.html,
Fixed Point Analysis for Recurrent Networks,"Patrice Simard, Mary Ottaway, Dana Ballard",1988,"This paper provides a systematic analysis of the recurrent backpropaga(cid:173) tion (RBP) algorithm, introducing a number of new results. The main  limitation of the RBP algorithm is that it assumes the convergence of  the network to a stable fixed point in order to backpropagate the error  signals. We show by experiment and eigenvalue analysis that this condi(cid:173) tion can be violated and that chaotic behavior can be avoided. Next we  examine the advantages of RBP over the standard backpropagation al(cid:173) gorithm. RBP is shown to build stable fixed points corresponding to the  input patterns. This makes it an appropriate tool for content address(cid:173) able memories, one-to-many function learning, and inverse problems.",https://papers.nips.cc/paper_files/paper/1988/file/fc221309746013ac554571fbd180e1c8-Paper.pdf,https://papers.nips.cc/paper_files/paper/1988/hash/fc221309746013ac554571fbd180e1c8-Abstract.html,
